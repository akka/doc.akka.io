<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
<title>Transactions &bull; Alpakka Kafka Documentation</title>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="description" content="Alpakka has support for Kafka Transactions which provide guarantees that messages processed in a consume-transform-produce workflow are processed exactly once or not at all."/>
<link rel="canonical" href="https://doc.akka.io/docs/alpakka-kafka/current/transactions.html"/>
<script type="text/javascript" src="lib/jquery/jquery.min.js"></script>
<script type="text/javascript" src="lib/foundation/dist/js/foundation.min.js"></script>
<link rel="stylesheet" type="text/css" href="lib/normalize.css/normalize.css"/>
<link rel="stylesheet" type="text/css" href="lib/foundation/dist/css/foundation.min.css"/>
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/docsearch.js/2/docsearch.min.css" />
<link rel="stylesheet" type="text/css" href="css/icons.css"/>
<link rel="stylesheet" type="text/css" href="css/page-8.css"/>
<link rel="stylesheet" type="text/css" href="css/banner-2.css"/>
<link rel="shortcut icon" href="images/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png"/>
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png"/>
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png"/>
<link rel="manifest" href="images/manifest.json"/>
<meta name="msapplication-TileImage" content="images/mstile-150x150.png"/>
<meta name="msapplication-TileColor" content="#15a9ce"/>
<meta name="theme-color" content="#15a9ce"/>
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) start -->
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js"  type="text/javascript" charset="UTF-8" data-domain-script="28b912e7-09e9-43d5-91e4-3d1897044004" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) end -->
<!--Google Analytics-->
<script type="text/plain" class="optanon-category-2">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', '']);
_gaq.push(['_setDomainName', '']);
_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})()
</script>
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-2">
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KBJGH35');
</script>
<!--Marketo-->
<script type="text/plain" class="optanon-category-3">
(function() {
var didInit = false;
function initMunchkin() {
if(didInit === false) {
didInit = true;
Munchkin.init('558-NCX-702', { 'asyncOnly': true, 'disableClickDelay': true });
}
}
var s = document.createElement('script');
s.type = 'text/javascript';
s.async = true;
s.src = '//munchkin.marketo.net/munchkin.js';
s.onreadystatechange = function() {
if (this.readyState == 'complete' || this.readyState == 'loaded') {
initMunchkin();
}
};
s.onload = initMunchkin;
document.getElementsByTagName('head')[0].appendChild(s);
})();
</script>
</head>

<body id="underlay" data-toggler="nav-open">
<div id="lightbend-banner" class="lightbend-banner akka full-width" data-category="OSS Lightbend Banner Impression" data-label="Akka Banner Impression">
<div class="oss-wrapper">
<div class="oss-brand">
<a class="oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Lightbend Logo - Akka Banner" href="https://www.lightbend.com">
<img class="lightbend-logo" src="images/banner-logos/lightbend-reverse.svg" alt="Lightbend" title="Lightbend">
</a>
</div>
<div class="oss-ad no-drop-down">
<nav id="lightbendRotator" class="lightbend-rotator">
<a class="oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Promo Rotator - Lightbend is Changing its Software Licensing Model for Akka Technology. [License FAQ] - Akka Banner" href="https://www.lightbend.com/akka/license-faq">
<strong>Lightbend</strong> is Changing its Software Licensing Model for Akka Technology. <span class="akka-btn">License FAQ</span>
</a>
</nav>
</div>
</div>
</div>

<header class="site-header hide-for-large">
<div class="sticky-header clearfix">
<a href="https://akka.io"><img class="logo" src="images/akka-alpakka-reverse.svg"></a>

<button class="menu-icon float-right" type="button" data-toggle="underlay overlay"></button>
</div>
<div id="overlay" class="overlay-nav" data-toggler="nav-open">
<header class="nav-header">
<div class="nav-header-title">
<h1><a href="index.html">Alpakka Kafka Documentation</a></h1>
</div>
<div class="nav-header-version">
Version 3.1.0-M1
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="nav-toc">
<ul>
  <li><a href="home.html" class="page">Overview</a></li>
  <li><a href="producer.html" class="page">Producer</a></li>
  <li><a href="consumer.html" class="page">Consumer</a></li>
  <li><a href="discovery.html" class="page">Service discovery</a></li>
  <li><a href="cluster-sharding.html" class="page">Akka Cluster Sharding</a></li>
  <li><a href="errorhandling.html" class="page">Error handling</a></li>
  <li><a href="atleastonce.html" class="page">At-Least-Once Delivery</a></li>
  <li><a href="transactions.html#transactions" class="active page">Transactions</a>
  <ul>
    <li><a href="transactions.html#transactional-source" class="header">Transactional Source</a></li>
    <li><a href="transactions.html#transactional-sink-and-flow" class="header">Transactional Sink and Flow</a></li>
    <li><a href="transactions.html#consume-transform-produce-workflow" class="header">Consume-Transform-Produce Workflow</a></li>
    <li><a href="transactions.html#caveats" class="header">Caveats</a></li>
    <li><a href="transactions.html#further-reading" class="header">Further Reading</a></li>
  </ul></li>
  <li><a href="serialization.html" class="page">Serialization</a></li>
  <li><a href="debugging.html" class="page">Debugging</a></li>
  <li><a href="testing.html" class="page">Testing</a></li>
  <li><a href="production.html" class="page">Production considerations</a></li>
  <li><a href="snapshots.html" class="page">Snapshots</a></li>
</ul>
</nav>
</div>
</header>
<div class="site-content-wrapper">
<aside class="sticky-sidebar show-for-large">
<header class="nav-header sticky-sidebar-header">
<div class="nav-header-title">
<h1><a href="index.html">Alpakka Kafka Documentation</a></h1>
</div>
<div class="nav-header-version">
Version 3.1.0-M1
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="site-nav sticky-sidebar-contents">
<div class="nav-toc">
<ul>
  <li><a href="home.html" class="page">Overview</a></li>
  <li><a href="producer.html" class="page">Producer</a></li>
  <li><a href="consumer.html" class="page">Consumer</a></li>
  <li><a href="discovery.html" class="page">Service discovery</a></li>
  <li><a href="cluster-sharding.html" class="page">Akka Cluster Sharding</a></li>
  <li><a href="errorhandling.html" class="page">Error handling</a></li>
  <li><a href="atleastonce.html" class="page">At-Least-Once Delivery</a></li>
  <li><a href="transactions.html#transactions" class="active page">Transactions</a>
  <ul>
    <li><a href="transactions.html#transactional-source" class="header">Transactional Source</a></li>
    <li><a href="transactions.html#transactional-sink-and-flow" class="header">Transactional Sink and Flow</a></li>
    <li><a href="transactions.html#consume-transform-produce-workflow" class="header">Consume-Transform-Produce Workflow</a></li>
    <li><a href="transactions.html#caveats" class="header">Caveats</a></li>
    <li><a href="transactions.html#further-reading" class="header">Further Reading</a></li>
  </ul></li>
  <li><a href="serialization.html" class="page">Serialization</a></li>
  <li><a href="debugging.html" class="page">Debugging</a></li>
  <li><a href="testing.html" class="page">Testing</a></li>
  <li><a href="production.html" class="page">Production considerations</a></li>
  <li><a href="snapshots.html" class="page">Snapshots</a></li>
</ul>
</div>
</nav>
<footer class="nav-footer sticky-sidebar-footer">
<a href="https://akka.io"><img class="logo" src="images/akka-alpakka-reverse.svg"></a>

</footer>
</aside>
<main class="fixed-shift-for-large expanded row">
<section class="site-content small-12 column">
<article class="page-content row">
<div class="small-12 column" id="docs">
<h1><a href="#transactions" name="transactions" class="anchor"><span class="anchor-link"></span></a>Transactions</h1>
<p>Kafka Transactions provide guarantees that messages processed in a consume-transform-produce workflow (consumed from a source topic, transformed, and produced to a destination topic) are processed exactly once or not at all. This is achieved through coordination between the Kafka consumer group coordinator, transaction coordinator, and the consumer and producer clients used in the user application. The Kafka producer marks messages that are consumed from the source topic as &ldquo;committed&rdquo; only once the transformed messages are successfully produced to the sink. </p>
<p>For full details on how transactions are achieved in Kafka you may wish to review the Kafka Improvement Proposal <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging">KIP-98: Exactly Once Delivery and Transactional Messaging</a> and its associated <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#heading=h.xq0ee1vnpz4o">design document</a>. </p>
<h2><a href="#transactional-source" name="transactional-source" class="anchor"><span class="anchor-link"></span></a>Transactional Source</h2>
<p>The <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.source</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.source</code></a></span> emits a <span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/ConsumerMessage$$TransactionalMessage.html" title="akka.kafka.ConsumerMessage.TransactionalMessage"><code>ConsumerMessage.TransactionalMessage</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/ConsumerMessage$$TransactionalMessage.html" title="akka.kafka.ConsumerMessage.TransactionalMessage"><code>ConsumerMessage.TransactionalMessage</code></a></span> which contains topic, partition, and offset information required by the producer during the commit process. Unlike with <span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/ConsumerMessage$$CommittableMessage.html" title="akka.kafka.ConsumerMessage.CommittableMessage"><code>ConsumerMessage.CommittableMessage</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/ConsumerMessage$$CommittableMessage.html" title="akka.kafka.ConsumerMessage.CommittableMessage"><code>ConsumerMessage.CommittableMessage</code></a></span>, the user is not responsible for committing transactions, this is handled by a <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.flow</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.flow</code></a></span> or <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.sink</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.sink</code></a></span>.</p>
<p>This source overrides the Kafka consumer property <code>isolation.level</code> to <code>read_committed</code>, so that only committed messages can be consumed.</p>
<p>A consumer group ID must be provided.</p>
<p>Only use this source if you have the intention to connect it to a <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.flow</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.flow</code></a></span> or <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.sink</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.sink</code></a></span>.</p>
<!-- TODO: uncomment when Transacitonal.partitionedSource is ready
## Transactional Partitioned Source

The @apidoc[Transactional.partitionedSource](Transactional$) is similar to the  `Transactional.source`.
It allows you to run transactional workloads per partition which makes it easier to distribute your transactional application across multiple instances.
When a topic-partition is assigned to a consumer the source will emit a tuple with the assigned topic-partition and a corresponding source.
When a topic-partition is revoked, the corresponding source completes.
 
One of the main advantages of using the `Transactional.partitionedSource` is that the transactional producer will automatically create a new `transactional.id` concatenated from the `transactionalId` provided by the user and the consumer group id, topic, and partition number associated with messages from the source.
This allows users to distribute multiple instances of the application without having to worry about *transactional producer fencing* from conflicting duplicate `transactional.id`'s, which would be the case when using the non-partitioned `Transactional.source`.

@@@note 

The partitioned source requires a Kafka Producer per source (per partition) in order to prevent producer fencing.
This can lead to several performance implications.

1. A single producer per application has the opportunity to collectively batch sends to allow for better throughput.
If we subdivide the same producing workload with multiple producers then we will lose the efficiency of consecutive batching to Kafka that one producer can manage.
Since the Kafka Producer is threadsafe we would ideally only have one Producer per Alpakka Kafka application, but this isn't possible if we want to distribute our transactional application across multiple instances.

2. The Kafka cluster will receive more connection and request overhead because there are more batches sent from more producers.

This is a known issue in the Apache Kafka community and there's a Kafka Improvement Proposal (KIP), [KIP-447](https://cwiki.apache.org/confluence/display/KAFKA/KIP-447%3A+Producer+scalability+for+exactly+once+semantics), that's been created to address the problem.
Below is an excerpt from its Motivation section.

> The problem we are trying to solve in this proposal is a semantic mismatch between consumers in a group and transactional producers. In a consumer group, ownership of partitions can transfer between group members through the rebalance protocol. For transactional producers, assignments are assumed to be static. Every transactional id must map to a consistent set of input partitions. To preserve the static partition mapping in a consumer group where assignments are frequently changing, the simplest solution is to create a separate producer for every input partition. This is what Kafka Streams does today.

For more details see [KIP-447](https://cwiki.apache.org/confluence/display/KAFKA/KIP-447%3A+Producer+scalability+for+exactly+once+semantics) ([Design Document](https://docs.google.com/document/d/1LhzHGeX7_Lay4xvrEXxfciuDWATjpUXQhrEIkph9qRE/edit)).

@@@

-->
<h2><a href="#transactional-sink-and-flow" name="transactional-sink-and-flow" class="anchor"><span class="anchor-link"></span></a>Transactional Sink and Flow</h2>
<p>The <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.sink</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.sink</code></a></span> is similar to the <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Producer$.html" title="akka.kafka.javadsl.Producer"><code>Producer.committableSink</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Producer$.html" title="akka.kafka.scaladsl.Producer"><code>Producer.committableSink</code></a></span> in that messages will be automatically committed as part of a transaction. The <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.flow</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.flow</code></a></span> or <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.sink</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.sink</code></a></span> are required when connecting a consumer to a producer to achieve a transactional workflow.</p>
<p>They override producer properties <code>enable.idempotence</code> to <code>true</code> and <code>max.in.flight.requests.per.connection</code> to <code>1</code> as required by the Kafka producer to enable transactions.</p>
<p>A <code>transactional.id</code> must be defined and unique for each instance of the application.</p>
<h2><a href="#consume-transform-produce-workflow" name="consume-transform-produce-workflow" class="anchor"><span class="anchor-link"></span></a>Consume-Transform-Produce Workflow</h2>
<p>Kafka transactions are handled transparently to the user. The <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.source</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.source</code></a></span> will enforce that a consumer group id is specified and the <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.flow</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.flow</code></a></span> or <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional.sink</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional.sink</code></a></span> will enforce that a <code>transactional.id</code> is specified. All other Kafka consumer and producer properties required to enable transactions are overridden.</p>
<p>Transactions are committed on an interval which can be controlled with the producer config <code>akka.kafka.producer.eos-commit-interval</code>, similar to how exactly once works with Kafka Streams. The default value is <code>100ms</code>. The larger commit interval is the more records will need to be reprocessed in the event of failure and the transaction is aborted.</p>
<p>When the stream is materialized the producer will initialize the transaction for the provided <code>transactional.id</code> and a transaction will begin. Every commit interval (<code>eos-commit-interval</code>) we check if there are any offsets available to commit. If offsets exist then we suspend backpressured demand while we drain all outstanding messages that have not yet been successfully acknowledged (if any) and then commit the transaction. After the commit succeeds a new transaction is begun and we re-initialize demand for upstream messages.</p>
<p>Messages are also drained from the stream when the consumer gets a rebalance of partitions. In that case, the consumer will wait in the <code>onPartitionsRevoked</code> callback until all of the messages have been drained from the stream and the transaction is committed before allowing the rebalance to continue. The amount of total time the consumer will wait for draining is controlled by the <code>akka.kafka.consumer.commit-timeout</code>, and the interval between checks is controlled by the <code>akka.kafka.consuner.eos-draining-check-interval</code> configuration settings.</p>
<p>To gracefully shutdown the stream and commit the current transaction you must call <code>shutdown()</code> on the <span class="group-java"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/javadsl/Consumer$$Control.html" title="akka.kafka.javadsl.Consumer.Control"><code>Consumer.Control</code></a></span><span class="group-scala"><a href="/api/alpakka-kafka/3.1.0-M1/akka/kafka/scaladsl/Consumer$$Control.html" title="akka.kafka.scaladsl.Consumer.Control"><code>Consumer.Control</code></a></span> materialized value to await all produced message acknowledgements and commit the final transaction. </p>
<h3><a href="#simple-example" name="simple-example" class="anchor"><span class="anchor-link"></span></a>Simple Example</h3>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><button class="snippet-button copy-snippet" title="Copy snippet to clipboard">copy</button><a class="snippet-button go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v3.1.0-M1/tests/src/test/scala/docs/scaladsl/TransactionsExample.scala#L34-L57" target="_blank" title="Go to snippet source">source</a><code class="language-scala">val control =
  Transactional
    .source(consumerSettings, Subscriptions.topics(sourceTopic))
    .via(businessFlow)
    .map { msg =&gt;
      ProducerMessage.single(new ProducerRecord(sinkTopic, msg.record.key, msg.record.value), msg.partitionOffset)
    }
    .toMat(Transactional.sink(producerSettings, transactionalId))(DrainingControl.apply)
    .run()

// ...

control.drainAndShutdown()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><button class="snippet-button copy-snippet" title="Copy snippet to clipboard">copy</button><a class="snippet-button go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v3.1.0-M1/tests/src/test/java/docs/javadsl/TransactionsExampleTest.java#L72-L94" target="_blank" title="Go to snippet source">source</a><code class="language-java">Consumer.DrainingControl&lt;Done&gt; control =
    Transactional.source(consumerSettings, Subscriptions.topics(sourceTopic))
        .via(business())
        .map(
            msg -&gt;
                ProducerMessage.single(
                    new ProducerRecord&lt;&gt;(targetTopic, msg.record().key(), msg.record().value()),
                    msg.partitionOffset()))
        .toMat(
            Transactional.sink(producerSettings, transactionalId),
            Consumer::createDrainingControl)
        .run(system);

// ...

control.drainAndShutdown(ec);</code></pre></dd>
</dl>
<!-- TODO: uncomment when Transacitonal.partitionedSource is ready
### Partitioned Source Example

Scala
: @@ snip [snip](/tests/src/test/scala/docs/scaladsl/TransactionsExample.scala) { #partitionedTransactionalSink }

Java
: @@ snip [snip](/tests/src/test/java/docs/javadsl/TransactionsExampleTest.java) { #partitionedTransactionalSink }
-->
<h3><a href="#recovery-from-failure" name="recovery-from-failure" class="anchor"><span class="anchor-link"></span></a>Recovery From Failure</h3>
<p>When any stage in the stream fails the whole stream will be torn down. In the general case it&rsquo;s desirable to allow transient errors to fail the whole stream because they cannot be recovered from within the application. Transient errors can be caused by network partitions, Kafka broker failures, <a href="https://kafka.apache.org/30/javadoc/org/apache/kafka/common/errors/ProducerFencedException.html" title="org.apache.kafka.common.errors.ProducerFencedException"><code>ProducerFencedException</code></a>&rsquo;s from other application instances, and so on. When the stream encounters transient errors then the current transaction will be aborted before the stream is torn down. Any produced messages that were not committed will not be available to downstream consumers as long as those consumers are configured with <code>isolation.level = read_committed</code>.</p>
<p>For transient errors we can choose to rely on the Kafka producer&rsquo;s configuration to retry, or we can handle it ourselves at the Akka Streams or Application layer. Using the <a href="https://doc.akka.io/docs/akka/2.7/stream/stream-error.html#delayed-restarts-with-a-backoff-stage">RestartSource</a> we can backoff connection attempts so that we don&rsquo;t hammer the Kafka cluster in a tight loop.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><button class="snippet-button copy-snippet" title="Copy snippet to clipboard">copy</button><a class="snippet-button go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v3.1.0-M1/tests/src/test/scala/docs/scaladsl/TransactionsExample.scala#L96-L121" target="_blank" title="Go to snippet source">source</a><code class="language-scala">val innerControl = new AtomicReference[Control](Consumer.NoopControl)

val stream = RestartSource.onFailuresWithBackoff(
  RestartSettings(
    minBackoff = 1.seconds,
    maxBackoff = 30.seconds,
    randomFactor = 0.2
  )
) { () =&gt;
  Transactional
    .source(consumerSettings, Subscriptions.topics(sourceTopic))
    .via(businessFlow)
    .map { msg =&gt;
      ProducerMessage.single(new ProducerRecord(sinkTopic, msg.record.key, msg.record.value), msg.partitionOffset)
    }
    // side effect out the `Control` materialized value because it can&#39;t be propagated through the `RestartSource`
    .mapMaterializedValue(c =&gt; innerControl.set(c))
    .via(Transactional.flow(producerSettings, transactionalId))
}

stream.runWith(Sink.ignore)

// Add shutdown hook to respond to SIGTERM and gracefully shutdown stream
sys.ShutdownHookThread {
  Await.result(innerControl.get.shutdown(), 10.seconds)
}</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><button class="snippet-button copy-snippet" title="Copy snippet to clipboard">copy</button><a class="snippet-button go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v3.1.0-M1/tests/src/test/java/docs/javadsl/TransactionsExampleTest.java#L137-L168" target="_blank" title="Go to snippet source">source</a><code class="language-java">AtomicReference&lt;Consumer.Control&gt; innerControl =
    new AtomicReference&lt;&gt;(Consumer.createNoopControl());

Source&lt;ProducerMessage.Results&lt;String, String, ConsumerMessage.PartitionOffset&gt;, NotUsed&gt;
    stream =
        RestartSource.onFailuresWithBackoff(
            RestartSettings.create(
                java.time.Duration.ofSeconds(3), // min backoff
                java.time.Duration.ofSeconds(30), // max backoff
                0.2), // adds 20% &quot;noise&quot; to vary the intervals slightly
            () -&gt;
                Transactional.source(consumerSettings, Subscriptions.topics(sourceTopic))
                    .via(business())
                    .map(
                        msg -&gt;
                            ProducerMessage.single(
                                new ProducerRecord&lt;&gt;(
                                    targetTopic, msg.record().key(), msg.record().value()),
                                msg.partitionOffset()))
                    // side effect out the `Control` materialized value because it can&#39;t be
                    // propagated through the `RestartSource`
                    .mapMaterializedValue(
                        control -&gt; {
                          innerControl.set(control);
                          return control;
                        })
                    .via(Transactional.flow(producerSettings, transactionalId)));

CompletionStage&lt;Done&gt; streamCompletion = stream.runWith(Sink.ignore(), system);

// Add shutdown hook to respond to SIGTERM and gracefully shutdown stream
Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; innerControl.get().shutdown()));</code></pre></dd>
</dl>
<h2><a href="#caveats" name="caveats" class="anchor"><span class="anchor-link"></span></a>Caveats</h2>
<p>There are several scenarios that this library&rsquo;s implementation of Kafka transactions does not automatically account for.</p>
<p>All of the scenarios covered in the <a href="atleastonce.html">At-Least-Once Delivery documentation</a> (Multiple Effects per Commit, Non-Sequential Processing, and Conditional Message Processing) are applicable to transactional scenarios as well.</p>
<p>Only one application instance per <code>transactional.id</code> is allowed. If two application instances with the same <code>transactional.id</code> are run at the same time then the instance that registers with Kafka&rsquo;s transaction coordinator second will throw a <a href="https://kafka.apache.org/30/javadoc/org/apache/kafka/common/errors/ProducerFencedException.html" title="org.apache.kafka.common.errors.ProducerFencedException"><code>ProducerFencedException</code></a> so it doesn&rsquo;t interfere with transactions in process by the first instance. To distribute multiple transactional workflows for the same subscription the user must manually subdivide the subscription across multiple instances of the application. This may be handled internally in future versions.</p>
<!-- TODO: replace above with Transactional.partitionedSources is available
When using `Transactional.source` only one application instance per `transactional.id` is allowed.  If two application instances with the same `transactional.id` are run at the same time then the instance that registers with Kafka's transaction coordinator second will throw a @javadoc[ProducerFencedException](org.apache.kafka.common.errors.ProducerFencedException) so it doesn't interfere with transactions in process by the first instance.  To distribute multiple transactional workflows for the same subscription you can use the @ref[Transactional Partitioned Source](#transactional-partitioned-source) `Transactional.partitionedSource`, which manages the `transactional.id` so that no producer fencing occurs.
-->
<p>Any state in the transformation logic is not part of a transaction. It&rsquo;s left to the user to rebuild state when applying stateful operations with transaction. It&rsquo;s possible to encode state into messages produced to topics during a transaction. For example you could produce messages to a topic that represents an event log as part of a transaction. This event log can be replayed to reconstitute the correct state before the stateful stream resumes consuming again at startup.</p>
<p>Any side effects that occur in the transformation logic is not part of a transaction (i.e. writes to an database). </p>
<p>The exactly-once-semantics are guaranteed only when your flow consumes from and produces to the same Kafka cluster. Producing to partitions from a 3rd-party source or consuming partitions from one Kafka cluster and producing to another Kafka cluster are not supported.</p>
<h2><a href="#further-reading" name="further-reading" class="anchor"><span class="anchor-link"></span></a>Further Reading</h2>
<p>For more information on exactly once and transactions in Kafka please consult the following resources.</p>
<ul>
  <li><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging">KIP-98: Exactly Once Delivery and Transactional Messaging</a> (<a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#heading=h.xq0ee1vnpz4o">Design Document</a>)</li>
  <li><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-129%3A+Streams+Exactly-Once+Semantics">KIP-129: Streams Exactly-Once Semantics</a> (<a href="https://docs.google.com/document/d/1pGZ8xtOOyGwDYgH5vA6h19zOMMaduFK1DAB8_gBYA2c/edit#heading=h.vkrkjfth3p8p">Design Document</a>)</li>
  <li><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-447%3A+Producer+scalability+for+exactly+once+semantics">KIP-447: EOS Scalability Design</a> (<a href="https://docs.google.com/document/d/1LhzHGeX7_Lay4xvrEXxfciuDWATjpUXQhrEIkph9qRE/edit">Design Document</a>)</li>
  <li><a href="https://bravenewgeek.com/you-cannot-have-exactly-once-delivery-redux/">You Cannot Have Exactly-Once Delivery Redux</a> by Tyler Treat</li>
  <li><a href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/">Exactly-once Semantics are Possible: Here’s How Kafka Does it</a></li>
</ul>
</div>
</article>
<div class="row">
<div class="small-12 column">
<section class="nav-prev-next row">
<div class="nav-prev small-6 column">
<a href="atleastonce.html"><i class="icon-prev"></i> <span class="link-prev">At-Least-Once Delivery</span></a>
</div>
<div class="nav-next small-6 column clearfix">
<a class="float-right" href="serialization.html">Serialization <i class="icon-next"></i></a>
</div>
</section>
</div>
</div>
<div class="source-github row">
Found an error in this documentation? The source code for this page can be found <a href="https://github.com/akka/alpakka-kafka/tree/v3.1.0-M1/docs/src/main/paradox/transactions.md">here</a>.
Please feel free to edit and contribute a pull request.
</div>

<footer class="page-footer row clearfix">
<img class="akka-icon float-left show-for-medium" src="images/akka-icon.svg" />
<section class="copyright">
<div>Alpakka Kafka is available under the <a href="https://www.lightbend.com/akka/license" target="_blank">Business Source License 1.1</a>.</div>
<p class="legal">
&copy; 2011-2022 <a href="https://www.lightbend.com" target="_blank">Lightbend, Inc.</a> |
<a href="https://www.lightbend.com/legal/licenses" target="_blank">Licenses</a> |
<a href="https://www.lightbend.com/legal/terms" target="_blank">Terms</a> |
<a href="https://www.lightbend.com/legal/privacy" target="_blank">Privacy Policy</a> |
<a href="https://akka.io/cookie/" target="_blank">Cookie Listing</a> |
<a class="optanon-toggle-display">Cookie Settings</a>
</p>
</section>

</footer>
</section>
</main>
</div>

<script type="text/javascript" src="js/scrollsneak.js"></script>
<script type="text/javascript">jQuery(document).foundation();</script>
<script type="text/javascript" src="js/groups.js"></script>
<script type="text/javascript" src="js/page.js"></script>
<script type="text/javascript" src="js/snippets.js"></script>
<script type="text/javascript" src="js/magellan.js"></script>
<script type="text/javascript" src="js/metadata-toggle.js"></script>
<script type="text/javascript" src="js/lbHeader.js"></script>

<style type="text/css">@import "lib/prettify/prettify.css";</style>
<script type="text/javascript" src="lib/prettify/prettify.js"></script>
<script type="text/javascript" src="lib/prettify/lang-scala.js"></script>
<script type="text/javascript">//<![CDATA[
jQuery(function(){window.prettyPrint && prettyPrint()});
//]]></script>
<script type="text/javascript" src="assets/js/warnOldVersion.js"></script>
<script type="text/javascript">jQuery(function(jq){initOldVersionWarnings(jq, '3.1.0-M1', 'https://doc.akka.io/docs/alpakka-kafka/current')});</script>


</body>
</html>
