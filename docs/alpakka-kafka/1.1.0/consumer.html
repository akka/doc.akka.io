<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
<title>Consumer &bull; Alpakka Kafka Documentation</title>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="description" content="Consume messages from Apache Kafka in Akka Streams sources and their commit offsets to Kafka."/>
<link rel="canonical" href="https://doc.akka.io/docs/alpakka-kafka/current/consumer.html"/>
<script type="text/javascript" src="lib/jquery/jquery.min.js"></script>
<script type="text/javascript" src="lib/foundation/dist/js/foundation.min.js"></script>
<link rel="stylesheet" type="text/css" href="lib/normalize.css/normalize.css"/>
<link rel="stylesheet" type="text/css" href="lib/foundation/dist/css/foundation.min.css"/>
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/docsearch.js/2/docsearch.min.css" />
<link rel="stylesheet" type="text/css" href="css/icons.css"/>
<link rel="stylesheet" type="text/css" href="css/page-5.css"/>
<link rel="stylesheet" type="text/css" href="css/banner.css"/>
<link rel="shortcut icon" href="images/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png"/>
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png"/>
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png"/>
<link rel="manifest" href="images/manifest.json"/>
<meta name="msapplication-TileImage" content="images/mstile-150x150.png"/>
<meta name="msapplication-TileColor" content="#15a9ce"/>
<meta name="theme-color" content="#15a9ce"/>
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) start -->
<script src="https://optanon.blob.core.windows.net/consent/159bb13d-6748-4399-806e-ac28db879785.js" type="text/javascript" charset="UTF-8"></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) end -->
<!--Google Analytics-->
<script type="text/plain" class="optanon-category-2">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', '']);
_gaq.push(['_setDomainName', '']);
_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})()
</script>
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-2">
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KBJGH35');
</script>
<!--Marketo-->
<script type="text/plain" class="optanon-category-3">
(function() {
var didInit = false;
function initMunchkin() {
if(didInit === false) {
didInit = true;
Munchkin.init('558-NCX-702', { 'asyncOnly': true, 'disableClickDelay': true });
}
}
var s = document.createElement('script');
s.type = 'text/javascript';
s.async = true;
s.src = '//munchkin.marketo.net/munchkin.js';
s.onreadystatechange = function() {
if (this.readyState == 'complete' || this.readyState == 'loaded') {
initMunchkin();
}
};
s.onload = initMunchkin;
document.getElementsByTagName('head')[0].appendChild(s);
})();
</script>
</head>

<body id="underlay" data-toggler="nav-open">
<div id="lightbend-banner" class="lightbend-banner akka full-width" data-category="OSS Lightbend Banner Impression" data-label="Akka Banner Impression">
<div class="wrapper">
<div class="brand">
<a class="oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Lightbend Logo - Akka Banner" href="https://www.lightbend.com/lightbend-platform" target="_blank">
<svg class="lightbend-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 372 80">
<title>Lightbend</title>
<g id="lightbend-reverse">
<path d="M1,59V76a3,3,0,0,0,3,3H88a3,3,0,0,0,3-3V59a167.38,167.38,0,0,1-45,6A167.38,167.38,0,0,1,1,59Z" fill="#fff" />
<path d="M88,1H4A3,3,0,0,0,1,4V53c14.57,4.2,29.65,7,45,7s30.43-2.8,45-7V4A3,3,0,0,0,88,1Z" fill="#fff" />
<g id="original_weight" data-name="original weight">
<path d="M107.2,20.08a2.14,2.14,0,1,1,4.27,0v32h13.6A2,2,0,0,1,127,54a1.94,1.94,0,0,1-1.94,2H109.31a2.15,2.15,0,0,1-2.11-2.16Z" fill="#fff" />
<path d="M135,19.32a2.47,2.47,0,0,1,4.91,0V20A2.47,2.47,0,0,1,135,20Zm.38,10.59a2.08,2.08,0,1,1,4.16,0V54.15a2,2,0,0,1-2.06,2.11,2.08,2.08,0,0,1-2.1-2.11Z" fill="#fff" />
<path d="M150.8,61.44a1.91,1.91,0,0,1-1.08-1.73,2,2,0,0,1,1.89-1.83,1.69,1.69,0,0,1,.92.27,17.63,17.63,0,0,0,9.88,3c6.15,0,10.15-3.4,10.15-9.93v-3.3c-2.43,3.24-5.83,5.89-11,5.89a12.9,12.9,0,0,1-13.12-13.07v-.11a13.21,13.21,0,0,1,24-7.56V29.91a2.07,2.07,0,0,1,2.06-2.11,2.11,2.11,0,0,1,2.1,2.11V51.13c0,4.32-1.29,7.61-3.56,9.88-2.49,2.48-6.21,3.73-10.64,3.73A21.87,21.87,0,0,1,150.8,61.44Zm21.87-20.73V40.6c0-5.72-5-9.45-10.26-9.45s-9.67,3.67-9.67,9.4v.1a9.45,9.45,0,0,0,9.67,9.51C167.7,50.16,172.67,46.32,172.67,40.71Z" fill="#fff" />
<path d="M186.22,18.41a2.08,2.08,0,1,1,4.16,0V32.93a10.57,10.57,0,0,1,9.56-5.45c6.75,0,10.69,4.53,10.69,11.18V54.15a2.08,2.08,0,1,1-4.16,0V39.68c0-5.18-2.81-8.42-7.72-8.42s-8.37,3.51-8.37,8.75V54.15a2,2,0,0,1-2,2.11,2.08,2.08,0,0,1-2.11-2.11Z" fill="#fff" />
<path d="M220.46,48.59V31.74h-2.27a1.89,1.89,0,0,1-1.84-1.83,1.86,1.86,0,0,1,1.84-1.84h2.27V21.48a2.06,2.06,0,0,1,2-2.1,2.14,2.14,0,0,1,2.1,2.1v6.59h7.24a1.91,1.91,0,0,1,1.89,1.84,1.86,1.86,0,0,1-1.89,1.83h-7.24V48.05c0,3.4,1.89,4.64,4.7,4.64a12,12,0,0,0,2.54-.37,1.8,1.8,0,0,1,1.78,1.78,1.73,1.73,0,0,1-1.19,1.62,10.57,10.57,0,0,1-4.1.75C223.86,56.47,220.46,54.26,220.46,48.59Z" fill="#fff" />
<path d="M242.65,18.41a2.08,2.08,0,1,1,4.16,0V33.69c2.27-3.35,5.56-6.21,10.69-6.21,6.7,0,13.34,5.29,13.34,14.47v.11c0,9.12-6.59,14.52-13.34,14.52a12.6,12.6,0,0,1-10.69-5.94v3.51a2.07,2.07,0,0,1-2.05,2.11,2.11,2.11,0,0,1-2.11-2.11Zm23.92,23.7V42c0-6.58-4.53-10.8-9.83-10.8A10.41,10.41,0,0,0,246.65,42v.11c0,6.48,4.91,10.8,10.09,10.8C262.14,52.86,266.57,48.86,266.57,42.11Z" fill="#fff" />
<path d="M290.17,56.64c-7.67,0-13.93-5.89-13.93-14.53V42c0-8,5.67-14.52,13.39-14.52,8.26,0,13,6.75,13,14.15a1.94,1.94,0,0,1-1.95,1.94H280.45c.59,6,4.86,9.45,9.83,9.45a11.4,11.4,0,0,0,8-3.24,1.83,1.83,0,0,1,1.19-.49,1.81,1.81,0,0,1,1.84,1.78,1.78,1.78,0,0,1-.65,1.35A14.2,14.2,0,0,1,290.17,56.64Zm8.26-16.15c-.43-5.07-3.35-9.5-8.91-9.5-4.86,0-8.53,4.05-9.07,9.5Z" fill="#fff" />
<path d="M309.5,29.91a2.08,2.08,0,1,1,4.16,0v3a10.57,10.57,0,0,1,9.56-5.45c6.75,0,10.69,4.53,10.69,11.18V54.15a2.08,2.08,0,1,1-4.16,0V39.68c0-5.18-2.81-8.42-7.72-8.42s-8.37,3.51-8.37,8.75V54.15a2,2,0,0,1-2.05,2.11,2.08,2.08,0,0,1-2.11-2.11Z" fill="#fff" />
<path d="M368.68,54.15a2.08,2.08,0,1,1-4.15,0V50.37c-2.27,3.35-5.57,6.21-10.7,6.21-6.69,0-13.33-5.29-13.33-14.47V42c0-9.12,6.64-14.52,13.33-14.52a12.61,12.61,0,0,1,10.7,5.94v-15a2,2,0,0,1,2.05-2.11,2.07,2.07,0,0,1,2.1,2.11ZM344.76,42v.11c0,6.58,4.59,10.8,9.83,10.8a10.43,10.43,0,0,0,10.1-10.8V42a10.38,10.38,0,0,0-10.1-10.75C349.19,31.2,344.76,35.2,344.76,42Z" fill="#fff" />
</g>
</g>
</svg>
</a>
</div>
<div class="nav">
<a class="banner-btn oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="See how Akka fits into Lightbend Platform [Button]" href="https://www.lightbend.com/akka-part-of-lightbend-platform" target="_blank">
<span>See how Akka fits into</span>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 274 80">
<title>Lightbend Platform</title>
<g id="lightbend-platform-reverse">
<g id="icon">
<path d="M1,59V76a3,3,0,0,0,3,3H88a3,3,0,0,0,3-3V59a167.38,167.38,0,0,1-45,6A167.38,167.38,0,0,1,1,59Z" fill="#fff" />
<path d="M88,1H4A3,3,0,0,0,1,4V53c14.57,4.2,29.65,7,45,7s30.43-2.8,45-7V4A3,3,0,0,0,88,1Z" fill="#fff" />
</g>
<g id="lightbend">
<path d="M104,14.73a1,1,0,1,1,2,0V29.4h6.24a.91.91,0,0,1,.89.92.88.88,0,0,1-.89.89H105a1,1,0,0,1-1-1Z" fill="#fff" />
<path d="M116.77,14.39a1.14,1.14,0,0,1,2.26,0v.32a1.14,1.14,0,0,1-2.26,0Zm.18,4.85a1,1,0,1,1,1.9,0V30.37a.93.93,0,0,1-.94,1,1,1,0,0,1-1-1Z" fill="#fff" />
<path d="M124,33.72a.89.89,0,0,1-.5-.8.91.91,0,0,1,.87-.84.8.8,0,0,1,.42.12,8.05,8.05,0,0,0,4.54,1.39C132.16,33.59,134,32,134,29V27.52a6,6,0,0,1-5,2.7,5.92,5.92,0,0,1-6-6v-.05a6.07,6.07,0,0,1,11-3.47V19.24a.94.94,0,0,1,.94-1,1,1,0,0,1,1,1V29a6.18,6.18,0,0,1-1.64,4.54,6.73,6.73,0,0,1-4.88,1.71A10,10,0,0,1,124,33.72Zm10-9.52v-.05a4.49,4.49,0,0,0-4.7-4.34,4.24,4.24,0,0,0-4.44,4.32v.05a4.34,4.34,0,0,0,4.44,4.36A4.52,4.52,0,0,0,134,24.2Z" fill="#fff" />
<path d="M140.26,14a1,1,0,1,1,1.91,0v6.66a4.84,4.84,0,0,1,4.39-2.5c3.1,0,4.9,2.08,4.9,5.13v7.11a1,1,0,1,1-1.9,0V23.73c0-2.38-1.29-3.87-3.55-3.87a3.77,3.77,0,0,0-3.84,4v6.49a.93.93,0,0,1-.94,1,1,1,0,0,1-1-1Z" fill="#fff" />
<path d="M156,27.82V20.09h-1a.87.87,0,0,1-.84-.85.85.85,0,0,1,.84-.84h1v-3a1,1,0,0,1,.94-1,1,1,0,0,1,1,1v3h3.32a.87.87,0,0,1,.87.84.85.85,0,0,1-.87.85h-3.32v7.48A1.9,1.9,0,0,0,160,29.7a5.31,5.31,0,0,0,1.16-.17.82.82,0,0,1,.82.82.79.79,0,0,1-.54.74,4.9,4.9,0,0,1-1.89.35A3.26,3.26,0,0,1,156,27.82Z" fill="#fff" />
<path d="M166.16,14a1,1,0,1,1,1.91,0v7A5.75,5.75,0,0,1,173,18.13c3.07,0,6.12,2.43,6.12,6.64v.05c0,4.19-3,6.67-6.12,6.67a5.78,5.78,0,0,1-4.91-2.73v1.61a1,1,0,0,1-.94,1,1,1,0,0,1-1-1Zm11,10.88V24.8c0-3-2.08-5-4.51-5A4.78,4.78,0,0,0,168,24.77v.05a4.78,4.78,0,0,0,4.64,5C175.11,29.78,177.14,27.94,177.14,24.85Z" fill="#fff" />
<path d="M188,31.51a6.37,6.37,0,0,1-6.4-6.66V24.8c0-3.7,2.61-6.67,6.15-6.67,3.79,0,5.95,3.1,5.95,6.49a.89.89,0,0,1-.89.89h-9.27A4.47,4.47,0,0,0,188,29.85a5.23,5.23,0,0,0,3.69-1.49.85.85,0,0,1,.54-.22.81.81,0,0,1,.55,1.44A6.52,6.52,0,0,1,188,31.51Zm3.79-7.41c-.2-2.33-1.54-4.36-4.09-4.36-2.23,0-3.91,1.86-4.16,4.36Z" fill="#fff" />
<path d="M196.84,19.24a1,1,0,1,1,1.91,0v1.39a4.84,4.84,0,0,1,4.38-2.5c3.1,0,4.91,2.08,4.91,5.13v7.11a1,1,0,1,1-1.91,0V23.73c0-2.38-1.29-3.87-3.54-3.87a3.77,3.77,0,0,0-3.84,4v6.49a.93.93,0,0,1-.94,1,1,1,0,0,1-1-1Z" fill="#fff" />
<path d="M224,30.37a1,1,0,1,1-1.91,0V28.64a5.77,5.77,0,0,1-4.9,2.85,6.25,6.25,0,0,1-6.13-6.64V24.8a6.29,6.29,0,0,1,6.13-6.67,5.78,5.78,0,0,1,4.9,2.73V14a.93.93,0,0,1,.94-1,1,1,0,0,1,1,1Zm-11-5.6v.05a4.64,4.64,0,0,0,4.51,5,4.79,4.79,0,0,0,4.64-5v-.05a4.76,4.76,0,0,0-4.64-4.93C215.05,19.84,213,21.67,213,24.77Z" fill="#fff" />
</g>
<path d="M103.36,41H114.8c6.68,0,10.72,4,10.72,9.68v.08c0,6.48-5,9.84-11.32,9.84h-4.68V69h-6.16Zm11,14.12c3.08,0,4.88-1.84,4.88-4.24V50.8c0-2.76-1.92-4.24-5-4.24h-4.76v8.56Z" fill="#fff" />
<path d="M128.89,39.8H135V69h-6.08Z" fill="#fff" />
<path d="M138.75,62.84v-.08c0-4.68,3.56-6.84,8.64-6.84a15.23,15.23,0,0,1,5.24.88v-.36c0-2.52-1.56-3.92-4.6-3.92a15.77,15.77,0,0,0-5.92,1.16L140.59,49a18.92,18.92,0,0,1,8.32-1.72c3.32,0,5.72.88,7.24,2.4s2.32,4,2.32,6.84V69h-5.88V66.68a8.24,8.24,0,0,1-6.48,2.72C142.07,69.4,138.75,67.08,138.75,62.84Zm14-1.4V60.36a9.43,9.43,0,0,0-3.88-.8c-2.6,0-4.2,1-4.2,3v.08c0,1.64,1.36,2.6,3.32,2.6C150.79,65.2,152.71,63.64,152.71,61.44Z" fill="#fff" />
<path d="M164,62.92V52.76h-2.56v-5.2H164V42.08h6.08v5.48h5v5.2h-5v9.16c0,1.4.6,2.08,2,2.08a6.18,6.18,0,0,0,3-.76v4.88a9,9,0,0,1-4.8,1.24C166.51,69.36,164,67.88,164,62.92Z" fill="#fff" />
<path d="M180.43,52.76h-2.52v-5h2.52V46.4a7,7,0,0,1,1.72-5.19,6.63,6.63,0,0,1,4.92-1.68,13.5,13.5,0,0,1,4.32.59v5a7.7,7.7,0,0,0-2.76-.52c-1.4,0-2.2.72-2.2,2.32v.84h4.92v5h-4.84V69h-6.08Z" fill="#fff" />
<path d="M192.84,58.4v-.08c0-6.16,5-11.16,11.64-11.16S216,52.08,216,58.24v.08c0,6.16-5,11.16-11.64,11.16S192.84,64.56,192.84,58.4Zm17.2,0v-.08a5.7,5.7,0,0,0-5.64-5.92c-3.48,0-5.56,2.68-5.56,5.84v.08a5.7,5.7,0,0,0,5.64,5.92C208,64.24,210,61.56,210,58.4Z" fill="#fff" />
<path d="M219.89,47.56H226v4.32c1.24-3,3.24-4.88,6.84-4.72v6.36h-.32c-4,0-6.52,2.44-6.52,7.56V69h-6.08Z" fill="#fff" />
<path d="M235.8,47.56h6.08v3a7.66,7.66,0,0,1,6.32-3.44,6.42,6.42,0,0,1,6,3.4,8.83,8.83,0,0,1,7-3.4c4.52,0,7.24,2.72,7.24,7.88V69h-6.08V57c0-2.88-1.28-4.36-3.56-4.36S255.2,54.16,255.2,57V69h-6.08V57c0-2.88-1.28-4.36-3.56-4.36s-3.68,1.48-3.68,4.36V69H235.8Z" fill="#fff" />
</g>
</svg>
</a>
<div class="drop-down">
<svg class="svg-chevon-circle-down" version="1.1" id="Chevron_circled_down" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 20 20" enable-background="new 0 0 20 20" xml:space="preserve">
<path fill="#ffffff" d="M12.505,8.698L10,11L7.494,8.698c-0.198-0.196-0.518-0.196-0.718,0c-0.197,0.196-0.197,0.515,0,0.71l2.864,2.807
c0.199,0.196,0.52,0.196,0.717,0l2.864-2.807c0.199-0.195,0.198-0.514,0-0.71C13.024,8.502,12.704,8.502,12.505,8.698z M10,0.4
c-5.302,0-9.6,4.298-9.6,9.6c0,5.303,4.298,9.6,9.6,9.6s9.6-4.297,9.6-9.6C19.6,4.698,15.302,0.4,10,0.4z M10,18.354
c-4.615,0-8.354-3.74-8.354-8.354c0-4.614,3.739-8.354,8.354-8.354c4.613,0,8.354,3.74,8.354,8.354
C18.354,14.614,14.613,18.354,10,18.354z" />
</svg>
<div class="drop-down-content">
<div class="lightbend-family">
<a href="https://www.lagomframework.com" class="lagom oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Lagom - Logo Tag Line - Akka Banner">
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 752 192">
<title>Lagom Framework</title>
<g>
<g id="Lagom">
<path d="M291.58,31.79h19.83v87.92c0,8.1,2.65,11.57,9.92,11.57a21.37,21.37,0,0,0,5.95-.66V148.3a35.73,35.73,0,0,1-9.42,1c-17.52,0-26.28-8.92-26.28-26.94Z" fill="#652b7c" />
<path d="M398.34,75.75V63.69h19.83v58.17c0,7.1,1.65,9.75,6.11,9.75,1.16,0,2.64-.17,4.13-.33v16a28.83,28.83,0,0,1-9.75,1.32c-4.79,0-8.59-.83-11.57-2.65a15.32,15.32,0,0,1-6.77-10.08C394.53,144.66,385,149,371.73,149A39.59,39.59,0,0,1,342,136.4c-7.93-8.43-11.89-18.67-11.89-31.07s4-22.64,11.89-30.9a39.59,39.59,0,0,1,29.75-12.56C385.12,61.87,395.2,67.82,398.34,75.75Zm-5.95,47.76a24.32,24.32,0,0,0,7.43-18.18,24.34,24.34,0,0,0-7.43-18.18,24.71,24.71,0,0,0-18-7.27,23.94,23.94,0,0,0-17.68,7.27c-4.63,4.8-6.94,10.91-6.94,18.18s2.31,13.39,6.94,18.18a23.94,23.94,0,0,0,17.68,7.27A24.71,24.71,0,0,0,392.39,123.51Z" fill="#652b7c" />
<path d="M495.67,63.69H515.5v69.4c0,20-2.81,32.23-14.05,41-7.93,6.11-17.68,9.25-29.41,9.25-15.54,0-28.92-4-40.32-12.06l9.58-14.71a50.5,50.5,0,0,0,28.59,9.09q11.66,0,18.34-5c5.29-3.8,7.94-10.91,7.94-21.15v-4.13c-4,7.1-13.89,12.23-27.11,12.23a40.51,40.51,0,0,1-29.74-12.23c-7.93-8.1-11.9-18.35-11.9-30.41s4-22.31,12.06-30.57a39.23,39.23,0,0,1,29.58-12.56c13.72,0,23.8,5.78,26.61,13.88ZM489.72,123a24.16,24.16,0,0,0,7.44-18,23.84,23.84,0,0,0-7.44-17.85,24.69,24.69,0,0,0-18-7.27A23.92,23.92,0,0,0,454,87.15,24.7,24.7,0,0,0,447.09,105,25,25,0,0,0,454,123a23.75,23.75,0,0,0,17.68,7.11C478.81,130.12,484.93,127.81,489.72,123Z" fill="#652b7c" />
<path d="M522.44,105.33a42.06,42.06,0,0,1,13.22-31.4c8.76-8.43,19.5-12.72,32.22-12.72s23.47,4.29,32.23,12.72a42.06,42.06,0,0,1,13.22,31.4,42,42,0,0,1-13.22,31.4c-8.76,8.43-19.5,12.72-32.23,12.72s-23.46-4.29-32.22-12.72A42,42,0,0,1,522.44,105.33ZM586.23,124a25.68,25.68,0,0,0,7.43-18.68,25,25,0,0,0-7.43-18.51,26.13,26.13,0,0,0-36.85,0,25.58,25.58,0,0,0-7.27,18.51A26.25,26.25,0,0,0,549.38,124a26.56,26.56,0,0,0,36.85,0Z" fill="#652b7c" />
<path d="M620.6,147V63.69h19.67v11.9C643.74,67.49,652,62,662.58,62,674,62,681.91,66.83,686,76.58,690.84,66.83,699.76,62,712.81,62c18,0,28.43,12.72,28.43,33.38v26.27c0,6.45,1.15,8.76,5.62,8.76a15,15,0,0,0,2.81-.33v16.69a28.48,28.48,0,0,1-8.6,1c-13.05,0-19.5-7.27-19.5-21.81V98.23c0-11.41-5.28-18.51-14.54-18.51s-16,8.09-16,19.33V147H671.34V98.23c0-11.41-5.46-18.51-14.88-18.51-9.25,0-16.19,8.09-16.19,19.33V147Z" fill="#652b7c" />
</g>
<g id="Icon">
<path d="M261,84l-70,34,70,34S260.78,84.27,261,84Z" fill="#652b7c" />
<polygon points="121 152 191 118 121 84 121 152" fill="#652b7c" />
<path d="M191,118l70,34c-.27,0-41.76,25-60.63,36.24a17.63,17.63,0,0,1-18,0C163.47,177,121,152,121,152Z" fill="#421540" />
<path d="M200.23,47.65C219.09,58.88,261,84,261,84l-70,34L121,84c.27,0,42.31-25.12,61.18-36.36A17.67,17.67,0,0,1,200.23,47.65Z" fill="#bf97c6" />
<path d="M145,20.58l-35,17,35,17S144.89,20.72,145,20.58Z" fill="#652b7c" />
<polygon points="75 54.58 110 37.58 75 20.58 75 54.58" fill="#652b7c" />
<path d="M110,37.58l35,17c-.14,0-20.88,12.5-30.31,18.12a8.81,8.81,0,0,1-9,0C96.23,67.08,75,54.58,75,54.58Z" fill="#421540" />
<path d="M114.61,2.4C124,8,145,20.58,145,20.58l-35,17-35-17c.14,0,21.16-12.56,30.59-18.18A8.84,8.84,0,0,1,114.61,2.4Z" fill="#bf97c6" />
<path d="M101,91,51,115.54l50,24.53S100.84,91.21,101,91Z" fill="#652b7c" />
<polygon points="1 140.07 51 115.54 1 91.02 1 140.07" fill="#652b7c" />
<path d="M51,115.54l50,24.53c-.19,0-29.83,18-43.3,26.14a12.48,12.48,0,0,1-12.89,0C31.34,158.1,1,140.07,1,140.07Z" fill="#421540" />
<path d="M57.59,64.79C71.06,72.9,101,91,101,91L51,115.54,1,91c.19,0,30.22-18.12,43.7-26.23A12.48,12.48,0,0,1,57.59,64.79Z" fill="#bf97c6" />
</g>
</g>
</svg>
<span>Opinionated Microservices Framework</span>
</a>
<a href="https://www.playframework.com" class="play oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Play - Logo Tag Line - Akka Banner">
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 517 270">
<title>Play Framework</title>
<path d="M36.92,26.44l6.19-6.16a23,23,0,0,1,33.68,1.3l85.77,100a23,23,0,0,1-.22,30.17l-85.5,96.91a23,23,0,0,1-33.49,1.05l-6.17-6.17a23,23,0,0,1-1.4-31l51.16-61.39a23,23,0,0,0,.29-29.08l-52-65A23,23,0,0,1,36.92,26.44Z" fill="#92d13d" />
<path d="M86.94,151.18,35.78,212.57a23,23,0,0,0,1.4,31l6.17,6.17a23,23,0,0,0,33.49-1.05l18.51-21a101.68,101.68,0,0,0,10.89-45.91A101.62,101.62,0,0,0,90,126.54,23,23,0,0,1,86.94,151.18Z" fill="#49691f" />
<path d="M206.64,209.51H186.72V93.81h19.92v12.12c2.82-8.13,13-13.94,26.72-13.94,11.79,0,21.75,4.15,29.72,12.61,8.13,8.3,12.12,18.59,12.12,31s-4,22.75-12.12,31.21c-8,8.3-17.93,12.45-29.72,12.45-13.77,0-23.9-5.81-26.72-13.94Zm5.81-92.13a26.46,26.46,0,0,0,0,36.52,25.42,25.42,0,0,0,18.26,7.31,23.36,23.36,0,0,0,17.59-7.31c4.82-4.81,7.14-11,7.14-18.26s-2.32-13.44-7.14-18.26a23.39,23.39,0,0,0-17.59-7.3A25.46,25.46,0,0,0,212.45,117.38Z" fill="#49691f" />
<path d="M282,61.77h19.92v88.32c0,8.13,2.66,11.62,10,11.62a21.11,21.11,0,0,0,6-.67V178.8a35.53,35.53,0,0,1-9.46,1c-17.6,0-26.4-9-26.4-27.06Z" fill="#49691f" />
<path d="M385.25,105.93V93.81h19.92v58.43c0,7.14,1.66,9.8,6.14,9.8,1.17,0,2.66-.17,4.16-.33v16.1a28.82,28.82,0,0,1-9.8,1.33c-4.81,0-8.63-.83-11.62-2.66a15.41,15.41,0,0,1-6.81-10.13c-5.8,8.8-15.43,13.12-28.71,13.12a39.74,39.74,0,0,1-29.88-12.62c-8-8.46-12-18.76-12-31.21s4-22.74,12-31A39.77,39.77,0,0,1,358.53,92C372,92,382.1,98,385.25,105.93Zm-6,48a24.39,24.39,0,0,0,7.47-18.26,24.42,24.42,0,0,0-7.47-18.26,24.82,24.82,0,0,0-18.1-7.3,24.05,24.05,0,0,0-17.76,7.3c-4.65,4.82-7,11-7,18.26s2.32,13.45,7,18.26a24,24,0,0,0,17.76,7.31A24.79,24.79,0,0,0,379.28,153.9Z" fill="#49691f" />
<path d="M422.36,194.24a29.76,29.76,0,0,0,9.29,1.33,13.13,13.13,0,0,0,7.31-1.83c1.82-1.16,3.48-3.65,4.81-7.13l2.16-6-35-86.82H431l24.57,63.08,23.07-63.08h20.09L461.2,191.09q-3.74,9.71-9,14.94c-5,4.65-11.62,7-20.09,7a34.91,34.91,0,0,1-9.79-1.33Z" fill="#49691f" />
</svg>
<span>High velocity<br> web framework</span>
</a>
<a href="https://www.scala-lang.org" class="scala oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Scala - Logo Tag Line - Akka Banner">
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 590 270">
<title>Scala</title>
<path d="M200.7,142.39c6,11.79,15.6,17.6,29.05,17.6,14.44,0,19.59-7.64,19.59-15.11,0-5.15-1.83-8.63-6.64-11.79-4.82-3.32-8.3-4.81-16.93-8-10.63-4-16.77-7-23.41-12.29-6.64-5.48-9.79-13-9.79-22.74a28.28,28.28,0,0,1,10.29-22.58c7-5.81,15.44-8.63,25.56-8.63,15.77,0,27.72,6.31,35.69,18.76L249.34,87.78c-4.48-6.81-11.29-10.3-20.59-10.3-9.13,0-15.77,5.15-15.77,12.29,0,4.81,2,7.14,4.82,10,1.82,1.33,6.47,3.32,8.63,4.48l6,2.32,6.8,2.66c11,4.48,18.76,9.3,23.57,14.44s7.31,12.12,7.31,20.75c0,20.42-14.11,34.2-40.51,34.2-21.41,0-37.18-10-44.48-26.4Z" fill="#380d09" />
<path d="M354.25,104.71,342,117.49a28.14,28.14,0,0,0-21.24-9.13,25,25,0,0,0-18.43,7.47,27.76,27.76,0,0,0,0,37.52,25,25,0,0,0,18.43,7.47A28.14,28.14,0,0,0,342,151.69l12.29,12.78c-9,9.63-20.09,14.44-33.53,14.44-12.79,0-23.58-4.15-32.37-12.62s-13.12-19.09-13.12-31.7,4.32-23.08,13.12-31.54,19.58-12.78,32.37-12.78C334.16,90.27,345.28,95.08,354.25,104.71Z" fill="#380d09" />
<path d="M393.88,125.62C408,124.3,413,122.47,413,116c0-5.15-4.64-9.13-13.94-9.13q-13.44,0-22.41,10.95l-12.28-10.46c8.13-11.45,19.58-17.09,34.36-17.09,20.75,0,33.7,10,33.7,27.05v37c0,5.81,2.15,6.48,7,6.48h.5v15.43c-2,1.17-5.15,1.83-9.3,1.83-4.48,0-8-1.33-10.62-4a14.06,14.06,0,0,1-3-5.48c-5.81,6.8-15.27,10.29-28.39,10.29-18.42,0-30.87-10.13-30.87-25.4C357.7,136.41,369.15,127.78,393.88,125.62ZM391.56,162c13.28,0,21.41-6,21.41-16.6v-9.3a9.75,9.75,0,0,1-4.14,2.49c-3.82,1.33-6.31,1.66-14.28,2.49-11.62,1.33-17.43,5-17.43,10.79C377.12,158.33,382.43,162,391.56,162Z" fill="#380d09" />
<path d="M444.84,60.88h19.92V149.2c0,8.13,2.66,11.62,10,11.62a21.15,21.15,0,0,0,6-.67v17.76a35.56,35.56,0,0,1-9.47,1c-17.59,0-26.39-9-26.39-27.06Z" fill="#380d09" />
<path d="M521.71,125.62c14.11-1.32,19.09-3.15,19.09-9.62,0-5.15-4.64-9.13-13.94-9.13q-13.44,0-22.41,10.95l-12.28-10.46c8.13-11.45,19.58-17.09,34.36-17.09,20.75,0,33.7,10,33.7,27.05v37c0,5.81,2.15,6.48,7,6.48h.5v15.43c-2,1.17-5.15,1.83-9.3,1.83-4.48,0-8-1.33-10.62-4a13.94,13.94,0,0,1-3-5.48c-5.81,6.8-15.27,10.29-28.39,10.29-18.42,0-30.87-10.13-30.87-25.4C485.53,136.41,497,127.78,521.71,125.62ZM519.39,162c13.28,0,21.41-6,21.41-16.6v-9.3a9.73,9.73,0,0,1-4.15,2.49c-3.81,1.33-6.3,1.66-14.27,2.49-11.62,1.33-17.43,5-17.43,10.79C505,158.33,510.26,162,519.39,162Z" fill="#380d09" />
<path d="M30.55,94.83C32.4,97.38,48,102.19,71.27,107.2c23.27,4.46,47.47,22.07,66.29,16.64,12.73-3.68,26.54-36.47,26.54-41.34V82c0-3.4-2.55-6.13-6.88-8.4-17.75-9.07-21.11-12.41-27.69-10.6C95.37,72.43,35.06,67.61,30.55,94.83Z" fill="#380d09" fill-rule="evenodd" />
<path d="M30.55,161.41C32.4,164,48,168.77,71.27,173.79c26,4.74,48.61,20.19,67.44,14.75,12.73-3.68,25.39-34.58,25.39-39.46v-.48c0-3.39-2.55-6.13-6.88-8.39-13.54-7.2-31.43-15.13-38-13.32C85,136.3,39.26,138.37,30.55,161.41Z" fill="#380d09" fill-rule="evenodd" />
<path d="M30.36,109.14v.48h0A3.73,3.73,0,0,1,30.36,109.14Z" fill="#555" fill-rule="evenodd" />
<path d="M138.66,28.78C107.2,37.87,57.29,43,30.4,43h0V94.35a.8.8,0,0,0,.19.48c18.35,0,75-6,109.18-15.4a129,129,0,0,0,17.49-5.81c4.18-1.88,6.88-3.86,6.88-5.92V15.91C164.1,20.79,151.39,25.11,138.66,28.78Z" fill="#de3423" fill-rule="evenodd" />
<path d="M138.66,95.37c-18.83,5.43-44.24,9.47-67.39,11.83-15.54,1.59-30.06,2.42-40.87,2.42h0v51.31a.8.8,0,0,0,.19.48c18.35,0,75-6,109.18-15.39a130.38,130.38,0,0,0,17.49-5.81c4.18-1.89,6.88-3.86,6.88-5.92V82.5C164.1,87.37,151.39,91.69,138.66,95.37Z" fill="#de3423" fill-rule="evenodd" />
<path d="M138.66,162c-18.83,5.43-44.24,9.46-67.39,11.83-15.56,1.59-30.1,2.42-40.91,2.42V228c18.16,0,75.1-5.95,109.37-15.39,12.63-3.48,24.37-7.44,24.37-11.74V149.08C164.1,154,151.39,158.28,138.66,162Z" fill="#de3423" fill-rule="evenodd" />
</svg>
<span>The JVM language<br> of pragmatism</span>
</a>
<div class="akka">
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 658 270">
<title>Akka</title>
<g id="akka-full-color">
<path d="M349.64,105.46V93.34h19.92v58.44c0,7.13,1.66,9.79,6.14,9.79,1.17,0,2.66-.17,4.15-.33v16.1a28.71,28.71,0,0,1-9.79,1.33c-4.81,0-8.63-.83-11.62-2.66a15.41,15.41,0,0,1-6.81-10.12C345.82,174.68,336.2,179,322.92,179A39.74,39.74,0,0,1,293,166.38c-8-8.46-12-18.75-12-31.2s4-22.75,12-31.05a39.77,39.77,0,0,1,29.88-12.61C336.36,91.52,346.49,97.49,349.64,105.46Zm-6,48a24.42,24.42,0,0,0,7.47-18.26,24.39,24.39,0,0,0-7.47-18.26,24.79,24.79,0,0,0-18.1-7.31,24,24,0,0,0-17.76,7.31c-4.65,4.81-7,11-7,18.26s2.32,13.44,7,18.26a24,24,0,0,0,17.76,7.3A24.82,24.82,0,0,0,343.67,153.44Z" fill="#0b5567" />
<path d="M388.48,177V61.31h19.76v67.56l30.87-35.53H462l-32.7,37.35L465.51,177H442.93l-26.39-33.7-8.3,9.3V177Z" fill="#0b5567" />
<path d="M470.82,177V61.31h19.75v67.56l30.88-35.53h22.91l-32.7,37.35L547.84,177H525.27l-26.4-33.7-8.3,9.3V177Z" fill="#0b5567" />
<path d="M607.87,105.46V93.34h19.92v58.44c0,7.13,1.66,9.79,6.14,9.79,1.17,0,2.66-.17,4.15-.33v16.1a28.71,28.71,0,0,1-9.79,1.33c-4.81,0-8.63-.83-11.62-2.66a15.41,15.41,0,0,1-6.81-10.12c-5.81,8.79-15.43,13.11-28.71,13.11a39.74,39.74,0,0,1-29.88-12.62c-8-8.46-12-18.75-12-31.2s4-22.75,12-31.05a39.77,39.77,0,0,1,29.88-12.61C594.59,91.52,604.72,97.49,607.87,105.46Zm-6,48a24.42,24.42,0,0,0,7.47-18.26,24.39,24.39,0,0,0-7.47-18.26,24.79,24.79,0,0,0-18.1-7.31A24,24,0,0,0,566,116.92c-4.65,4.81-7,11-7,18.26s2.32,13.44,7,18.26a24,24,0,0,0,17.76,7.3A24.82,24.82,0,0,0,601.9,153.44Z" fill="#0b5567" />
<path d="M230.26,212.82c35.88,28.67,58.91-57,1.74-72.82-48-13.29-96.33,9.5-144.66,62.74C87.34,202.74,176.67,170,230.26,212.82Z" fill="#0b5567" />
<path d="M88.08,202c34.41-35.69,91.64-75.53,144.9-60.75A46.09,46.09,0,0,1,259.9,160.6L209.48,58.78c-7.2-11.46-25.58-9.15-35.95-.26L40.29,170.07a27.4,27.4,0,0,0-1.56,40.15l0,0a27.4,27.4,0,0,0,36.51,2L88.14,202Z" fill="#15a9ce" />
</g>
</svg>
<span>Actor-based,<br> cloud native toolkit</span>
</div>
<div class="platform">
<span>From the creators of <strong>Akka</strong>, get technology enhancements, monitoring, and expert support with Lightbend Platform.</span>
<a class="oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="See Akka In Lightbend Platform [Button]" href="https://www.lightbend.com/akka-part-of-lightbend-platform" target="_blank">See Akka In Lightbend Platform</a>
</div>
</div>
<div class="title">The Lightbend Family</div>
</div>
</div>
</div>
</div>
</div>
<header class="site-header hide-for-large">
<div class="sticky-header clearfix">
<a href="http://akka.io"><img class="logo" src="images/akka-alpakka-reverse.svg"></a>

<button class="menu-icon float-right" type="button" data-toggle="underlay overlay"></button>
</div>
<div id="overlay" class="overlay-nav" data-toggler="nav-open">
<header class="nav-header">
<div class="nav-header-title">
<h1><a href="index.html">Alpakka Kafka Documentation</a></h1>
</div>
<div class="nav-header-version">
Version 1.1.0
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="nav-toc">
<ul>
  <li><a href="home.html" class="page">Overview</a></li>
  <li><a href="producer.html" class="page">Producer</a></li>
  <li><a href="consumer.html#consumer" class="active page">Consumer</a>
  <ul>
    <li><a href="consumer.html#choosing-a-consumer" class="header">Choosing a consumer</a></li>
    <li><a href="consumer.html#settings" class="header">Settings</a></li>
    <li><a href="consumer.html#offset-storage-external-to-kafka" class="header">Offset Storage external to Kafka</a></li>
    <li><a href="consumer.html#offset-storage-in-kafka-committing" class="header">Offset Storage in Kafka - committing</a></li>
    <li><a href="consumer.html#offset-storage-in-kafka-external" class="header">Offset Storage in Kafka &amp; external</a></li>
    <li><a href="consumer.html#consume-" class="header">Consume &ldquo;at-most-once&rdquo;</a></li>
    <li><a href="consumer.html#consume-" class="header">Consume &ldquo;at-least-once&rdquo;</a></li>
    <li><a href="consumer.html#connecting-producer-and-consumer" class="header">Connecting Producer and Consumer</a></li>
    <li><a href="consumer.html#source-per-partition" class="header">Source per partition</a></li>
    <li><a href="consumer.html#sharing-the-kafkaconsumer-instance" class="header">Sharing the KafkaConsumer instance</a></li>
    <li><a href="consumer.html#accessing-kafkaconsumer-metrics" class="header">Accessing KafkaConsumer metrics</a></li>
    <li><a href="consumer.html#accessing-kafkaconsumer-metadata" class="header">Accessing KafkaConsumer metadata</a></li>
    <li><a href="consumer.html#listening-for-rebalance-events" class="header">Listening for rebalance events</a></li>
    <li><a href="consumer.html#controlled-shutdown" class="header">Controlled shutdown</a></li>
    <li><a href="subscription.html" class="page">Subscription</a></li>
    <li><a href="consumer-metadata.html" class="page">Consumer Metadata</a></li>
  </ul></li>
  <li><a href="errorhandling.html" class="page">Error handling</a></li>
  <li><a href="atleastonce.html" class="page">At-Least-Once Delivery</a></li>
  <li><a href="transactions.html" class="page">Transactions</a></li>
  <li><a href="serialization.html" class="page">Serialization</a></li>
  <li><a href="debugging.html" class="page">Debugging</a></li>
  <li><a href="testing.html" class="page">Testing</a></li>
  <li><a href="production.html" class="page">Production considerations</a></li>
  <li><a href="snapshots.html" class="page">Snapshots</a></li>
</ul>
</nav>
</div>
</header>
<div class="site-content-wrapper">
<aside class="sticky-sidebar show-for-large">
<header class="nav-header sticky-sidebar-header">
<div class="nav-header-title">
<h1><a href="index.html">Alpakka Kafka Documentation</a></h1>
</div>
<div class="nav-header-version">
Version 1.1.0
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="site-nav sticky-sidebar-contents">
<div class="nav-toc">
<ul>
  <li><a href="home.html" class="page">Overview</a></li>
  <li><a href="producer.html" class="page">Producer</a></li>
  <li><a href="consumer.html#consumer" class="active page">Consumer</a>
  <ul>
    <li><a href="consumer.html#choosing-a-consumer" class="header">Choosing a consumer</a></li>
    <li><a href="consumer.html#settings" class="header">Settings</a></li>
    <li><a href="consumer.html#offset-storage-external-to-kafka" class="header">Offset Storage external to Kafka</a></li>
    <li><a href="consumer.html#offset-storage-in-kafka-committing" class="header">Offset Storage in Kafka - committing</a></li>
    <li><a href="consumer.html#offset-storage-in-kafka-external" class="header">Offset Storage in Kafka &amp; external</a></li>
    <li><a href="consumer.html#consume-" class="header">Consume &ldquo;at-most-once&rdquo;</a></li>
    <li><a href="consumer.html#consume-" class="header">Consume &ldquo;at-least-once&rdquo;</a></li>
    <li><a href="consumer.html#connecting-producer-and-consumer" class="header">Connecting Producer and Consumer</a></li>
    <li><a href="consumer.html#source-per-partition" class="header">Source per partition</a></li>
    <li><a href="consumer.html#sharing-the-kafkaconsumer-instance" class="header">Sharing the KafkaConsumer instance</a></li>
    <li><a href="consumer.html#accessing-kafkaconsumer-metrics" class="header">Accessing KafkaConsumer metrics</a></li>
    <li><a href="consumer.html#accessing-kafkaconsumer-metadata" class="header">Accessing KafkaConsumer metadata</a></li>
    <li><a href="consumer.html#listening-for-rebalance-events" class="header">Listening for rebalance events</a></li>
    <li><a href="consumer.html#controlled-shutdown" class="header">Controlled shutdown</a></li>
    <li><a href="subscription.html" class="page">Subscription</a></li>
    <li><a href="consumer-metadata.html" class="page">Consumer Metadata</a></li>
  </ul></li>
  <li><a href="errorhandling.html" class="page">Error handling</a></li>
  <li><a href="atleastonce.html" class="page">At-Least-Once Delivery</a></li>
  <li><a href="transactions.html" class="page">Transactions</a></li>
  <li><a href="serialization.html" class="page">Serialization</a></li>
  <li><a href="debugging.html" class="page">Debugging</a></li>
  <li><a href="testing.html" class="page">Testing</a></li>
  <li><a href="production.html" class="page">Production considerations</a></li>
  <li><a href="snapshots.html" class="page">Snapshots</a></li>
</ul>
</div>
</nav>
<footer class="nav-footer sticky-sidebar-footer">
<a href="http://akka.io"><img class="logo" src="images/akka-alpakka-reverse.svg"></a>

</footer>
</aside>
<main class="fixed-shift-for-large expanded row">
<section class="site-content small-12 column">
<article class="page-content row">
<div class="small-12 column" id="docs">
<h1><a href="#consumer" name="consumer" class="anchor"><span class="anchor-link"></span></a>Consumer</h1>
<p>A consumer subscribes to Kafka topics and passes the messages into an Akka Stream.</p>
<p>The underlying implementation is using the <code>KafkaConsumer</code>, see <a href="https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html" title="org.apache.kafka.clients.consumer.KafkaConsumer"><code>Kafka API</code></a> for a description of consumer groups, offsets, and other details.</p>
<h2><a href="#choosing-a-consumer" name="choosing-a-consumer" class="anchor"><span class="anchor-link"></span></a>Choosing a consumer</h2>
<p>Alpakka Kafka offers a large variety of consumers that connect to Kafka and stream data. The tables below may help you to find the consumer best suited for your use-case.</p>
<h3><a href="#consumers" name="consumers" class="anchor"><span class="anchor-link"></span></a>Consumers</h3>
<p>These factory methods are part of the <span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Consumer$.html" title="akka.kafka.scaladsl.Consumer"><code>Consumer API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Consumer$.html" title="akka.kafka.javadsl.Consumer"><code>Consumer API</code></a></span>.</p>
<table>
  <thead>
    <tr>
      <th>Offsets handling </th>
      <th>Partition aware </th>
      <th>Subscription </th>
      <th>Shared consumer </th>
      <th>Factory method </th>
      <th>Stream element type </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>No (auto commit can be enabled) </td>
      <td>No </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>plainSource</code> </td>
      <td><code>ConsumerRecord</code> </td>
    </tr>
    <tr>
      <td>No (auto commit can be enabled) </td>
      <td>No </td>
      <td>Partition </td>
      <td>Yes </td>
      <td><code>plainExternalSource</code> </td>
      <td><code>ConsumerRecord</code> </td>
    </tr>
    <tr>
      <td>Explicit committing </td>
      <td>No </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>committableSource</code> </td>
      <td><code>CommittableMessage</code> </td>
    </tr>
    <tr>
      <td>Explicit committing </td>
      <td>No </td>
      <td>Partition </td>
      <td>Yes </td>
      <td><code>committableExternalSource</code> </td>
      <td><code>CommittableMessage</code> </td>
    </tr>
    <tr>
      <td>Explicit committing with metadata </td>
      <td>No </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>commitWithMetadataSource</code> </td>
      <td><code>CommittableMessage</code> </td>
    </tr>
    <tr>
      <td>Explicit committing (with metadata) </td>
      <td>No </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>sourceWithOffsetContext</code> </td>
      <td><code>ConsumerRecord</code> </td>
    </tr>
    <tr>
      <td>Offset committed per element </td>
      <td>No </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>atMostOnceSource</code> </td>
      <td><code>ConsumerRecord</code> </td>
    </tr>
    <tr>
      <td>No (auto commit can be enabled) </td>
      <td>Yes </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>plainPartitionedSource</code> </td>
      <td><code>(TopicPartition, Source[ConsumerRecord, ..])</code> </td>
    </tr>
    <tr>
      <td>External to Kafka </td>
      <td>Yes </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>plainPartitionedManualOffsetSource</code> </td>
      <td><code>(TopicPartition, Source[ConsumerRecord, ..])</code> </td>
    </tr>
    <tr>
      <td>Explicit committing </td>
      <td>Yes </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>committablePartitionedSource</code> </td>
      <td><code>(TopicPartition, Source[CommittableMessage, ..])</code> </td>
    </tr>
    <tr>
      <td>External to Kafka &amp; Explicit Committing </td>
      <td>Yes </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>committablePartitionedManualOffsetSource</code> </td>
      <td><code>(TopicPartition, Source[CommittableMessage, ..])</code> </td>
    </tr>
    <tr>
      <td>Explicit committing with metadata </td>
      <td>Yes </td>
      <td>Topic or Partition </td>
      <td>No </td>
      <td><code>commitWithMetadataPartitionedSource</code> </td>
      <td><code>(TopicPartition, Source[CommittableMessage, ..])</code> </td>
    </tr>
  </tbody>
</table>
<h3><a href="#transactional-consumers" name="transactional-consumers" class="anchor"><span class="anchor-link"></span></a>Transactional consumers</h3>
<p>These factory methods are part of the <span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Transactional$.html" title="akka.kafka.scaladsl.Transactional"><code>Transactional API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Transactional$.html" title="akka.kafka.javadsl.Transactional"><code>Transactional API</code></a></span>. For details see <a href="transactions.html">Transactions</a>.</p>
<table>
  <thead>
    <tr>
      <th>Offsets handling </th>
      <th>Partition aware </th>
      <th>Shared consumer </th>
      <th>Factory method </th>
      <th>Stream element type </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Transactional </td>
      <td>No </td>
      <td>No </td>
      <td><code>Transactional.source</code> </td>
      <td><code>TransactionalMessage</code> </td>
    </tr>
    <tr>
      <td>Transactional </td>
      <td>No </td>
      <td>No </td>
      <td><code>Transactional.sourceWithOffsetContext</code> </td>
      <td><code>ConsumerRecord</code> </td>
    </tr>
  </tbody>
</table>
<h2><a href="#settings" name="settings" class="anchor"><span class="anchor-link"></span></a>Settings</h2>
<p>When creating a consumer source you need to pass in <code>ConsumerSettings</code> (<a href="/api/alpakka-kafka/1.1.0/akka/kafka/ConsumerSettings.html" title="akka.kafka.ConsumerSettings"><code>API</code></a>) that define things like:</p>
<ul>
  <li>de-serializers for the keys and values</li>
  <li>bootstrap servers of the Kafka cluster</li>
  <li>group id for the consumer, note that offsets are always committed for a given consumer group</li>
  <li>Kafka consumer tuning parameters</li>
</ul>
<p>Alpakka Kafka&rsquo;s defaults for all settings are defined in <code>reference.conf</code> which is included in the library JAR.</p>
<dl>
  <dt>Important consumer settings</dt>
  <dd>
  <table>
    <thead>
      <tr>
        <th>Setting </th>
        <th>Description </th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>stop-timeout </td>
        <td>The stage will delay stopping the internal actor to allow processing of messages already in the stream (required for successful committing). This can be set to 0 for streams using <code>DrainingControl</code>. </td>
      </tr>
      <tr>
        <td>kafka-clients </td>
        <td>Section for properties passed unchanged to the Kafka client (see <a href="http://kafka.apache.org/documentation/#consumerconfigs" title="Kafka&#39;s Consumer Configs" target="_blank" rel="noopener noreferrer">Kafka&rsquo;s Consumer Configs</a> ) </td>
      </tr>
      <tr>
        <td>connection-checker </td>
        <td>Configuration to let the stream fail if the connection to the Kafka broker fails. </td>
      </tr>
    </tbody>
  </table></dd>
  <dt>reference.conf (HOCON)
  </dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/core/src/main/resources/reference.conf#L31-L119" target="_blank" title="Go to snippet source"></a><code class="language-conf"># Properties for akka.kafka.ConsumerSettings can be
# defined in this section or a configuration section with
# the same layout.
akka.kafka.consumer {
  # Tuning property of scheduled polls.
  # Controls the interval from one scheduled poll to the next.
  poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that the thread that
  # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
  poll-timeout = 50ms

  # The stage will delay stopping the internal actor to allow processing of
  # messages already in the stream (required for successful committing).
  # This can be set to 0 for streams using `DrainingControl`.
  stop-timeout = 30s

  # Duration to wait for `KafkaConsumer.close` to finish.
  close-timeout = 20s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `CommitTimeoutException`.
  # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
  # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
  commit-timeout = 15s

  # If commits take longer than this time a warning is logged
  commit-time-warning = 1s

  # Not relevant for Kafka after version 2.1.0.
  # If set to a finite duration, the consumer will re-send the last committed offsets periodically
  # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
  commit-refresh-interval = infinite

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = &quot;akka.kafka.default-dispatcher&quot;

  # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Disable auto-commit by default
    enable.auto.commit = false
  }

  # Time to wait for pending requests when a partition is closed
  wait-close-partition = 500ms

  # Limits the query to Kafka for a topic&#39;s position
  position-timeout = 5s

  # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
  # call to Kafka&#39;s API
  offset-for-times-timeout = 5s

  # Timeout for akka.kafka.Metadata requests
  # This value is used instead of Kafka&#39;s default from `default.api.timeout.ms`
  # which is 1 minute.
  metadata-request-timeout = 5s

  # Interval for checking that transaction was completed before closing the consumer.
  # Used in the transactional flow for exactly-once-semantics processing.
  eos-draining-check-interval = 30ms

  # Issue warnings when a call to a partition assignment handler method takes
  # longer than this.
  partition-handler-warning = 5s

  # Settings for checking the connection to the Kafka broker. Connection checking uses `listTopics` requests with the timeout
  # configured by `consumer.metadata-request-timeout`
  connection-checker {

    #Flag to turn on connection checker
    enable = false

    # Amount of attempts to be performed after a first connection failure occurs
    # Required, non-negative integer
    max-retries = 3

    # Interval for the connection check. Used as the base for exponential retry.
    check-interval = 15s

    # Check interval multiplier for backoff interval
    # Required, positive number
    backoff-factor = 2.0
  }

}</code></pre></dd>
</dl>
<p>The Kafka documentation <a href="http://kafka.apache.org/documentation/#consumerconfigs">Consumer Configs</a> lists the settings, their defaults and importance. More detailed explanations are given in the <a href="https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html" title="org.apache.kafka.clients.consumer.KafkaConsumer"><code>KafkaConsumer API</code></a> and constants are defined in <a href="https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/ConsumerConfig.html" title="org.apache.kafka.clients.consumer.ConsumerConfig"><code>ConsumerConfig API</code></a>.</p>
<h3><a href="#programmatic-construction" name="programmatic-construction" class="anchor"><span class="anchor-link"></span></a>Programmatic construction</h3>
<p>Stream-specific settings like the de-serializers and consumer group ID should be set programmatically. Settings that apply to many consumers may be set in <code>application.conf</code> or use <a href="consumer.html#config-inheritance">config inheritance</a>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L92-L97" target="_blank" title="Go to snippet source"></a><code class="language-scala">val config = system.settings.config.getConfig(&quot;akka.kafka.consumer&quot;)
val consumerSettings =
  ConsumerSettings(config, new StringDeserializer, new ByteArrayDeserializer)
    .withBootstrapServers(bootstrapServers)
    .withGroupId(&quot;group1&quot;)
    .withProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L74-L79" target="_blank" title="Go to snippet source"></a><code class="language-java">final Config config = system.settings().config().getConfig(&quot;akka.kafka.consumer&quot;);
final ConsumerSettings&lt;String, byte[]&gt; consumerSettings =
    ConsumerSettings.create(config, new StringDeserializer(), new ByteArrayDeserializer())
        .withBootstrapServers(&quot;localhost:9092&quot;)
        .withGroupId(&quot;group1&quot;)
        .withProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);</code></pre></dd>
</dl>
<h3><a href="#config-inheritance" name="config-inheritance" class="anchor"><span class="anchor-link"></span></a>Config inheritance</h3>
<p><code>ConsumerSettings</code> (<a href="/api/alpakka-kafka/1.1.0/akka/kafka/ConsumerSettings.html" title="akka.kafka.ConsumerSettings"><code>API</code></a>) are created from configuration in <code>application.conf</code> (with defaults in <code>reference.conf</code>). The format of these settings files are described in the <a href="https://github.com/lightbend/config#using-hocon-the-json-superset">HOCON Config Documentation</a>. A recommended setup is to rely on config inheritance as below:</p>
<dl>
  <dt>application.conf (HOCON)
  </dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/resources/application.conf#L22-L26" target="_blank" title="Go to snippet source"></a><code class="language-conf">our-kafka-consumer: ${akka.kafka.consumer} {
  kafka-clients {
    bootstrap.servers = &quot;kafka-host:9092&quot;
  }
}</code></pre></dd>
</dl>
<p>Read the settings that inherit the defaults from &ldquo;akka.kafka.consumer&rdquo; settings:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L115-L116" target="_blank" title="Go to snippet source"></a><code class="language-scala">val config = system.settings.config.getConfig(&quot;our-kafka-consumer&quot;)
val consumerSettings = ConsumerSettings(config, new StringDeserializer, new StringDeserializer)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L149-L151" target="_blank" title="Go to snippet source"></a><code class="language-java">Config config = system.settings().config().getConfig(&quot;our-kafka-consumer&quot;);
ConsumerSettings&lt;String, String&gt; consumerSettings =
    ConsumerSettings.create(config, new StringDeserializer(), new StringDeserializer());</code></pre></dd>
</dl>
<h2><a href="#offset-storage-external-to-kafka" name="offset-storage-external-to-kafka" class="anchor"><span class="anchor-link"></span></a>Offset Storage external to Kafka</h2>
<p>The Kafka read offset can either be stored in Kafka (see below), or at a data store of your choice.</p>
<p><code>Consumer.plainSource</code> (<span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Consumer$.html" title="akka.kafka.scaladsl.Consumer"><code>Consumer API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Consumer$.html" title="akka.kafka.javadsl.Consumer"><code>Consumer API</code></a></span>) and <code>Consumer.plainPartitionedManualOffsetSource</code> can be used to emit <code>ConsumerRecord</code> (<a href="https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html" title="org.apache.kafka.clients.consumer.ConsumerRecord"><code>Kafka API</code></a>) elements as received from the underlying <code>KafkaConsumer</code>. They do not have support for committing offsets to Kafka. When using these Sources, either store an offset externally, or use auto-commit (note that auto-commit is disabled by default).</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L106-L108" target="_blank" title="Go to snippet source"></a><code class="language-scala">consumerSettings
  .withProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &quot;true&quot;)
  .withProperty(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;5000&quot;)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L84-L86" target="_blank" title="Go to snippet source"></a><code class="language-java">consumerSettings
    .withProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &quot;true&quot;)
    .withProperty(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;5000&quot;);</code></pre></dd>
</dl>
<p>The consumer application doesn&rsquo;t need to use Kafka&rsquo;s built-in offset storage, it can store offsets in a store of its own choosing. The primary use case for this is allowing the application to store both the offset and the results of the consumption in the same system in a way that both the results and offsets are stored atomically. This is not always possible, but when it is it will make the consumption fully atomic and give &ldquo;exactly once&rdquo; semantics that are stronger than the &ldquo;at-least-once&rdquo; semantics you get with Kafka&rsquo;s offset commit functionality.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L45-L86" target="_blank" title="Go to snippet source"></a><code class="language-scala">  val db = new OffsetStore
  val control = db.loadOffset().map { fromOffset =&gt;
    Consumer
      .plainSource(
        consumerSettings,
        Subscriptions.assignmentWithOffset(
          new TopicPartition(topic, /* partition = */ 0) -&gt; fromOffset
        )
      )
      .mapAsync(1)(db.businessLogicAndStoreOffset)
      .toMat(Sink.seq)(Keep.both)
      .mapMaterializedValue(DrainingControl.apply)
      .run()
  }

class OffsetStore {
  def businessLogicAndStoreOffset(record: ConsumerRecord[String, String]): Future[Done] = // ...
  def loadOffset(): Future[Long] = // ...
}</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L95-L143" target="_blank" title="Go to snippet source"></a><code class="language-java">  final OffsetStorage db = new OffsetStorage();

  CompletionStage&lt;Consumer.Control&gt; controlCompletionStage =
      db.loadOffset()
          .thenApply(
              fromOffset -&gt;
                  Consumer.plainSource(
                          consumerSettings,
                          Subscriptions.assignmentWithOffset(
                              new TopicPartition(topic, partition0), fromOffset))
                      .mapAsync(1, db::businessLogicAndStoreOffset)
                      .to(Sink.ignore())
                      .run(materializer));

class OffsetStorage {
  public CompletionStage&lt;Done&gt; businessLogicAndStoreOffset(
      ConsumerRecord&lt;String, String&gt; record) { // ... }
  public CompletionStage&lt;Long&gt; loadOffset() { // ... }
}</code></pre></dd>
</dl>
<p>For <code>Consumer.plainSource</code> the <code>Subscriptions.assignmentWithOffset</code> specifies the starting point (offset) for a given consumer group id, topic and partition. The group id is defined in the <code>ConsumerSettings</code>.</p>
<p>Alternatively, with <code>Consumer.plainPartitionedManualOffsetSource</code> (<span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Consumer$.html" title="akka.kafka.scaladsl.Consumer"><code>Consumer API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Consumer$.html" title="akka.kafka.javadsl.Consumer"><code>Consumer API</code></a></span>), only the consumer group id and the topic are required on creation. The starting point is fetched by calling the <code>getOffsetsOnAssign</code> function passed in by the user. This function should return a <code>Map</code> of <code>TopicPartition</code> (<a href="https://kafka.apache.org/21/javadoc/org/apache/kafka/common/TopicPartition.html" title="org.apache.kafka.common.TopicPartition"><code>API</code></a>) to <code>Long</code>, with the <code>Long</code> representing the starting point. If a consumer is assigned a partition that is not included in the <code>Map</code> that results from <code>getOffsetsOnAssign</code>, the default starting position will be used, according to the consumer configuration value <code>auto.offset.reset</code>. Also note that <code>Consumer.plainPartitionedManualOffsetSource</code> emits tuples of assigned topic-partition and a corresponding source, as in <a href="#source-per-partition">Source per partition</a>.</p>
<h2><a href="#offset-storage-in-kafka-committing" name="offset-storage-in-kafka-committing" class="anchor"><span class="anchor-link"></span></a>Offset Storage in Kafka - committing</h2>
<p>The <code>Consumer.committableSource</code> (<span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Consumer$.html" title="akka.kafka.scaladsl.Consumer"><code>Consumer API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Consumer$.html" title="akka.kafka.javadsl.Consumer"><code>Consumer API</code></a></span>) makes it possible to commit offset positions to Kafka. Compared to auto-commit this gives exact control of when a message is considered consumed.</p>
<p>This is useful when &ldquo;at-least-once&rdquo; delivery is desired, as each message will likely be delivered one time, but in failure cases could be received more than once.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L159-L177" target="_blank" title="Go to snippet source"></a><code class="language-scala">  val control =
    Consumer
      .committableSource(consumerSettings, Subscriptions.topics(topic))
      .mapAsync(10) { msg =&gt;
        business(msg.record.key, msg.record.value).map(_ =&gt; msg.committableOffset)
      }
      .via(Committer.flow(committerDefaults.withMaxBatch(1)))
      .toMat(Sink.seq)(Keep.both)
      .mapMaterializedValue(DrainingControl.apply)
      .run()

def business(key: String, value: Array[Byte]): Future[Done] = // ???
</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L174-L196" target="_blank" title="Go to snippet source"></a><code class="language-java">CompletionStage&lt;String&gt; business(String key, String value) { // .... }
  Consumer.DrainingControl&lt;Done&gt; control =
      Consumer.committableSource(consumerSettings, Subscriptions.topics(topic))
          .mapAsync(
              1,
              msg -&gt;
                  business(msg.record().key(), msg.record().value())
                      .thenApply(done -&gt; msg.committableOffset()))
          .toMat(Committer.sink(committerSettings.withMaxBatch(1)), Keep.both())
          .mapMaterializedValue(Consumer::createDrainingControl)
          .run(materializer);
</code></pre></dd>
</dl>
<p>Committing the offset for each message (<code>withMaxBatch(1)</code>) as illustrated above is rather slow. It is recommended to batch the commits for better throughput, with the trade-off that more messages may be re-delivered in case of failures.</p>
<h3><a href="#committer-sink" name="committer-sink" class="anchor"><span class="anchor-link"></span></a>Committer sink</h3>
<p>You can use a pre-defined <code>Committer.sink</code> to perform commits in batches:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L225-L236" target="_blank" title="Go to snippet source"></a><code class="language-scala">val committerSettings = CommitterSettings(system)

val control: DrainingControl[Done] =
  Consumer
    .committableSource(consumerSettings, Subscriptions.topics(topic))
    .mapAsync(1) { msg =&gt;
      business(msg.record.key, msg.record.value)
        .map(_ =&gt; msg.committableOffset)
    }
    .toMat(Committer.sink(committerSettings))(Keep.both)
    .mapMaterializedValue(DrainingControl.apply)
    .run()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L209-L220" target="_blank" title="Go to snippet source"></a><code class="language-java">CommitterSettings committerSettings = CommitterSettings.create(config);

Consumer.DrainingControl&lt;Done&gt; control =
    Consumer.committableSource(consumerSettings, Subscriptions.topics(topic))
        .mapAsync(
            1,
            msg -&gt;
                business(msg.record().key(), msg.record().value())
                    .&lt;ConsumerMessage.Committable&gt;thenApply(done -&gt; msg.committableOffset()))
        .toMat(Committer.sink(committerSettings), Keep.both())
        .mapMaterializedValue(Consumer::createDrainingControl)
        .run(materializer);</code></pre></dd>
</dl>
<p>When creating a <code>Committer.sink</code> you need to pass in <code>CommitterSettings</code> (<a href="/api/alpakka-kafka/1.1.0/akka/kafka/CommitterSettings.html" title="akka.kafka.CommitterSettings"><code>API</code></a>). These may be created by passing the actor system to read the defaults from the config section <code>akka.kafka.committer</code>, or by passing a <code>Config</code> (<a href="https://lightbend.github.io/config/latest/api/com/typesafe/config/Config.html" title="com.typesafe.config.Config"><code>API</code></a>) instance with the same structure.</p>
<dl>
  <dt>Table</dt>
  <dd>
  <table>
    <thead>
      <tr>
        <th>Setting </th>
        <th>Description </th>
        <th>Default Value </th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>maxBatch </td>
        <td>maximum number of messages to commit at once </td>
        <td>1000 </td>
      </tr>
      <tr>
        <td>maxInterval </td>
        <td>maximum interval between commits </td>
        <td>10 seconds </td>
      </tr>
      <tr>
        <td>parallelism </td>
        <td>maximum number of commit batches in flight </td>
        <td>100 </td>
      </tr>
    </tbody>
  </table></dd>
  <dt>reference.conf
  </dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/core/src/main/resources/reference.conf#L123-L142" target="_blank" title="Go to snippet source"></a><code class="language-conf"># Properties for akka.kafka.CommitterSettings can be
# defined in this section or a configuration section with
# the same layout.
akka.kafka.committer {

  # Maximum number of messages in a single commit batch
  max-batch = 1000

  # Maximum interval between commits
  max-interval = 10s

  # Parallelsim for async committing
  parallelism = 100

  # API may change.
  # Delivery of commits to the internal actor
  # WaitForAck: Expect replies for commits, and backpressure the stream if replies do not arrive.
  # SendAndForget: Send off commits to the internal actor without expecting replies (experimental feature since 1.1)
  delivery = WaitForAck
}</code></pre></dd>
</dl>
<p>All commit batches are aggregated internally and passed on to Kafka very often (in every poll cycle), the Committer settings configure how the stream sends the offsets to the internal actor which communicates with the Kafka broker. Increasing these values means that in case of a failure you may have to re-process more messages.</p>
<p>If you use Kafka older than version 2.1.0 and consume from a topic with low activity, and possibly no messages arrive for more than 24 hours, consider enabling periodical commit refresh (<code>akka.kafka.consumer.commit-refresh-interval</code> configuration parameters), otherwise offsets might expire in the Kafka storage. This has been fixed in Kafka 2.1.0 (See <a href="https://issues.apache.org/jira/browse/KAFKA-4682">KAFKA-4682</a>).</p>
<h4><a href="#committer-variants" name="committer-variants" class="anchor"><span class="anchor-link"></span></a>Committer variants</h4>
<p>These factory methods are part of the <span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Committer$.html" title="akka.kafka.scaladsl.Committer"><code>Committer API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Committer$.html" title="akka.kafka.javadsl.Committer"><code>Committer API</code></a></span>.</p>
<table>
  <thead>
    <tr>
      <th>Factory method </th>
      <th>Stream element type </th>
      <th>Emits </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>sink</code> </td>
      <td><code>Committable</code> </td>
      <td>N/A </td>
    </tr>
    <tr>
      <td><code>sinkWithOffsetContext</code> </td>
      <td>Any (<code>CommittableOffset</code> in context) </td>
      <td>N/A </td>
    </tr>
    <tr>
      <td><code>flow</code> </td>
      <td><code>Committable</code> </td>
      <td><code>Done</code> </td>
    </tr>
    <tr>
      <td><code>batchFlow</code> </td>
      <td><code>Committable</code> </td>
      <td><code>CommittableOffsetBatch</code> </td>
    </tr>
    <tr>
      <td><code>flowWithOffsetContext</code> </td>
      <td>Any (<code>CommittableOffset</code> in context) </td>
      <td><code>NotUsed</code> (<code>CommittableOffsetBatch</code> in context) </td>
    </tr>
  </tbody>
</table>
<h3><a href="#commit-with-meta-data" name="commit-with-meta-data" class="anchor"><span class="anchor-link"></span></a>Commit with meta-data</h3>
<p>The <code>Consumer.commitWithMetadataSource</code> allows you to add metadata to the committed offset based on the last consumed record.</p>
<p>Note that the first offset provided to the consumer during a partition assignment will not contain metadata. This offset can get committed due to a periodic commit refresh (<code>akka.kafka.consumer.commit-refresh-interval</code> configuration parameters) and the commit will not contain metadata.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L203-L215" target="_blank" title="Go to snippet source"></a><code class="language-scala">def metadataFromRecord(record: ConsumerRecord[String, String]): String =
  record.timestamp().toString

val control =
  Consumer
    .commitWithMetadataSource(consumerSettings, Subscriptions.topics(topic), metadataFromRecord)
    .mapAsync(1) { msg =&gt;
      business(msg.record.key, msg.record.value)
        .map(_ =&gt; msg.committableOffset)
    }
    .toMat(Committer.sink(committerDefaults))(Keep.both)
    .mapMaterializedValue(DrainingControl.apply)
    .run()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L233-L245" target="_blank" title="Go to snippet source"></a><code class="language-java">Consumer.DrainingControl&lt;Done&gt; control =
    Consumer.commitWithMetadataSource(
            consumerSettings,
            Subscriptions.topics(topic),
            (record) -&gt; Long.toString(record.timestamp()))
        .mapAsync(
            1,
            msg -&gt;
                business(msg.record().key(), msg.record().value())
                    .thenApply(done -&gt; msg.committableOffset()))
        .toMat(Committer.sink(committerSettings), Keep.both())
        .mapMaterializedValue(Consumer::createDrainingControl)
        .run(materializer);</code></pre></dd>
</dl>
<h2><a href="#offset-storage-in-kafka-external" name="offset-storage-in-kafka-external" class="anchor"><span class="anchor-link"></span></a>Offset Storage in Kafka &amp; external</h2>
<p>In some cases you may wish to use external offset storage as your primary means to manage offsets, but also commit offsets to Kafka. This gives you all the benefits of controlling offsets described in <a href="consumer.html#offset-storage-external-to-kafka">Offset Storage external to Kafka</a> and allows you to use tooling in the Kafka ecosystem to track consumer group lag. You can use the <code>Consumer.committablePartitionedManualOffsetSource</code> (<span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Consumer$.html" title="akka.kafka.scaladsl.Consumer"><code>Consumer API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Consumer$.html" title="akka.kafka.javadsl.Consumer"><code>Consumer API</code></a></span>) source, which emits a <code>CommittableMessage</code>, to seek to appropriate offsets on startup, do your processing, commit to external storage, and then commit offsets back to Kafka. This will only provide at-least-once guarantees for your consumer group lag monitoring because it&rsquo;s possible for a failure between storing your offsets externally and committing to Kafka, but it will give you a more accurate representation of consumer group lag then when turning on auto commits with the <code>enable.auto.commit</code> consumer property.</p>
<h2><a href="#consume-" name="consume-" class="anchor"><span class="anchor-link"></span></a>Consume &ldquo;at-most-once&rdquo;</h2>
<p>If you commit the offset before processing the message you get &ldquo;at-most-once&rdquo; delivery semantics, this is provided by <code>Consumer.atMostOnceSource</code>. However, <code>atMostOnceSource</code> <strong>commits the offset for each message and that is rather slow</strong>, batching of commits is recommended. If your &ldquo;at-most-once&rdquo; requirements are more relaxed, consider a <code>Consumer.plainSource</code> and enable Kafka&rsquo;s auto committing with <code>enable.auto.commit = true</code>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L133-L149" target="_blank" title="Go to snippet source"></a><code class="language-scala">  val control: DrainingControl[immutable.Seq[Done]] =
    Consumer
      .atMostOnceSource(consumerSettings, Subscriptions.topics(topic))
      .mapAsync(1)(record =&gt; business(record.key, record.value()))
      .toMat(Sink.seq)(Keep.both)
      .mapMaterializedValue(DrainingControl.apply)
      .run()

def business(key: String, value: String): Future[Done] = // ???</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L162-L174" target="_blank" title="Go to snippet source"></a><code class="language-java">  Consumer.Control control =
      Consumer.atMostOnceSource(consumerSettings, Subscriptions.topics(topic))
          .mapAsync(10, record -&gt; business(record.key(), record.value()))
          .to(Sink.foreach(it -&gt; System.out.println(&quot;Done with &quot; + it)))
          .run(materializer);

CompletionStage&lt;String&gt; business(String key, String value) { // .... }</code></pre></dd>
</dl>
<h2><a href="#consume-" name="consume-" class="anchor"><span class="anchor-link"></span></a>Consume &ldquo;at-least-once&rdquo;</h2>
<p>How to achieve at-least-once delivery semantics is covered in <a href="atleastonce.html">At-Least-Once Delivery</a>.</p>
<h2><a href="#connecting-producer-and-consumer" name="connecting-producer-and-consumer" class="anchor"><span class="anchor-link"></span></a>Connecting Producer and Consumer</h2>
<p>For cases when you need to read messages from one topic, transform or enrich them, and then write to another topic you can use <code>Consumer.committableSource</code> and connect it to a <code>Producer.committableSink</code>. The <code>committableSink</code> will commit the offset back to the consumer when it has successfully published the message.</p>
<p>The <code>committableSink</code> accepts implementations <code>ProducerMessage.Envelope</code> (<a href="/api/alpakka-kafka/1.1.0/akka/kafka/ProducerMessage$$Envelope.html" title="akka.kafka.ProducerMessage$$Envelope"><code>API</code></a>) that contain the offset to commit the consumption of the originating message (of type <code>ConsumerMessage.Committable</code> (<a href="/api/alpakka-kafka/1.1.0/akka/kafka/ConsumerMessage$$Committable.html" title="akka.kafka.ConsumerMessage$$Committable"><code>API</code></a>)). See <a href="producer.html#producing-messages">Producing messages</a> about different implementations of <code>Envelope</code> supported.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L250-L261" target="_blank" title="Go to snippet source"></a><code class="language-scala">val control =
  Consumer
    .committableSource(consumerSettings, Subscriptions.topics(topic1, topic2))
    .map { msg =&gt;
      ProducerMessage.single(
        new ProducerRecord(targetTopic, msg.record.key, msg.record.value),
        msg.committableOffset
      )
    }
    .toMat(Producer.committableSink(producerSettings))(Keep.both)
    .mapMaterializedValue(DrainingControl.apply)
    .run()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L261-L270" target="_blank" title="Go to snippet source"></a><code class="language-java">Consumer.DrainingControl&lt;Done&gt; control =
    Consumer.committableSource(consumerSettings, Subscriptions.topics(topic1, topic2))
        .map(
            msg -&gt;
                ProducerMessage.&lt;String, String, ConsumerMessage.Committable&gt;single(
                    new ProducerRecord&lt;&gt;(targetTopic, msg.record().key(), msg.record().value()),
                    msg.committableOffset()))
        .toMat(Producer.committableSink(producerSettings), Keep.both())
        .mapMaterializedValue(Consumer::createDrainingControl)
        .run(materializer);</code></pre></dd>
</dl><div class="callout note "><div class="callout-title">Note</div>
<p>There is a risk that something fails after publishing, but before committing, so <code>committableSink</code> has &ldquo;at-least-once&rdquo; delivery semantics.</p>
<p>To get delivery guarantees, please read about <a href="transactions.html">transactions</a>.</p></div>
<p>As <code>Producer.committableSink</code>&rsquo;s committing of messages one-by-one is rather slow, prefer a flow together with batching of commits with <code>Committer.sink</code>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L284-L296" target="_blank" title="Go to snippet source"></a><code class="language-scala">val control = Consumer
  .committableSource(consumerSettings, Subscriptions.topics(topic))
  .map { msg =&gt;
    ProducerMessage.single(
      new ProducerRecord(targetTopic, msg.record.key, msg.record.value),
      passThrough = msg.committableOffset
    )
  }
  .via(Producer.flexiFlow(producerSettings))
  .map(_.passThrough)
  .toMat(Committer.sink(committerSettings))(Keep.both)
  .mapMaterializedValue(DrainingControl.apply)
  .run()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L290-L302" target="_blank" title="Go to snippet source"></a><code class="language-java">Consumer.DrainingControl&lt;Done&gt; control =
    Consumer.committableSource(consumerSettings, Subscriptions.topics(topic))
        .map(
            msg -&gt;
                ProducerMessage.single(
                    new ProducerRecord&lt;&gt;(targetTopic, msg.record().key(), msg.record().value()),
                    msg.committableOffset() // the passThrough
                    ))
        .via(Producer.flexiFlow(producerSettings))
        .map(m -&gt; m.passThrough())
        .toMat(Committer.sink(committerSettings), Keep.both())
        .mapMaterializedValue(Consumer::createDrainingControl)
        .run(materializer);</code></pre></dd>
</dl>
<h2><a href="#source-per-partition" name="source-per-partition" class="anchor"><span class="anchor-link"></span></a>Source per partition</h2>
<p><code>Consumer.plainPartitionedSource</code> (<span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Consumer$.html" title="akka.kafka.scaladsl.Consumer"><code>Consumer API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Consumer$.html" title="akka.kafka.javadsl.Consumer"><code>Consumer API</code></a></span>) , <code>Consumer.committablePartitionedSource</code>, and <code>Consumer.commitWithMetadataPartitionedSource</code> support tracking the automatic partition assignment from Kafka. When a topic-partition is assigned to a consumer, this source will emit a tuple with the assigned topic-partition and a corresponding source. When a topic-partition is revoked, the corresponding source completes.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L315-L322" target="_blank" title="Go to snippet source"></a><code class="language-scala">val control = Consumer
  .committablePartitionedSource(consumerSettings, Subscriptions.topics(topic))
  .flatMapMerge(maxPartitions, _._2)
  .via(businessFlow)
  .map(_.committableOffset)
  .toMat(Committer.sink(committerDefaults))(Keep.both)
  .mapMaterializedValue(DrainingControl.apply)
  .run()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L320-L327" target="_blank" title="Go to snippet source"></a><code class="language-java">Consumer.DrainingControl&lt;Done&gt; control =
    Consumer.committablePartitionedSource(consumerSettings, Subscriptions.topics(topic))
        .flatMapMerge(maxPartitions, Pair::second)
        .via(business())
        .map(msg -&gt; msg.committableOffset())
        .toMat(Committer.sink(committerSettings), Keep.both())
        .mapMaterializedValue(Consumer::createDrainingControl)
        .run(materializer);</code></pre></dd>
</dl>
<p>Separate streams per partition:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L334-L345" target="_blank" title="Go to snippet source"></a><code class="language-scala">val control = Consumer
  .committablePartitionedSource(consumerSettings, Subscriptions.topics(topic))
  .mapAsyncUnordered(maxPartitions) {
    case (topicPartition, source) =&gt;
      source
        .via(businessFlow)
        .map(_.committableOffset)
        .runWith(Committer.sink(comitterSettings))
  }
  .toMat(Sink.ignore)(Keep.both)
  .mapMaterializedValue(DrainingControl.apply)
  .run()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L341-L355" target="_blank" title="Go to snippet source"></a><code class="language-java">Consumer.DrainingControl&lt;Done&gt; control =
    Consumer.committablePartitionedSource(consumerSettings, Subscriptions.topics(topic))
        .mapAsyncUnordered(
            maxPartitions,
            pair -&gt; {
              Source&lt;ConsumerMessage.CommittableMessage&lt;String, String&gt;, NotUsed&gt; source =
                  pair.second();
              return source
                  .via(business())
                  .map(message -&gt; message.committableOffset())
                  .runWith(Committer.sink(committerSettings), materializer);
            })
        .toMat(Sink.ignore(), Keep.both())
        .mapMaterializedValue(Consumer::createDrainingControl)
        .run(materializer);</code></pre></dd>
</dl>
<p>Join flows based on automatically assigned partitions:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L519-L554" target="_blank" title="Go to snippet source"></a><code class="language-scala">type Msg = CommittableMessage[String, Array[Byte]]

def zipper(left: Source[Msg, _], right: Source[Msg, _]): Source[(Msg, Msg), NotUsed] = ???

Consumer
  .committablePartitionedSource(consumerSettings, Subscriptions.topics(&quot;topic1&quot;))
  .map {
    case (topicPartition, source) =&gt;
      // get corresponding partition from other topic
      val otherTopicPartition = new TopicPartition(&quot;otherTopic&quot;, topicPartition.partition())
      val otherSource = Consumer.committableSource(consumerSettings, Subscriptions.assignment(otherTopicPartition))
      zipper(source, otherSource)
  }
  .flatMapMerge(maxPartitions, identity)
  .via(businessFlow)
  //build commit offsets
  .batch(
    max = 20,
    seed = {
      case (left, right) =&gt;
        (
          CommittableOffsetBatch(left.committableOffset),
          CommittableOffsetBatch(right.committableOffset)
        )
    }
  )(
    aggregate = {
      case ((batchL, batchR), (l, r)) =&gt;
        batchL.updated(l.committableOffset)
        batchR.updated(r.committableOffset)
        (batchL, batchR)
    }
  )
  .mapAsync(1) { case (l, r) =&gt; l.commitScaladsl().map(_ =&gt; r) }
  .mapAsync(1)(_.commitScaladsl())
  .runWith(Sink.ignore)</code></pre></dd>
</dl>
<h2><a href="#sharing-the-kafkaconsumer-instance" name="sharing-the-kafkaconsumer-instance" class="anchor"><span class="anchor-link"></span></a>Sharing the KafkaConsumer instance</h2>
<p>If you have many streams it can be more efficient to share the underlying <code>KafkaConsumer</code> (<a href="https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html" title="org.apache.kafka.clients.consumer.KafkaConsumer"><code>Kafka API</code></a>) instance. It is shared by creating a <code>KafkaConsumerActor</code> (<a href="/api/alpakka-kafka/1.1.0/akka/kafka/KafkaConsumerActor$.html" title="akka.kafka.KafkaConsumerActor"><code>API</code></a>). You need to create the actor and stop it by sending <code>KafkaConsumerActor.Stop</code> when it is not needed any longer. You pass the <code>ActorRef</code> as a parameter to the <code>Consumer</code> (<span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Consumer$.html" title="akka.kafka.scaladsl.Consumer"><code>Consumer API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Consumer$.html" title="akka.kafka.javadsl.Consumer"><code>Consumer API</code></a></span>)  factory methods.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/PartitionExamples.scala#L38-L75" target="_blank" title="Go to snippet source"></a><code class="language-scala">//Consumer is represented by actor
val consumer: ActorRef = system.actorOf(KafkaConsumerActor.props(consumerSettings))

//Manually assign topic partition to it
val (controlPartition1, result1) = Consumer
  .plainExternalSource[String, Array[Byte]](
    consumer,
    Subscriptions.assignment(new TopicPartition(topic, partition1))
  )
  .via(businessFlow)
  .toMat(Sink.seq)(Keep.both)
  .run()

//Manually assign another topic partition
val (controlPartition2, result2) = Consumer
  .plainExternalSource[String, Array[Byte]](
    consumer,
    Subscriptions.assignment(new TopicPartition(topic, partition2))
  )
  .via(businessFlow)
  .toMat(Sink.seq)(Keep.both)
  .run()

// ....

              controlPartition1.shutdown()
              controlPartition2.shutdown()
consumer ! KafkaConsumerActor.Stop</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L370-L398" target="_blank" title="Go to snippet source"></a><code class="language-java">// Consumer is represented by actor
ActorRef consumer = system.actorOf((KafkaConsumerActor.props(consumerSettings)));

// Manually assign topic partition to it
Consumer.Control controlPartition1 =
    Consumer.plainExternalSource(
            consumer, Subscriptions.assignment(new TopicPartition(topic, partition0)))
        .via(business())
        .to(Sink.ignore())
        .run(materializer);

// Manually assign another topic partition
Consumer.Control controlPartition2 =
    Consumer.plainExternalSource(
            consumer, Subscriptions.assignment(new TopicPartition(topic, partition1)))
        .via(business())
        .to(Sink.ignore())
        .run(materializer);


consumer.tell(KafkaConsumerActor.stop(), self);</code></pre></dd>
</dl>
<h2><a href="#accessing-kafkaconsumer-metrics" name="accessing-kafkaconsumer-metrics" class="anchor"><span class="anchor-link"></span></a>Accessing KafkaConsumer metrics</h2>
<p>You can access the underlying consumer metrics via the materialized <code>Control</code> instance: </p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/PartitionExamples.scala#L88-L100" target="_blank" title="Go to snippet source"></a><code class="language-scala">val control: Consumer.Control = Consumer
  .plainSource(consumerSettings, Subscriptions.assignment(new TopicPartition(topic, partition)))
  .via(businessFlow)
  .to(Sink.ignore)
  .run()


val metrics: Future[Map[MetricName, Metric]] = control.metrics
metrics.foreach(map =&gt; println(s&quot;metrics: ${map.mkString(&quot;\n&quot;)}&quot;))</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L488-L503" target="_blank" title="Go to snippet source"></a><code class="language-java">// run the stream to obtain the materialized Control value
Consumer.DrainingControl&lt;Done&gt; control =
    Consumer.plainSource(
            consumerSettings, Subscriptions.assignment(new TopicPartition(topic, 0)))
        .via(business())
        .toMat(Sink.ignore(), Keep.both())
        .mapMaterializedValue(Consumer::createDrainingControl)
        .run(materializer);

CompletionStage&lt;Map&lt;MetricName, Metric&gt;&gt; metrics = control.getMetrics();
metrics.thenAccept(map -&gt; System.out.println(&quot;Metrics: &quot; + map));</code></pre></dd>
</dl>
<h2><a href="#accessing-kafkaconsumer-metadata" name="accessing-kafkaconsumer-metadata" class="anchor"><span class="anchor-link"></span></a>Accessing KafkaConsumer metadata</h2>
<p>Accessing of Kafka consumer metadata is possible as described in <a href="consumer-metadata.html">Consumer Metadata</a>.</p>
<h2><a href="#listening-for-rebalance-events" name="listening-for-rebalance-events" class="anchor"><span class="anchor-link"></span></a>Listening for rebalance events</h2>
<p>You may set up an rebalance event listener actor that will be notified when your consumer will be assigned or revoked from consuming from specific topic partitions. Two kinds of messages will be sent to this listener actor </p>
<ul>
  <li><code>akka.kafka.TopicPartitionsAssigned</code> and</li>
  <li><code>akka.kafka.TopicPartitionsRevoked</code>, like this:</li>
</ul>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L358-L388" target="_blank" title="Go to snippet source"></a><code class="language-scala">import akka.kafka.{TopicPartitionsAssigned, TopicPartitionsRevoked}

class RebalanceListener extends Actor with ActorLogging {
  def receive: Receive = {
    case TopicPartitionsAssigned(subscription, topicPartitions) =&gt;
      log.info(&quot;Assigned: {}&quot;, topicPartitions)

    case TopicPartitionsRevoked(subscription, topicPartitions) =&gt;
      log.info(&quot;Revoked: {}&quot;, topicPartitions)
  }
}

val rebalanceListener = system.actorOf(Props(new RebalanceListener))
val subscription = Subscriptions
  .topics(topic)
  // additionally, pass the actor reference:
  .withRebalanceListener(rebalanceListener)

// use the subscription as usual:
  Consumer
    .plainSource(consumerSettings, subscription)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L432-L475" target="_blank" title="Go to snippet source"></a><code class="language-java">static class RebalanceListener extends AbstractLoggingActor {

  @Override
  public Receive createReceive() {
    return receiveBuilder()
        .match(
            TopicPartitionsAssigned.class,
            assigned -&gt; {
              log().info(&quot;Assigned: {}&quot;, assigned);
            })
        .match(
            TopicPartitionsRevoked.class,
            revoked -&gt; {
              log().info(&quot;Revoked: {}&quot;, revoked);
            })
        .build();
  }
}

  ActorRef rebalanceListener = system.actorOf(Props.create(RebalanceListener.class));

  Subscription subscription =
      Subscriptions.topics(topic)
          // additionally, pass the actor reference:
          .withRebalanceListener(rebalanceListener);

  // use the subscription as usual:
  Consumer.DrainingControl&lt;List&lt;ConsumerRecord&lt;String, String&gt;&gt;&gt; control =
      Consumer.plainSource(consumerSettings, subscription)
          .toMat(Sink.seq(), Keep.both())
          .mapMaterializedValue(Consumer::createDrainingControl)
          .run(materializer);</code></pre></dd>
</dl>
<h2><a href="#controlled-shutdown" name="controlled-shutdown" class="anchor"><span class="anchor-link"></span></a>Controlled shutdown</h2>
<p>The <code>Source</code> created with <code>Consumer.plainSource</code> and similar methods materializes to a <code>Consumer.Control</code> (<span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Consumer$$Control.html" title="akka.kafka.scaladsl.Consumer$$Control"><code>API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Consumer$$Control.html" title="akka.kafka.javadsl.Consumer$$Control"><code>API</code></a></span>) instance. This can be used to stop the stream in a controlled manner.</p>
<p>When using external offset storage, a call to <code>Consumer.Control.shutdown()</code> suffices to complete the <code>Source</code>, which starts the completion of the stream.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L404-L414" target="_blank" title="Go to snippet source"></a><code class="language-scala">val (consumerControl, streamComplete) =
  Consumer
    .plainSource(consumerSettings,
                 Subscriptions.assignmentWithOffset(
                   new TopicPartition(topic, 0) -&gt; offset
                 ))
    .via(businessFlow)
    .toMat(Sink.ignore)(Keep.both)
    .run()

consumerControl.shutdown()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L514-L534" target="_blank" title="Go to snippet source"></a><code class="language-java">final OffsetStorage db = new OffsetStorage();

CompletionStage&lt;Consumer.DrainingControl&lt;Done&gt;&gt; control =
    db.loadOffset()
        .thenApply(
            fromOffset -&gt;
                Consumer.plainSource(
                        consumerSettings,
                        Subscriptions.assignmentWithOffset(
                            new TopicPartition(topic, 0), fromOffset))
                    .mapAsync(
                        10,
                        record -&gt;
                            business(record.key(), record.value())
                                .thenApply(res -&gt; db.storeProcessedOffset(record.offset())))
                    .toMat(Sink.ignore(), Keep.both())
                    .mapMaterializedValue(Consumer::createDrainingControl)
                    .run(materializer));

// Shutdown the consumer when desired
control.thenAccept(c -&gt; c.drainAndShutdown(executor));</code></pre></dd>
</dl>
<p>When you are using offset storage in Kafka, the shutdown process involves several steps:</p>
<ol>
  <li><code>Consumer.Control.stop()</code> to stop producing messages from the <code>Source</code>. This does not stop the underlying Kafka Consumer.</li>
  <li>Wait for the stream to complete, so that a commit request has been made for all offsets of all processed messages (via <code>Committer.sink/flow</code>, <code>commitScaladsl()</code> or <code>commitJavadsl()</code>).</li>
  <li><code>Consumer.Control.shutdown()</code> to wait for all outstanding commit requests to finish and stop the Kafka Consumer.</li>
</ol>
<h3><a href="#draining-control" name="draining-control" class="anchor"><span class="anchor-link"></span></a>Draining control</h3>
<p>To manage this shutdown process, use the <code>Consumer.DrainingControl</code> (<span class="group-scala"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/scaladsl/Consumer$$DrainingControl.html" title="akka.kafka.scaladsl.Consumer$$DrainingControl"><code>API</code></a></span><span class="group-java"><a href="/api/alpakka-kafka/1.1.0/akka/kafka/javadsl/Consumer$$DrainingControl.html" title="akka.kafka.javadsl.Consumer$$DrainingControl"><code>API</code></a></span>) by combining the <code>Consumer.Control</code> with the sink&rsquo;s materialized completion future in <code>mapMaterializedValue</code>. That control offers the method <code>drainAndShutdown</code> which implements the process descibed above.</p>
<p>Note: The <code>ConsumerSettings</code> <code>stop-timeout</code> delays stopping the Kafka Consumer and the stream, but when using <code>drainAndShutdown</code> that delay is not required and can be set to zero (as below).</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/scala/docs/scaladsl/ConsumerExample.scala#L424-L434" target="_blank" title="Go to snippet source"></a><code class="language-scala">val drainingControl =
  Consumer
    .committableSource(consumerSettings.withStopTimeout(Duration.Zero), Subscriptions.topics(topic))
    .mapAsync(1) { msg =&gt;
      business(msg.record).map(_ =&gt; msg.committableOffset)
    }
    .toMat(Committer.sink(committerSettings))(Keep.both)
    .mapMaterializedValue(DrainingControl.apply)
    .run()

val streamComplete = drainingControl.drainAndShutdown()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/tests/src/test/java/docs/javadsl/ConsumerExampleTest.java#L547-L569" target="_blank" title="Go to snippet source"></a><code class="language-java">final Executor ec = Executors.newCachedThreadPool();

Consumer.DrainingControl&lt;Done&gt; control =
    Consumer.committableSource(
            consumerSettings.withStopTimeout(Duration.ZERO), Subscriptions.topics(topic))
        .mapAsync(
            1,
            msg -&gt;
                business(msg.record().key(), msg.record().value())
                    .thenApply(done -&gt; msg.committableOffset()))
        .toMat(Committer.sink(committerSettings.withMaxBatch(1)), Keep.both())
        .mapMaterializedValue(Consumer::createDrainingControl)
        .run(materializer);

control.drainAndShutdown(ec);</code></pre></dd>
</dl>
</div>
</article>
<div class="row">
<div class="small-12 column">
<section class="nav-prev-next row">
<div class="nav-prev small-6 column">
<a href="producer.html"><i class="icon-prev"></i> <span class="link-prev">Producer</span></a>
</div>
<div class="nav-next small-6 column clearfix">
<a class="float-right" href="subscription.html">Subscription <i class="icon-next"></i></a>
</div>
</section>
</div>
</div>
<div class="source-github row">
Found an error in this documentation? The source code for this page can be found <a href="https://github.com/akka/alpakka-kafka/tree/v1.1.0/docs/src/main/paradox/consumer.md">here</a>.
Please feel free to edit and contribute a pull request.
</div>

<footer class="page-footer row clearfix">
<img class="akka-icon float-left show-for-medium" src="images/akka-icon.svg" />
<section class="copyright">
<div>Alpakka Kafka is Open Source and available under the Apache 2 License.</div>
<p class="legal">
&copy; 2011-2019 <a href="https://www.lightbend.com" target="_blank">Lightbend, Inc.</a> |
<a href="https://www.lightbend.com/legal/licenses" target="_blank">Licenses</a> |
<a href="https://www.lightbend.com/legal/terms" target="_blank">Terms</a> |
<a href="https://www.lightbend.com/legal/privacy" target="_blank">Privacy Policy</a> |
<a href="https://akka.io/cookie/" target="_blank">Cookie Listing</a> |
<a class="optanon-toggle-display">Cookie Settings</a>
</p>
</section>

</footer>
</section>
</main>
</div>

<script type="text/javascript" src="js/scrollsneak.js"></script>
<script type="text/javascript">jQuery(document).foundation();</script>
<script type="text/javascript" src="js/groups.js"></script>
<script type="text/javascript" src="js/page.js"></script>
<script type="text/javascript" src="js/magellan.js"></script>

<style type="text/css">@import "lib/prettify/prettify.css";</style>
<script type="text/javascript" src="lib/prettify/prettify.js"></script>
<script type="text/javascript" src="lib/prettify/lang-scala.js"></script>
<script type="text/javascript">//<![CDATA[
jQuery(function(){window.prettyPrint && prettyPrint()});
//]]></script>
<script type="text/javascript" src="assets/js/warnOldVersion.js"></script>
<script type="text/javascript">jQuery(function(jq){initOldVersionWarnings(jq, '1.1.0', 'https://doc.akka.io/docs/alpakka-kafka/current')});</script>


</body>
</html>
