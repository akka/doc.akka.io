<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
<title>At-Least-Once Delivery &bull; Alpakka Kafka Documentation</title>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="description" content="Alpakka Kafka"/>
<link rel="canonical" href="https://doc.akka.io/docs/alpakka-kafka/current/atleastonce.html"/>
<script type="text/javascript" src="lib/jquery/jquery.min.js"></script>
<script type="text/javascript" src="lib/foundation/dist/js/foundation.min.js"></script>
<link rel="stylesheet" type="text/css" href="lib/normalize.css/normalize.css"/>
<link rel="stylesheet" type="text/css" href="lib/foundation/dist/css/foundation.min.css"/>
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/docsearch.js/2/docsearch.min.css" />
<link rel="stylesheet" type="text/css" href="css/icons.css"/>
<link rel="stylesheet" type="text/css" href="css/page.css"/>
<link rel="shortcut icon" href="images/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png"/>
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png"/>
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png"/>
<link rel="manifest" href="images/manifest.json"/>
<meta name="msapplication-TileImage" content="images/mstile-150x150.png"/>
<meta name="msapplication-TileColor" content="#15a9ce"/>
<meta name="theme-color" content="#15a9ce"/>
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) start -->
<script src="https://optanon.blob.core.windows.net/consent/159bb13d-6748-4399-806e-ac28db879785.js" type="text/javascript" charset="UTF-8"></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) end -->
<!--Google Analytics-->
<script type="text/plain" class="optanon-category-2">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', '']);
_gaq.push(['_setDomainName', '']);
_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})()
</script>
<script type="text/plain" class="optanon-category-2">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-23127719-1', 'lightbend.com', {'allowLinker': true, 'name': 'tsTracker'});
ga('tsTracker.require', 'linker');
ga('tsTracker.linker:autoLink', ['lightbend.com','playframework.com','scala-lang.org','scaladays.org','spray.io','akka.io','scala-sbt.org','scala-ide.org']);
ga('tsTracker.send', 'pageview');
</script>
<!--Marketo-->
<script type="text/plain" class="optanon-category-3">
(function() {
var didInit = false;
function initMunchkin() {
if(didInit === false) {
didInit = true;
Munchkin.init('558-NCX-702', { 'asyncOnly': true, 'disableClickDelay': true });
}
}
var s = document.createElement('script');
s.type = 'text/javascript';
s.async = true;
s.src = '//munchkin.marketo.net/munchkin.js';
s.onreadystatechange = function() {
if (this.readyState == 'complete' || this.readyState == 'loaded') {
initMunchkin();
}
};
s.onload = initMunchkin;
document.getElementsByTagName('head')[0].appendChild(s);
})();
</script>
</head>

<body id="underlay" data-toggler="nav-open">

<header class="site-header hide-for-large">
<div class="sticky-header clearfix">
<a href="http://akka.io"><img class="logo" src="images/akka-alpakka-reverse.svg"></a>

<button class="menu-icon float-right" type="button" data-toggle="underlay overlay"></button>
</div>
<div id="overlay" class="overlay-nav" data-toggler="nav-open">
<header class="nav-header">
<div class="nav-header-title">
<h1><a href="index.html">Alpakka Kafka Documentation</a></h1>
</div>
<div class="nav-header-version">
Version 1.0
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="nav-toc">
<ul>
  <li><a href="home.html" class="page">Overview</a></li>
  <li><a href="producer.html" class="page">Producer</a></li>
  <li><a href="consumer.html" class="page">Consumer</a></li>
  <li><a href="errorhandling.html" class="page">Error handling</a></li>
  <li><a href="atleastonce.html#at-least-once-delivery" class="active page">At-Least-Once Delivery</a>
  <ul>
    <li><a href="atleastonce.html#multiple-effects-per-commit" class="header">Multiple Effects per Commit</a></li>
    <li><a href="atleastonce.html#non-sequential-processing" class="header">Non-Sequential Processing</a></li>
    <li><a href="atleastonce.html#conditional-message-processing" class="header">Conditional Message Processing</a></li>
  </ul></li>
  <li><a href="transactions.html" class="page">Transactions</a></li>
  <li><a href="serialization.html" class="page">Serialization</a></li>
  <li><a href="debugging.html" class="page">Debugging</a></li>
  <li><a href="testing.html" class="page">Testing</a></li>
  <li><a href="production.html" class="page">Production considerations</a></li>
  <li><a href="snapshots.html" class="page">Snapshots</a></li>
</ul>
</nav>
</div>
</header>

<aside class="show-for-large">
<header class="nav-header fixed-sidebar-header">
<div class="nav-header-title">
<h1><a href="index.html">Alpakka Kafka Documentation</a></h1>
</div>
<div class="nav-header-version">
Version 1.0
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="site-nav fixed-sidebar-contents">
<div class="nav-toc">
<ul>
  <li><a href="home.html" class="page">Overview</a></li>
  <li><a href="producer.html" class="page">Producer</a></li>
  <li><a href="consumer.html" class="page">Consumer</a></li>
  <li><a href="errorhandling.html" class="page">Error handling</a></li>
  <li><a href="atleastonce.html#at-least-once-delivery" class="active page">At-Least-Once Delivery</a>
  <ul>
    <li><a href="atleastonce.html#multiple-effects-per-commit" class="header">Multiple Effects per Commit</a></li>
    <li><a href="atleastonce.html#non-sequential-processing" class="header">Non-Sequential Processing</a></li>
    <li><a href="atleastonce.html#conditional-message-processing" class="header">Conditional Message Processing</a></li>
  </ul></li>
  <li><a href="transactions.html" class="page">Transactions</a></li>
  <li><a href="serialization.html" class="page">Serialization</a></li>
  <li><a href="debugging.html" class="page">Debugging</a></li>
  <li><a href="testing.html" class="page">Testing</a></li>
  <li><a href="production.html" class="page">Production considerations</a></li>
  <li><a href="snapshots.html" class="page">Snapshots</a></li>
</ul>
</div>
</nav>
<footer class="nav-footer fixed-sidebar-footer">
<a href="http://akka.io"><img class="logo" src="images/akka-alpakka-reverse.svg"></a>

</footer>
</aside>

<main class="fixed-shift-for-large expanded row">
<section class="site-content small-12 column">

<article class="page-content row">
<div class="small-12 large-9 column" id="docs">
<h1><a href="#at-least-once-delivery" name="at-least-once-delivery" class="anchor"><span class="anchor-link"></span></a>At-Least-Once Delivery</h1>
<p>At-least-once delivery semantics, the requirement to process every message, is a basic requirement of most applications. </p>
<p>When using committable sources (<a href="consumer.html#offset-storage-in-kafka">Offset Storage in Kafka</a>), care is needed to ensure at-least-once delivery semantics are not lost inadvertently by committing an offset too early.</p>
<p>Below are some scenarios where this risk is present. These risks can easily be overlooked. Problems can also go undetected during tests since they depend on abruptly interrupting the flow in a particular state, and that state could be unlikely to occur. </p>
<h2><a href="#multiple-effects-per-commit" name="multiple-effects-per-commit" class="anchor"><span class="anchor-link"></span></a>Multiple Effects per Commit</h2>
<h3><a href="#multiple-messages" name="multiple-messages" class="anchor"><span class="anchor-link"></span></a>Multiple Messages</h3>
<p>When connecting a committable source to a producer flow, some applications may require each consumed message to produce more than one message. In that case, in order to preserve at-least-once semantics, the message offset should only be committed after all associated messages have been produced.</p>
<p>To achieve this, use the <code>ProducerMessage.MultiMessage</code> implementation of <code>Envelope</code>:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.0/tests/src/test/scala/docs/scaladsl/AtLeastOnce.scala#L9-L56" target="_blank" title="Go to snippet source"></a><code class="language-scala">import akka.Done
import akka.kafka.ConsumerMessage.CommittableOffset
import akka.kafka.ProducerMessage.Envelope
import akka.kafka.scaladsl.Consumer.DrainingControl
import akka.kafka.{KafkaPorts, ProducerMessage, Subscriptions}
import akka.kafka.scaladsl.{Committer, Consumer, Producer}
import akka.stream.scaladsl.{Keep, Sink}
import akka.stream.testkit.scaladsl.StreamTestKit.assertAllStagesStopped
import net.manub.embeddedkafka.EmbeddedKafkaConfig
import org.apache.kafka.clients.producer.ProducerRecord

import scala.collection.immutable
import scala.concurrent.Await
import scala.concurrent.duration._

Consumer
  .committableSource(consumerSettings, Subscriptions.topics(topic1))
  .map(
    msg =&gt;
      ProducerMessage.multi(
        immutable.Seq(
          new ProducerRecord(topic2, msg.record.key, msg.record.value),
          new ProducerRecord(topic3, msg.record.key, msg.record.value)
        ),
        msg.committableOffset
    )
  )
  .via(Producer.flexiFlow(producerSettings))
  .map(_.passThrough)
  .toMat(Committer.sink(committerSettings))(Keep.both)
  .mapMaterializedValue(DrainingControl.apply)
  .run()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.0/tests/src/test/java/docs/javadsl/AtLeastOnceTest.java#L28-L81" target="_blank" title="Go to snippet source"></a><code class="language-java">import akka.Done;
import akka.japi.Pair;
import akka.kafka.*;
import akka.kafka.ConsumerMessage.CommittableOffset;
import akka.kafka.ProducerMessage.Envelope;
import akka.kafka.javadsl.Committer;
import akka.kafka.javadsl.Consumer;
import akka.kafka.javadsl.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;

Consumer.committableSource(consumerSettings, Subscriptions.topics(topic1))
    .map(
        msg -&gt; {
          Envelope&lt;String, String, CommittableOffset&gt; multiMsg =
              ProducerMessage.multi(
                  Arrays.asList(
                      new ProducerRecord&lt;&gt;(topic2, msg.record().value()),
                      new ProducerRecord&lt;&gt;(topic3, msg.record().value())),
                  msg.committableOffset());
          return multiMsg;
        })
    .via(Producer.flexiFlow(producerSettings))
    .map(m -&gt; m.passThrough())
    .toMat(Committer.sink(committerSettings), Keep.both())
    .mapMaterializedValue(Consumer::createDrainingControl)
    .run(materializer);</code></pre></dd>
</dl>
<h3><a href="#batches" name="batches" class="anchor"><span class="anchor-link"></span></a>Batches</h3>
<p>If committable messages are processed in batches (using <code>batch</code> or <code>grouped</code>), it is also important to commit the resulting <code>CommittableOffsetBatch</code> only after all messages in the batch are fully processed.</p>
<p>Should the batch need to be split up again, using mapConcat, care should be taken to associate the <code>CommittableOffsetBatch</code> only with the last message. This scenario could occur if we created batches to more efficiently update a database and then needed to split up the batches to send individual messages to a Kafka producer flow.</p>
<h3><a href="#multiple-destinations" name="multiple-destinations" class="anchor"><span class="anchor-link"></span></a>Multiple Destinations</h3>
<p>In the Conditional Message Processing section below we discuss how to handle producing to multiple Kafka topics, but here we consider writing to other types of persistent storage, or performing other side-effects, perhaps in addition to producing to Kafka topics.</p>
<p>To commit an offset or an offset batch only after the multiple effects have been performed, we will usually want to asssemble our side-effecting flows in series, one after the other. This still allows the side effects to be performed concurrently on distinct messages, using <code>mapAsync</code> for example.</p>
<p>Alternatively, we could split-off the flow using <code>alsoTo</code> to perform the effects in distinct parallel flows. We would then use <code>zip</code> to bring the two flows back together and re-associate the matching committable offsets. This step is important to ensure that we only commit an offset once the effects from both flows are complete. This constrains the two flows to output the exact same sequence of committable offsets. So this approach may not be significantly more flexible then a serial arrangement.</p>
<h2><a href="#non-sequential-processing" name="non-sequential-processing" class="anchor"><span class="anchor-link"></span></a>Non-Sequential Processing</h2>
<p>Messages from committable sources should be processed in order, otherwise a larger offset for a partition could be committed before the work associated to a smaller offset has been completed.</p>
<p>Reordering would be acceptable if the original order was reconstituted before committing the offsets, but that is a fairly complex and possibly brittle process that we will not consider here.</p>
<p>Using <code>mapAsync</code> is safe since it preserves the order of messages. That is in constrast to <code>mapAsyncUnordered</code> which would not be safe to use here. As indicated in the <a href="https://doc.akka.io/docs/akka/2.5.21/scala/stream/stream-flows-and-basics.html#Stream_ordering">Akka Streams documentation</a> almost all stages will preserve input ordering.</p>
<h3><a href="#using-groupby" name="using-groupby" class="anchor"><span class="anchor-link"></span></a>Using groupBy</h3>
<p>Using <code>groupBy</code> followed at some point by a <code>mergeSubstreams</code> can reorder messages, so in general it is not safe with respect to the at-least-once guarantee.</p>
<p>However it can only lead to reordering between messages sent to different substreams, so it is possible to use <code>groupBy</code> and preserve at-least-once semantics as long as all messages from the same partition are sent to the same substream.</p>
<p>If a particular substream expects to see all messages regarding some entity, it then requires that writers to the source topic become responsible for placing messages about various entities in the appropriate partitions. If your application already has a requirement to preserve the order of messages about a particular entity within a Kafka topic, you will already need to ensure those messages go to the same partition since Kafka only preserves order information within a partition.</p>
<h2><a href="#conditional-message-processing" name="conditional-message-processing" class="anchor"><span class="anchor-link"></span></a>Conditional Message Processing</h2>
<p>Most flows will require some messages to be handled differently from others. Unfortunately this is difficult to do while preserving the at-least-once guarantee because the order of messages must be maintained.</p>
<p>We cannot safely send off the messages to be handled differently to a distinct flow: this other flow cannot commit on its own, and even if we merge it back, downstream, with the main flow, ordering will not be preserved. The reason this separate flow cannot commit on its own is that it will only be seeing a subset of the committable messages. If it commits an offset, it cannot know that all prior offsets have been processed in the main flow.</p>
<p>This is a significant challenge. Below we suggest a few strategies to deal with some special cases of this general problem.</p>
<h3><a href="#publishing-to-message-dependent-topics" name="publishing-to-message-dependent-topics" class="anchor"><span class="anchor-link"></span></a>Publishing to Message-Dependent Topics</h3>
<p>Since <code>ProducerRecord</code> contains the destination topic, it is possible to use a single producer flow to write to any number of topics. This preserves the ordering of messages coming from the committable source. Since the destination topics likely admit different types of messages, it will be necessary to serialize the messages to the appropriate input type for the common producer flow, which could be a byte array or a string.</p>
<p>In case a committable message should lead to the production of multiple messages, the <code>ProducerMessage.MultiMessage</code> is available. If no messages should be produced, the <code>ProducerMessage.PassThroughMessage</code> can be used.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.0/tests/src/test/scala/docs/scaladsl/AtLeastOnce.scala#L80-L105" target="_blank" title="Go to snippet source"></a><code class="language-scala">Consumer
  .committableSource(consumerSettings, Subscriptions.topics(topic1))
  .map(msg =&gt; {
    val out: Envelope[String, String, CommittableOffset] =
      if (duplicate(msg.record.value))
        ProducerMessage.multi(
          immutable.Seq(
            new ProducerRecord(topic2, msg.record.key, msg.record.value),
            new ProducerRecord(topic3, msg.record.key, msg.record.value)
          ),
          msg.committableOffset
        )
      else if (ignore(msg.record.value))
        ProducerMessage.passThrough(msg.committableOffset)
      else
        ProducerMessage.single(
          new ProducerRecord(topic4, msg.record.key, msg.record.value),
          msg.committableOffset
        )
    out
  })
  .via(Producer.flexiFlow(producerSettings))
  .map(_.passThrough)
  .toMat(Committer.sink(committerSettings))(Keep.both)
  .mapMaterializedValue(DrainingControl.apply)
  .run()</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/akka/alpakka-kafka/tree/v1.0/tests/src/test/java/docs/javadsl/AtLeastOnceTest.java#L28-L143" target="_blank" title="Go to snippet source"></a><code class="language-java">import akka.Done;
import akka.japi.Pair;
import akka.kafka.*;
import akka.kafka.ConsumerMessage.CommittableOffset;
import akka.kafka.ProducerMessage.Envelope;
import akka.kafka.javadsl.Committer;
import akka.kafka.javadsl.Consumer;
import akka.kafka.javadsl.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;

Consumer.committableSource(consumerSettings, Subscriptions.topics(topic1))
    .map(
        msg -&gt; {
          final Envelope&lt;String, String, CommittableOffset&gt; produce;
          if (duplicate(msg.record().value())) {
            produce =
                ProducerMessage.multi(
                    Arrays.asList(
                        new ProducerRecord&lt;&gt;(topic2, msg.record().value()),
                        new ProducerRecord&lt;&gt;(topic3, msg.record().value())),
                    msg.committableOffset());
          } else if (ignore(msg.record().value())) {
            produce = ProducerMessage.passThrough(msg.committableOffset());
          } else {
            produce =
                ProducerMessage.single(
                    new ProducerRecord&lt;&gt;(topic4, msg.record().value()),
                    msg.committableOffset());
          }
          return produce;
        })
    .via(Producer.flexiFlow(producerSettings))
    .map(m -&gt; m.passThrough())
    .toMat(Committer.sink(committerSettings), Keep.both())
    .mapMaterializedValue(Consumer::createDrainingControl)
    .run(materializer);</code></pre></dd>
</dl>
<h3><a href="#excluding-messages" name="excluding-messages" class="anchor"><span class="anchor-link"></span></a>Excluding Messages</h3>
<p>Failure to deserialize a message is a particular case of conditional message processing. It is also likely that we would have no message to produce to Kafka when we encounter messages that fail to deserialize. As described above, the producer flow will not let us pass through the corresponding committable offset without producing a message. </p>
<p>Why can&rsquo;t we commit the offsets of bad messages as soon as we encounter them, instead of passing them downstream? Because the previous offsets, for messages that have deserialized successfully, may not have been committed yet. That&rsquo;s possible if the downstream flow includes a buffer, an asynchronous boundary or performs batching. It is then likely that some previous messages would concurrently be making their way downstream to a final committing stage.</p>
<p>Note that here we assume that we take the full control over the handling of messages that fail to deserialize. To do this, we should not ask for the deserialization to be performed by the committable source. We can instead create a <code>ConsumerSettings</code> parametrized by byte arrays. A subsequent <code>map</code> can deserialize and use <code>ProducerMessage.PassThroughMessage</code> to skip bad messages.</p>
</div>
</article>

<div class="row">
<div class="small-12 large-9 column">
<section class="nav-prev-next row">
<div class="nav-prev small-6 column">
<a href="errorhandling.html"><i class="icon-prev"></i> <span class="link-prev">Error handling</span></a>
</div>
<div class="nav-next small-6 column clearfix">
<a class="float-right" href="transactions.html">Transactions <i class="icon-next"></i></a>
</div>
</section>
</div>
</div>

<div class="source-github row">
Found an error in this documentation? The source code for this page can be found <a href="https://github.com/akka/alpakka-kafka/tree/v1.0/docs/src/main/paradox/atleastonce.md">here</a>.
Please feel free to edit and contribute a pull request.
</div>


<footer class="page-footer row clearfix">
<img class="akka-icon float-left show-for-medium" src="images/akka-icon.svg"/>
<section class="copyright">
<div>Alpakka Kafka is Open Source and available under the Apache 2 License.</div>
<p class="legal">
&copy; 2011-2019 <a href="https://www.lightbend.com" target="_blank">Lightbend, Inc.</a> | 
<a href="https://www.lightbend.com/legal/licenses" target="_blank">Licenses</a> | 
<a href="https://www.lightbend.com/legal/terms" target="_blank">Terms</a> | 
<a href="https://www.lightbend.com/legal/privacy" target="_blank">Privacy Policy</a> | 
<a href="https://akka.io/cookie/" target="_blank">Cookie Listing</a> | 
<a class="optanon-toggle-display">Cookie Settings</a>
</p>
</section>
</footer>

</section>
</main>

<script type="text/javascript" src="js/scrollsneak.js"></script>
<script type="text/javascript">jQuery(document).foundation();</script>
<script type="text/javascript" src="js/groups.js"></script>
<script type="text/javascript" src="js/page.js"></script>
<script type="text/javascript" src="js/magellan.js"></script>

<style type="text/css">@import "lib/prettify/prettify.css";</style>
<script type="text/javascript" src="lib/prettify/prettify.js"></script>
<script type="text/javascript" src="lib/prettify/lang-scala.js"></script>
<script type="text/javascript">//<![CDATA[
jQuery(function(){window.prettyPrint && prettyPrint()});
//]]></script>
<script type="text/javascript" src="assets/js/warnOldVersion.js"></script>
<script type="text/javascript">jQuery(function(jq){initOldVersionWarnings(jq, '1.0', 'https://doc.akka.io/docs/alpakka-kafka/current/')});</script>


</body>
</html>
