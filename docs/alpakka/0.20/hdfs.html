<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
<title>Hadoop Distributed File System - HDFS · Alpakka</title>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="description" content="Alpakka is a Reactive Enterprise Integration library for Java and Scala, based on Reactive Streams and Akka."/><link rel="canonical" href="https://doc.akka.io/docs/alpakka/current/hdfs.html"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:100normal,100italic,300normal,300italic,400normal,400italic,500normal,500italic,700normal,700italic,900normal,900italicc" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="lib/jquery/jquery.min.js"></script>
<script type="text/javascript" src="js/page.js"></script>
<script type="text/javascript" src="js/warnOldVersion.js"></script>
<script type="text/javascript" src="js/groups.js"></script>
<link rel="stylesheet" type="text/css" href="lib/normalize.css/normalize.css"/>
<link rel="stylesheet" type="text/css" href="lib/foundation/dist/foundation.min.css"/>
<link rel="stylesheet" type="text/css" href="css/page.css"/>

<!--
<link rel="shortcut icon" href="images/favicon.ico" />
-->
</head>

<body>
<div class="off-canvas-wrapper">
<div class="off-canvas-wrapper-inner" data-off-canvas-wrapper>

<div class="off-canvas position-left" id="off-canvas-menu" data-off-canvas>
<nav class="off-canvas-nav">
<div class="nav-home">
<a href="index.html" >
<span class="home-icon">⌂</span>Alpakka
</a>
<div class="version-number">
0.20
</div>
</div>
<select class="supergroup" name="Language"><option class="group" value="group-scala">Scala</option><option class="group" value="group-java">Java</option></select>
<div class="nav-toc">
<ul>
  <li><a href="connectors.html" class="page">Connectors</a>
  <ul>
    <li><a href="amqp.html" class="page">AMQP</a></li>
    <li><a href="external/apache-camel.html" class="page">Apache Camel</a></li>
    <li><a href="cassandra.html" class="page">Apache Cassandra</a></li>
    <li><a href="geode.html" class="page">Apache Geode</a></li>
    <li><a href="kafka.html" class="page">Apache Kafka</a></li>
    <li><a href="kudu.html" class="page">Apache Kudu</a></li>
    <li><a href="solr.html" class="page">Apache Solr</a></li>
    <li><a href="dynamodb.html" class="page">AWS DynamoDB</a></li>
    <li><a href="kinesis.html" class="page">AWS Kinesis</a></li>
    <li><a href="awslambda.html" class="page">AWS Lambda</a></li>
    <li><a href="s3.html" class="page">AWS S3</a></li>
    <li><a href="sns.html" class="page">AWS SNS</a></li>
    <li><a href="sqs.html" class="page">AWS SQS</a></li>
    <li><a href="external/azure-event-hubs.html" class="page">Azure Event Hubs</a></li>
    <li><a href="external/azure-iot-hub.html" class="page">Azure IoT Hub</a></li>
    <li><a href="azure-storage-queue.html" class="page">Azure Storage Queue</a></li>
    <li><a href="elasticsearch.html" class="page">Elasticsearch</a></li>
    <li><a href="external/eventuate.html" class="page">Eventuate</a></li>
    <li><a href="file.html" class="page">Files</a></li>
    <li><a href="external/fs2.html" class="page">FS2</a></li>
    <li><a href="ftp.html" class="page">FTP</a></li>
    <li><a href="google-cloud-pub-sub.html" class="page">Google Cloud Pub/Sub</a></li>
    <li><a href="google-fcm.html" class="page">Google Firebase Cloud Messaging</a></li>
    <li><a href="hdfs.html" class="active page">Hadoop Distributed File System - HDFS</a></li>
    <li><a href="hbase.html" class="page">HBase</a></li>
    <li><a href="external/http.html" class="page">HTTP</a></li>
    <li><a href="ironmq.html" class="page">IronMQ</a></li>
    <li><a href="jms.html" class="page">JMS</a></li>
    <li><a href="mongodb.html" class="page">MongoDB</a></li>
    <li><a href="mqtt.html" class="page">MQTT</a></li>
    <li><a href="orientdb.html" class="page">OrientDB</a></li>
    <li><a href="external/pulsar.html" class="page">Pulsar</a></li>
    <li><a href="sse.html" class="page">Server-sent Events (SSE)</a></li>
    <li><a href="slick.html" class="page">Slick (JDBC)</a></li>
    <li><a href="spring-web.html" class="page">Spring Web</a></li>
    <li><a href="external/tcp.html" class="page">TCP</a></li>
    <li><a href="udp.html" class="page">UDP</a></li>
    <li><a href="unix-domain-socket.html" class="page">Unix Domain Socket</a></li>
  </ul></li>
  <li><a href="external-components.html" class="page">External Components</a></li>
  <li><a href="examples/index.html" class="page">Self-contained examples</a>
  <ul>
    <li><a href="examples/csv-samples.html" class="page">CSV</a></li>
    <li><a href="examples/elasticsearch-samples.html" class="page">Elasticsearch</a></li>
    <li><a href="examples/ftp-samples.html" class="page">FTP</a></li>
    <li><a href="examples/jms-samples.html" class="page">JMS</a></li>
  </ul></li>
  <li><a href="other-docs/index.html" class="page">Other documentation</a>
  <ul>
    <li><a href="other-docs/webinars-presentations-articles.html" class="page">Webinars, Presentations and Articles</a></li>
    <li><a href="other-docs/snapshots.html" class="page">Snapshots</a></li>
  </ul></li>
  <li><a href="patterns.html" class="page">Integration Patterns</a></li>
  <li><a href="data-transformations/index.html" class="page">Data Transformations</a>
  <ul>
    <li><a href="data-transformations/parsing-lines.html" class="page">Parsing Lines</a></li>
    <li><a href="data-transformations/json.html" class="page">JSON</a></li>
    <li><a href="data-transformations/compression.html" class="page">Compressing/decompressing</a></li>
    <li><a href="data-transformations/csv.html" class="page">Comma-Separated Values - CSV</a></li>
    <li><a href="data-transformations/recordio.html" class="page">RecordIO Framing</a></li>
    <li><a href="data-transformations/text.html" class="page">Text and charsets</a></li>
    <li><a href="data-transformations/xml.html" class="page">Extensible Markup Language - XML</a></li>
  </ul></li>
</ul>
</div>

</nav>
</div>

<div class="off-canvas-content" data-off-canvas-content>

<header class="site-header expanded row">
<div class="small-12 column">
<a href="#" class="off-canvas-toggle hide-for-medium" data-toggle="off-canvas-menu"><svg class="svg-icon svg-icon-menu" version="1.1" id="Menu" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 20 20" enable-background="new 0 0 20 20" xml:space="preserve"> <path class="svg-icon-menu-path" fill="#53CDEC" d="M16.4,9H3.6C3.048,9,3,9.447,3,10c0,0.553,0.048,1,0.6,1H16.4c0.552,0,0.6-0.447,0.6-1C17,9.447,16.952,9,16.4,9z M16.4,13
H3.6C3.048,13,3,13.447,3,14c0,0.553,0.048,1,0.6,1H16.4c0.552,0,0.6-0.447,0.6-1C17,13.447,16.952,13,16.4,13z M3.6,7H16.4
C16.952,7,17,6.553,17,6c0-0.553-0.048-1-0.6-1H3.6C3.048,5,3,5.447,3,6C3,6.553,3.048,7,3.6,7z"/></svg>
</a>
<div class="title"><a href="index.html" class="logo" style="margin-top: 15px;"><svg class="svg-icon svg-icon-logo" style="height: 45px; width: 184px;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1070 262"><title>akka-stream-alpakka</title><g id="akka-stream-alpakka" class="svg-icon-logo-text" fill="#fff"><path d="M349.6 105.5v-12.2h19.9v58.4c0 7.1 1.7 9.8 6.1 9.8 1.2 0 2.7-.2 4.1-.3v16.1c-2.2.8-5.5 1.3-9.8 1.3-4.8 0-8.6-.8-11.6-2.7-3.7-2.5-6-5.8-6.8-10.1-5.8 8.8-15.4 13.1-28.7 13.1-11.8 0-21.7-4.1-29.9-12.6-8-8.5-12-18.8-12-31.2s4-22.7 12-31c8.1-8.5 18.1-12.6 29.9-12.6 13.6 0 23.7 6 26.8 14zm-5.9 47.9c5-4.8 7.5-11 7.5-18.3s-2.5-13.4-7.5-18.3c-4.8-4.8-11-7.3-18.1-7.3-7.1 0-12.9 2.5-17.8 7.3-4.6 4.8-7 11-7 18.3s2.3 13.4 7 18.3c4.8 4.8 10.6 7.3 17.8 7.3 7.1 0 13.3-2.5 18.1-7.3zM388.5 177v-115.7h19.8v67.6l30.9-35.5h22.8l-32.7 37.4 36.2 46.3h-22.6l-26.4-33.7-8.3 9.3v24.3h-19.7zM470.8 177v-115.7h19.8v67.6l30.9-35.5h22.9l-32.7 37.4 36.2 46.3h-22.6l-26.4-33.7-8.3 9.3v24.3h-19.8zM607.9 105.5v-12.2h19.9v58.4c0 7.1 1.7 9.8 6.1 9.8 1.2 0 2.7-.2 4.1-.3v16.1c-2.2.8-5.5 1.3-9.8 1.3-4.8 0-8.6-.8-11.6-2.7-3.7-2.5-6-5.8-6.8-10.1-5.8 8.8-15.4 13.1-28.7 13.1-11.8 0-21.7-4.1-29.9-12.6-8-8.5-12-18.8-12-31.2s4-22.7 12-31c8.1-8.5 18.1-12.6 29.9-12.6 13.5 0 23.6 6 26.8 14zm-6 47.9c5-4.8 7.5-11 7.5-18.3s-2.5-13.4-7.5-18.3c-4.8-4.8-11-7.3-18.1-7.3-7.1 0-12.9 2.5-17.8 7.3-4.6 4.8-7 11-7 18.3s2.3 13.4 7 18.3c4.8 4.8 10.6 7.3 17.8 7.3 7.1 0 13.3-2.5 18.1-7.3z"/></g><path fill="#0B5567" d="M230.3 212.8c35.9 28.7 58.9-57 1.7-72.8-48-13.3-96.3 9.5-144.7 62.7 0 0 89.4-32.7 143 10.1z"/><path fill="#15A9CE" d="M88.1 202c34.4-35.7 91.6-75.5 144.9-60.8 12.4 3.5 21.2 10.7 26.9 19.3l-50.4-101.7c-7.2-11.5-25.6-9.1-36-.3l-133.2 111.6c-12.1 10.4-12.8 28.9-1.6 40.1 9.9 9.9 25.6 10.8 36.5 2l12.9-10.2z"/></g></svg>
</a></div>

<!--
<a href="https://www.example.com" class="logo show-for-medium">logo</a>
-->
</div>
</header>

<div class="expanded row">

<div class="medium-3 large-2 show-for-medium column">
<nav class="site-nav">
<div class="nav-home">
<a href="index.html" >
<span class="home-icon">⌂</span>Alpakka
</a>
<div class="version-number">
0.20
</div>
</div>
<select class="supergroup" name="Language"><option class="group" value="group-scala">Scala</option><option class="group" value="group-java">Java</option></select>
<div class="nav-toc">
<ul>
  <li><a href="connectors.html" class="page">Connectors</a>
  <ul>
    <li><a href="amqp.html" class="page">AMQP</a></li>
    <li><a href="external/apache-camel.html" class="page">Apache Camel</a></li>
    <li><a href="cassandra.html" class="page">Apache Cassandra</a></li>
    <li><a href="geode.html" class="page">Apache Geode</a></li>
    <li><a href="kafka.html" class="page">Apache Kafka</a></li>
    <li><a href="kudu.html" class="page">Apache Kudu</a></li>
    <li><a href="solr.html" class="page">Apache Solr</a></li>
    <li><a href="dynamodb.html" class="page">AWS DynamoDB</a></li>
    <li><a href="kinesis.html" class="page">AWS Kinesis</a></li>
    <li><a href="awslambda.html" class="page">AWS Lambda</a></li>
    <li><a href="s3.html" class="page">AWS S3</a></li>
    <li><a href="sns.html" class="page">AWS SNS</a></li>
    <li><a href="sqs.html" class="page">AWS SQS</a></li>
    <li><a href="external/azure-event-hubs.html" class="page">Azure Event Hubs</a></li>
    <li><a href="external/azure-iot-hub.html" class="page">Azure IoT Hub</a></li>
    <li><a href="azure-storage-queue.html" class="page">Azure Storage Queue</a></li>
    <li><a href="elasticsearch.html" class="page">Elasticsearch</a></li>
    <li><a href="external/eventuate.html" class="page">Eventuate</a></li>
    <li><a href="file.html" class="page">Files</a></li>
    <li><a href="external/fs2.html" class="page">FS2</a></li>
    <li><a href="ftp.html" class="page">FTP</a></li>
    <li><a href="google-cloud-pub-sub.html" class="page">Google Cloud Pub/Sub</a></li>
    <li><a href="google-fcm.html" class="page">Google Firebase Cloud Messaging</a></li>
    <li><a href="hdfs.html" class="active page">Hadoop Distributed File System - HDFS</a></li>
    <li><a href="hbase.html" class="page">HBase</a></li>
    <li><a href="external/http.html" class="page">HTTP</a></li>
    <li><a href="ironmq.html" class="page">IronMQ</a></li>
    <li><a href="jms.html" class="page">JMS</a></li>
    <li><a href="mongodb.html" class="page">MongoDB</a></li>
    <li><a href="mqtt.html" class="page">MQTT</a></li>
    <li><a href="orientdb.html" class="page">OrientDB</a></li>
    <li><a href="external/pulsar.html" class="page">Pulsar</a></li>
    <li><a href="sse.html" class="page">Server-sent Events (SSE)</a></li>
    <li><a href="slick.html" class="page">Slick (JDBC)</a></li>
    <li><a href="spring-web.html" class="page">Spring Web</a></li>
    <li><a href="external/tcp.html" class="page">TCP</a></li>
    <li><a href="udp.html" class="page">UDP</a></li>
    <li><a href="unix-domain-socket.html" class="page">Unix Domain Socket</a></li>
  </ul></li>
  <li><a href="external-components.html" class="page">External Components</a></li>
  <li><a href="examples/index.html" class="page">Self-contained examples</a>
  <ul>
    <li><a href="examples/csv-samples.html" class="page">CSV</a></li>
    <li><a href="examples/elasticsearch-samples.html" class="page">Elasticsearch</a></li>
    <li><a href="examples/ftp-samples.html" class="page">FTP</a></li>
    <li><a href="examples/jms-samples.html" class="page">JMS</a></li>
  </ul></li>
  <li><a href="other-docs/index.html" class="page">Other documentation</a>
  <ul>
    <li><a href="other-docs/webinars-presentations-articles.html" class="page">Webinars, Presentations and Articles</a></li>
    <li><a href="other-docs/snapshots.html" class="page">Snapshots</a></li>
  </ul></li>
  <li><a href="patterns.html" class="page">Integration Patterns</a></li>
  <li><a href="data-transformations/index.html" class="page">Data Transformations</a>
  <ul>
    <li><a href="data-transformations/parsing-lines.html" class="page">Parsing Lines</a></li>
    <li><a href="data-transformations/json.html" class="page">JSON</a></li>
    <li><a href="data-transformations/compression.html" class="page">Compressing/decompressing</a></li>
    <li><a href="data-transformations/csv.html" class="page">Comma-Separated Values - CSV</a></li>
    <li><a href="data-transformations/recordio.html" class="page">RecordIO Framing</a></li>
    <li><a href="data-transformations/text.html" class="page">Text and charsets</a></li>
    <li><a href="data-transformations/xml.html" class="page">Extensible Markup Language - XML</a></li>
  </ul></li>
</ul>
</div>

</nav>
</div>

<div class="small-12 medium-9 large-10 column">
<section class="site-content">

<span id="version-warning"></span>

<div class="page-header row">
<div class="medium-12 show-for-medium column">
<div class="nav-breadcrumbs">
<ul>
  <li><a href="index.html">Alpakka</a></li>
  <li><a href="connectors.html">Connectors</a></li>
  <li>Hadoop Distributed File System - HDFS</li>
</ul>
</div>
</div>
</div>

<div class="page-content row">
<div class="small-12 large-9 column" id="docs">
<h1><a href="#hadoop-distributed-file-system-hdfs" name="hadoop-distributed-file-system-hdfs" class="anchor"><span class="anchor-link"></span></a>Hadoop Distributed File System - HDFS</h1>
<p>The connector offers Flows and Sources that interact with HDFS file systems.</p>
<p>For more information about Hadoop, please visit the <a href="https://hadoop.apache.org/">Hadoop documentation</a>.</p>
<h3><a href="#reported-issues" name="reported-issues" class="anchor"><span class="anchor-link"></span></a>Reported issues</h3>
<p><a href="https://github.com/akka/alpakka/labels/p%3Ahdfs">Tagged issues at Github</a></p>
<h2><a href="#artifacts" name="artifacts" class="anchor"><span class="anchor-link"></span></a>Artifacts</h2><dl class="dependency"><dt>sbt</dt><dd><pre class="prettyprint"><code class="language-scala">libraryDependencies += "com.lightbend.akka" %% "akka-stream-alpakka-hdfs" % "0.20"</code></pre></dd><dt>Maven</dt><dd><pre class="prettyprint"><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.lightbend.akka&lt;/groupId&gt;
  &lt;artifactId&gt;akka-stream-alpakka-hdfs_2.12&lt;/artifactId&gt;
  &lt;version&gt;0.20&lt;/version&gt;
&lt;/dependency&gt;</code></pre></dd><dt>Gradle</dt><dd><pre class="prettyprint"><code class="language-gradle">dependencies {
  compile group: 'com.lightbend.akka', name: 'akka-stream-alpakka-hdfs_2.12', version: '0.20'
}</code></pre></dd></dl>
<h2><a href="#specifying-a-hadoop-version" name="specifying-a-hadoop-version" class="anchor"><span class="anchor-link"></span></a>Specifying a Hadoop Version</h2>
<p>By default, HDFS connector uses Hadoop <strong>3.1.0</strong>. If you are using a different version of Hadoop, you should exclude the Hadoop libraries from the connector dependency and add the dependency for your preferred version.</p>
<h2><a href="#set-up-client" name="set-up-client" class="anchor"><span class="anchor-link"></span></a>Set up client</h2>
<p>Flows provided by this connector need a prepared <code>org.apache.hadoop.fs.FileSystem</code> to interact with HDFS.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileSystem

val conf = new Configuration()
conf.set(&quot;fs.default.name&quot;, &quot;hdfs://localhost:54310&quot;)

val fs: FileSystem = FileSystem.get(conf)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L34-L40">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Configuration conf = new Configuration();
conf.set(&quot;fs.default.name&quot;, &quot;hdfs://localhost:54310&quot;);

fs = FileSystem.get(conf);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L492-L495">Full source at GitHub</a></dd>
</dl>
<h2><a href="#writing" name="writing" class="anchor"><span class="anchor-link"></span></a>Writing</h2>
<p>The connector provides three Flows. Each flow requires <code>RotationStrategy</code> and <code>SyncStrategy</code> to run. <span class="group-scala"><a href="http://developer.lightbend.com/docs/api/alpakka/0.20/akka/stream/alpakka/hdfs/scaladsl/HdfsFlow$.html">HdfsFlow</a>.</span> <span class="group-java"><a href="http://developer.lightbend.com/docs/api/alpakka/0.20/akka/stream/alpakka/hdfs/javadsl/HdfsFlow$.html">HdfsFlow</a>.</span></p>
<p>The flows push <code>OutgoingMessage</code> to a downstream.</p>
<h3><a href="#data-writer" name="data-writer" class="anchor"><span class="anchor-link"></span></a>Data Writer</h3>
<p>Use <code>HdfsFlow.data</code> to stream with <a href="https://hadoop.apache.org/docs/r3.1.0/api/?org/apache/hadoop/fs/FSDataOutputStream.html">FSDataOutputStream</a> without any compression.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val flow = HdfsFlow.data(
  fs,
  SyncStrategy.count(500),
  RotationStrategy.size(1, FileUnit.GB),
  HdfsWritingSettings()
)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L110-L115">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Flow&lt;HdfsWriteMessage&lt;ByteString, NotUsed&gt;, RotationMessage, NotUsed&gt; flow =
    HdfsFlow.data(
        fs, SyncStrategy.count(500), RotationStrategy.size(1, FileUnit.GB()), settings);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L145-L147">Full source at GitHub</a></dd>
</dl>
<h3><a href="#compressed-data-writer" name="compressed-data-writer" class="anchor"><span class="anchor-link"></span></a>Compressed Data Writer</h3>
<p>First, create <a href="https://hadoop.apache.org/docs/r3.1.0/api/?org/apache/hadoop/io/compress/CompressionCodec.html">CompressionCodec</a>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val codec = new DefaultCodec()
codec.setConf(fs.getConf)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L267-L268">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">DefaultCodec codec = new DefaultCodec();
codec.setConf(fs.getConf());</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L295-L296">Full source at GitHub</a></dd>
</dl>
<p>Then, use <code>HdfsFlow.compress</code> to stream with <a href="https://hadoop.apache.org/docs/r3.1.0/api/?org/apache/hadoop/io/compress/CompressionOutputStream.html">CompressionOutputStream</a> and <a href="https://hadoop.apache.org/docs/r3.1.0/api/?org/apache/hadoop/io/compress/CompressionCodec.html">CompressionCodec</a>. </p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val flow = HdfsFlow.compressed(
  fs,
  SyncStrategy.count(1),
  RotationStrategy.size(0.1, FileUnit.MB),
  codec,
  settings
)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L272-L278">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Flow&lt;HdfsWriteMessage&lt;ByteString, NotUsed&gt;, RotationMessage, NotUsed&gt; flow =
    HdfsFlow.compressed(
        fs, SyncStrategy.count(50), RotationStrategy.size(0.1, FileUnit.MB()), codec, settings);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L300-L302">Full source at GitHub</a></dd>
</dl>
<h3><a href="#sequence-writer" name="sequence-writer" class="anchor"><span class="anchor-link"></span></a>Sequence Writer</h3>
<p>Use <code>HdfsFlow.sequence</code> to stream a flat file consisting of binary key/value pairs.</p>
<h4><a href="#without-compression" name="without-compression" class="anchor"><span class="anchor-link"></span></a>Without Compression</h4>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val flow = HdfsFlow.sequence(
  fs,
  SyncStrategy.none,
  RotationStrategy.size(1, FileUnit.MB),
  settings,
  classOf[Text],
  classOf[Text]
)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L366-L373">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Flow&lt;HdfsWriteMessage&lt;Pair&lt;Text, Text&gt;, NotUsed&gt;, RotationMessage, NotUsed&gt; flow =
    HdfsFlow.sequence(
        fs,
        SyncStrategy.none(),
        RotationStrategy.size(1, FileUnit.MB()),
        SequenceFile.CompressionType.BLOCK,
        codec,
        settings,
        Text.class,
        Text.class);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L417-L426">Full source at GitHub</a></dd>
</dl>
<h4><a href="#with-compression" name="with-compression" class="anchor"><span class="anchor-link"></span></a>With Compression</h4>
<p>First, define a codec.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val codec = new DefaultCodec()
codec.setConf(fs.getConf)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L267-L268">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">DefaultCodec codec = new DefaultCodec();
codec.setConf(fs.getConf());</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L295-L296">Full source at GitHub</a></dd>
</dl>
<p>Then, create a flow.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val flow = HdfsFlow.sequence(
  fs,
  SyncStrategy.none,
  RotationStrategy.size(1, FileUnit.MB),
  CompressionType.BLOCK,
  codec,
  settings,
  classOf[Text],
  classOf[Text]
)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L396-L405">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Flow&lt;HdfsWriteMessage&lt;Pair&lt;Text, Text&gt;, NotUsed&gt;, RotationMessage, NotUsed&gt; flow =
    HdfsFlow.sequence(
        fs,
        SyncStrategy.none(),
        RotationStrategy.size(1, FileUnit.MB()),
        settings,
        Text.class,
        Text.class);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L386-L393">Full source at GitHub</a></dd>
</dl>
<h3><a href="#passing-data-through-hdfsflow" name="passing-data-through-hdfsflow" class="anchor"><span class="anchor-link"></span></a>Passing data through HdfsFlow</h3>
<p>Use <code>HdfsFlow.dataWithPassThrough</code>, <code>HdfsFlow.compressedWithPassThrough</code> or <code>HdfsFlow.sequenceWithPassThrough</code>.</p>
<p>When streaming documents from Kafka, you might want to commit to Kafka. The flow will emit two messages. For every input, it will produce <code>WrittenMessage</code> and when it rotates, <code>RotationMessage</code>.</p>
<p>Let&rsquo;s say that we have these classes.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">case class Book(title: String)
case class KafkaOffset(offset: Int)
case class KafkaMessage(book: Book, offset: KafkaOffset)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L198-L200">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">public static class Book {
  final String title;

  Book(String title) {
    this.title = title;
  }
}

static class KafkaCommitter {
  List&lt;Integer&gt; committedOffsets = new ArrayList&lt;&gt;();

  void commit(KafkaOffset offset) {
    committedOffsets.add(offset.offset);
  }
}

static class KafkaOffset {
  final int offset;

  KafkaOffset(int offset) {
    this.offset = offset;
  }
}

static class KafkaMessage {
  final Book book;
  final KafkaOffset offset;

  KafkaMessage(Book book, KafkaOffset offset) {
    this.book = book;
    this.offset = offset;
  }
}</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L58-L90">Full source at GitHub</a></dd>
</dl>
<p>Then, we can stream with <code>passThrough</code>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">// We&#39;re going to pretend we got messages from kafka.
// After we&#39;ve written them to HDFS, we want
// to commit the offset to Kafka
val messagesFromKafka = List(
  KafkaMessage(Book(&quot;Akka Concurrency&quot;), KafkaOffset(0)),
  KafkaMessage(Book(&quot;Akka in Action&quot;), KafkaOffset(1)),
  KafkaMessage(Book(&quot;Effective Akka&quot;), KafkaOffset(2)),
  KafkaMessage(Book(&quot;Learning Scala&quot;), KafkaOffset(3)),
  KafkaMessage(Book(&quot;Scala Puzzlers&quot;), KafkaOffset(4)),
  KafkaMessage(Book(&quot;Scala for Spark in Production&quot;), KafkaOffset(5))
)

var committedOffsets = List[KafkaOffset]()

def commitToKafka(offset: KafkaOffset): Unit =
  committedOffsets = committedOffsets :+ offset

val resF = Source(messagesFromKafka)
  .map { kafkaMessage: KafkaMessage =&gt;
    val book = kafkaMessage.book
    // Transform message so that we can write to hdfs
    HdfsWriteMessage(ByteString(book.title), kafkaMessage.offset)
  }
  .via(
    HdfsFlow.dataWithPassThrough[KafkaOffset](
      fs,
      SyncStrategy.count(50),
      RotationStrategy.count(4),
      HdfsWritingSettings(newLine = true)
    )
  )
  .map { message =&gt;
    message match {
      case WrittenMessage(passThrough, _) =&gt;
        commitToKafka(passThrough)
      case _ =&gt; ()
    }
    message
  }
  .collect {
    case rm: RotationMessage =&gt; rm
  }
  .runWith(Sink.seq)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L204-L246">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">// We&#39;re going to pretend we got messages from kafka.
// After we&#39;ve written them to HDFS, we want
// to commit the offset to Kafka
List&lt;KafkaMessage&gt; messagesFromKafka =
    Arrays.asList(
        new KafkaMessage(new Book(&quot;Akka Concurrency&quot;), new KafkaOffset(0)),
        new KafkaMessage(new Book(&quot;Akka in Action&quot;), new KafkaOffset(1)),
        new KafkaMessage(new Book(&quot;Effective Akka&quot;), new KafkaOffset(2)),
        new KafkaMessage(new Book(&quot;Learning Scala&quot;), new KafkaOffset(3)),
        new KafkaMessage(new Book(&quot;Scala Puzzlers&quot;), new KafkaOffset(4)),
        new KafkaMessage(new Book(&quot;Scala for Spark in Production&quot;), new KafkaOffset(5)));

final KafkaCommitter kafkaCommitter = new KafkaCommitter();

Flow&lt;HdfsWriteMessage&lt;ByteString, KafkaOffset&gt;, OutgoingMessage&lt;KafkaOffset&gt;, NotUsed&gt; flow =
    HdfsFlow.dataWithPassThrough(
        fs,
        SyncStrategy.count(50),
        RotationStrategy.count(4),
        HdfsWritingSettings.create().withNewLine(true));

CompletionStage&lt;List&lt;RotationMessage&gt;&gt; resF =
    Source.from(messagesFromKafka)
        .map(
            kafkaMessage -&gt; {
              Book book = kafkaMessage.book;
              // Transform message so that we can write to hdfs\
              return HdfsWriteMessage.create(
                  ByteString.fromString(book.title), kafkaMessage.offset);
            })
        .via(flow)
        .map(
            message -&gt; {
              if (message instanceof WrittenMessage) {
                kafkaCommitter.commit(((WrittenMessage&lt;KafkaOffset&gt;) message).passThrough());
                return message;
              } else {
                return message;
              }
            })
        .collectType(RotationMessage.class) // Collect only rotation messages
        .runWith(Sink.seq(), materializer);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L230-L271">Full source at GitHub</a></dd>
</dl>
<h2><a href="#configuration" name="configuration" class="anchor"><span class="anchor-link"></span></a>Configuration</h2>
<p>We can configure the sink by <code>HdfsWritingSettings</code>. </p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val settings =
  HdfsWritingSettings(
    overwrite = true,
    newLine = false,
    lineSeparator = System.getProperty(&quot;line.separator&quot;),
    pathGenerator = pathGenerator
  )</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L499-L505">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">HdfsWritingSettings.create()
    .withOverwrite(true)
    .withNewLine(false)
    .withLineSeparator(System.getProperty(&quot;line.separator&quot;))
    .withPathGenerator(pathGenerator);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L528-L532">Full source at GitHub</a></dd>
</dl>
<h3><a href="#file-path-generator" name="file-path-generator" class="anchor"><span class="anchor-link"></span></a>File path generator</h3>
<p><a href="http://developer.lightbend.com/docs/api/alpakka/0.20/akka/stream/alpakka/hdfs/FilePathGenerator$.html">FilePathGenerator</a> provides a functionality to generate rotation path in HDFS. </p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val pathGenerator =
  FilePathGenerator(
    (rotationCount: Long, timestamp: Long) =&gt; s&quot;/tmp/alpakka/$rotationCount-$timestamp&quot;
  )</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L493-L496">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">BiFunction&lt;Long, Long, String&gt; func =
    (rotationCount, timestamp) -&gt; &quot;/tmp/alpakka/&quot; + rotationCount + &quot;-&quot; + timestamp;
FilePathGenerator pathGenerator = FilePathGenerator.create(func);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L523-L525">Full source at GitHub</a></dd>
</dl>
<h3><a href="#rotation-strategy" name="rotation-strategy" class="anchor"><span class="anchor-link"></span></a>Rotation Strategy</h3>
<p><a href="http://developer.lightbend.com/docs/api/alpakka/0.20/akka/stream/alpakka/hdfs/RotationStrategyFactory$.html">RotationStrategy</a> provides a functionality to decide when to rotate files.</p>
<h3><a href="#sync-strategy" name="sync-strategy" class="anchor"><span class="anchor-link"></span></a>Sync Strategy</h3>
<p><a href="http://developer.lightbend.com/docs/api/alpakka/0.20/akka/stream/alpakka/hdfs/SyncStrategyFactory$.html">SyncStrategy</a> provides a functionality to decide when to synchronize the output.</p>
<h2><a href="#reading" name="reading" class="anchor"><span class="anchor-link"></span></a>Reading</h2>
<p>Use <code>HdfsSource</code> to read from HDFS. <span class="group-scala"><a href="http://developer.lightbend.com/docs/api/alpakka/0.20/akka/stream/alpakka/hdfs/scaladsl/HdfsSource$.html">HdfsSource</a>.</span> <span class="group-java"><a href="http://developer.lightbend.com/docs/api/alpakka/0.20/akka/stream/alpakka/hdfs/javadsl/HdfsSource$.html">HdfsSource</a>.</span></p>
<h3><a href="#data-reader" name="data-reader" class="anchor"><span class="anchor-link"></span></a>Data Reader</h3>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val source = HdfsSource.data(fs, path)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsReaderSpec.scala#L61">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Source&lt;ByteString, CompletionStage&lt;IOResult&gt;&gt; source = HdfsSource.data(fs, path);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsReaderTest.java#L64">Full source at GitHub</a></dd>
</dl>
<h3><a href="#compressed-data-reader" name="compressed-data-reader" class="anchor"><span class="anchor-link"></span></a>Compressed Data Reader</h3>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val source = HdfsSource.compressed(fs, path, codec)</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsReaderSpec.scala#L99">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Source&lt;ByteString, CompletionStage&lt;IOResult&gt;&gt; source = HdfsSource.compressed(fs, path, codec);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsReaderTest.java#L103">Full source at GitHub</a></dd>
</dl>
<h3><a href="#sequence-reader" name="sequence-reader" class="anchor"><span class="anchor-link"></span></a>Sequence Reader</h3>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val source = HdfsSource.sequence(fs, path, classOf[Text], classOf[Text])</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsReaderSpec.scala#L135">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Source&lt;Pair&lt;Text, Text&gt;, NotUsed&gt; source =
    HdfsSource.sequence(fs, path, Text.class, Text.class);</code></pre><a href="https://github.com/akka/alpakka/tree/master/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsReaderTest.java#L144-L145">Full source at GitHub</a></dd>
</dl>
<h2><a href="#running-the-example-code" name="running-the-example-code" class="anchor"><span class="anchor-link"></span></a>Running the example code</h2>
<p>The code in this guide is part of runnable tests of this project. You are welcome to edit the code and run it in sbt.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre><code>sbt
&gt; hdfs/testOnly *.HdfsWriterSpec
&gt; hdfs/testOnly *.HdfsReaderSpec
</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre><code>sbt
&gt; hdfs/testOnly *.HdfsWriterTest
&gt; hdfs/testOnly *.HdfsReaderTest
</code></pre></dd>
</dl>
<div class="source-github">
The source code for this page can be found <a href="https://github.com/akka/alpakka/tree/master/docs/src/main/paradox/hdfs.md">here</a>.
</div>

<div class="nav-next">
<p><strong>Next:</strong> <a href="hbase.html">HBase</a></p>
</div>
</div>
<div class="large-3 show-for-large column" data-sticky-container>
<nav class="sidebar sticky" data-sticky data-anchor="docs" data-sticky-on="large">
<div class="page-nav">
<div class="nav-title">On this page:</div>
<div class="nav-toc">
<ul>
  <li><a href="hdfs.html#hadoop-distributed-file-system-hdfs" class="header">Hadoop Distributed File System - HDFS</a>
  <ul>
    <li><a href="hdfs.html#reported-issues" class="header">Reported issues</a></li>
    <li><a href="hdfs.html#artifacts" class="header">Artifacts</a></li>
    <li><a href="hdfs.html#specifying-a-hadoop-version" class="header">Specifying a Hadoop Version</a></li>
    <li><a href="hdfs.html#set-up-client" class="header">Set up client</a></li>
    <li><a href="hdfs.html#writing" class="header">Writing</a></li>
    <li><a href="hdfs.html#configuration" class="header">Configuration</a></li>
    <li><a href="hdfs.html#reading" class="header">Reading</a></li>
    <li><a href="hdfs.html#running-the-example-code" class="header">Running the example code</a></li>
  </ul></li>
</ul>
</div>
</div>
</nav>
</div>
</div>

</section>
</div>

</div>

<footer class="site-footer">

<section class="site-footer-nav">
<div class="expanded row">
<div class="small-12 large-offset-2 large-10 column">
<div class="row site-footer-content">

<div class="small-12 medium-4 large-3 text-center column">
<div class="nav-links">
<ul>
<!-- <li><a href="https://www.example.com/products/">Products</a> -->
</ul>
</div>
</div>

</div>
</div>
</div>
</section>

<section class="site-footer-base">
<div class="expanded row">
<div class="small-12 large-offset-2 large-10 column">
<div class="row site-footer-content">

<div class="small-12 text-center large-9 column">

<!--
<div class="copyright">
<span class="text">&copy; 2019</span>
<a href="https://www.example.com" class="logo">logo</a>
</div>
-->
</div>

</div>
</div>
</div>
</section>
</footer>

</div>
</div>
</div>
</body>

<script type="text/javascript" src="lib/foundation/dist/foundation.min.js"></script>
<script type="text/javascript">jQuery(document).foundation();</script>
<script type="text/javascript" src="js/magellan.js"></script>

<style type="text/css">@import "lib/prettify/prettify.css";</style>
<script type="text/javascript" src="lib/prettify/prettify.js"></script>
<script type="text/javascript" src="lib/prettify/lang-scala.js"></script>
<script type="text/javascript">jQuery(function(){window.prettyPrint && prettyPrint()});</script>
<script type="text/javascript">jQuery(function(jq){initOldVersionWarnings(jq, '0.20', '')});</script>


</html>
