<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
<title>Hadoop Distributed File System - HDFS &bull; Alpakka Documentation</title>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="description" content="Alpakka is a Reactive Enterprise Integration library for Java and Scala, based on Reactive Streams and Akka."/><link rel="canonical" href="https://doc.akka.io/docs/alpakka/current/hdfs.html"/>
<script type="text/javascript" src="lib/jquery/jquery.min.js"></script>
<script type="text/javascript" src="lib/foundation/dist/js/foundation.min.js"></script>
<link rel="stylesheet" type="text/css" href="lib/normalize.css/normalize.css"/>
<link rel="stylesheet" type="text/css" href="lib/foundation/dist/css/foundation.min.css"/>
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/docsearch.js/2/docsearch.min.css" />
<link rel="stylesheet" type="text/css" href="css/icons.css"/>
<link rel="stylesheet" type="text/css" href="css/page.css"/>
<link rel="shortcut icon" href="images/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/manifest.json">
<meta name="msapplication-TileImage" content="images/mstile-150x150.png">
<meta name="msapplication-TileColor" content="#15a9ce">
<meta name="theme-color" content="#15a9ce">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) start -->
<script src="https://optanon.blob.core.windows.net/consent/159bb13d-6748-4399-806e-ac28db879785.js" type="text/javascript" charset="UTF-8"></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) end -->
<!--Google Analytics-->
<script type="text/plain" class="optanon-category-2">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', '']);
_gaq.push(['_setDomainName', '']);
_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})()
</script>
<script type="text/plain" class="optanon-category-2">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-23127719-1', 'lightbend.com', {'allowLinker': true, 'name': 'tsTracker'});
ga('tsTracker.require', 'linker');
ga('tsTracker.linker:autoLink', ['lightbend.com','playframework.com','scala-lang.org','scaladays.org','spray.io','akka.io','scala-sbt.org','scala-ide.org']);
ga('tsTracker.send', 'pageview');
</script>
<!--Marketo-->
<script type="text/plain" class="optanon-category-3">
(function() {
var didInit = false;
function initMunchkin() {
if(didInit === false) {
didInit = true;
Munchkin.init('558-NCX-702', { 'asyncOnly': true, 'disableClickDelay': true });
}
}
var s = document.createElement('script');
s.type = 'text/javascript';
s.async = true;
s.src = '//munchkin.marketo.net/munchkin.js';
s.onreadystatechange = function() {
if (this.readyState == 'complete' || this.readyState == 'loaded') {
initMunchkin();
}
};
s.onload = initMunchkin;
document.getElementsByTagName('head')[0].appendChild(s);
})();
</script>
</head>

<body id="underlay" data-toggler="nav-open">

<header class="site-header hide-for-large">
<div class="sticky-header clearfix">
<a href="https://akka.io"><img class="logo" src="images/akka-alpakka-reverse.svg"></a>

<button class="menu-icon float-right" type="button" data-toggle="underlay overlay"></button>
</div>
<div id="overlay" class="overlay-nav" data-toggler="nav-open">
<header class="nav-header">
<div class="nav-header-title">
<h1><a href="index.html">Alpakka Documentation</a></h1>
</div>
<div class="nav-header-version">
Version 1.0-M1
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="nav-toc">
<ul>
  <li><a href="overview.html" class="page">Overview</a></li>
  <li><a href="data-transformations/index.html" class="page">Data Transformations</a></li>
  <li><a href="amqp.html" class="page">AMQP</a></li>
  <li><a href="external/apache-camel.html" class="page">Apache Camel</a></li>
  <li><a href="cassandra.html" class="page">Apache Cassandra</a></li>
  <li><a href="geode.html" class="page">Apache Geode</a></li>
  <li><a href="kafka.html" class="page">Apache Kafka</a></li>
  <li><a href="kudu.html" class="page">Apache Kudu</a></li>
  <li><a href="solr.html" class="page">Apache Solr</a></li>
  <li><a href="avroparquet.html" class="page">Avro Parquet</a></li>
  <li><a href="dynamodb.html" class="page">AWS DynamoDB</a></li>
  <li><a href="kinesis.html" class="page">AWS Kinesis</a></li>
  <li><a href="awslambda.html" class="page">AWS Lambda</a></li>
  <li><a href="s3.html" class="page">AWS S3</a></li>
  <li><a href="sns.html" class="page">AWS SNS</a></li>
  <li><a href="sqs.html" class="page">AWS SQS</a></li>
  <li><a href="external/azure-event-hubs.html" class="page">Azure Event Hubs</a></li>
  <li><a href="external/azure-iot-hub.html" class="page">Azure IoT Hub</a></li>
  <li><a href="azure-storage-queue.html" class="page">Azure Storage Queue</a></li>
  <li><a href="external/couchbase.html" class="page">Couchbase</a></li>
  <li><a href="elasticsearch.html" class="page">Elasticsearch</a></li>
  <li><a href="external/eventuate.html" class="page">Eventuate</a></li>
  <li><a href="file.html" class="page">Files</a></li>
  <li><a href="external/fs2.html" class="page">FS2</a></li>
  <li><a href="ftp.html" class="page">FTP</a></li>
  <li><a href="google-cloud-pub-sub.html" class="page">Google Cloud Pub/Sub</a></li>
  <li><a href="google-cloud-pub-sub-grpc.html" class="page">Google Cloud Pub/Sub gRPC</a></li>
  <li><a href="google-fcm.html" class="page">Google Firebase Cloud Messaging</a></li>
  <li><a href="external/grpc.html" class="page">gRPC</a></li>
  <li><a href="hdfs.html#hadoop-distributed-file-system-hdfs" class="active page">Hadoop Distributed File System - HDFS</a>
  <ul>
    <li><a href="hdfs.html#reported-issues" class="header">Reported issues</a></li>
    <li><a href="hdfs.html#artifacts" class="header">Artifacts</a></li>
    <li><a href="hdfs.html#specifying-a-hadoop-version" class="header">Specifying a Hadoop Version</a></li>
    <li><a href="hdfs.html#set-up-client" class="header">Set up client</a></li>
    <li><a href="hdfs.html#writing" class="header">Writing</a></li>
    <li><a href="hdfs.html#configuration" class="header">Configuration</a></li>
    <li><a href="hdfs.html#reading" class="header">Reading</a></li>
    <li><a href="hdfs.html#running-the-example-code" class="header">Running the example code</a></li>
  </ul></li>
  <li><a href="hbase.html" class="page">HBase</a></li>
  <li><a href="external/http.html" class="page">HTTP</a></li>
  <li><a href="ironmq.html" class="page">IronMQ</a></li>
  <li><a href="jms.html" class="page">JMS</a></li>
  <li><a href="mongodb.html" class="page">MongoDB</a></li>
  <li><a href="mqtt.html" class="page">MQTT</a></li>
  <li><a href="orientdb.html" class="page">OrientDB</a></li>
  <li><a href="external/pulsar.html" class="page">Pulsar</a></li>
  <li><a href="sse.html" class="page">Server-sent Events (SSE)</a></li>
  <li><a href="slick.html" class="page">Slick (JDBC)</a></li>
  <li><a href="spring-web.html" class="page">Spring Web</a></li>
  <li><a href="external/tcp.html" class="page">TCP</a></li>
  <li><a href="udp.html" class="page">UDP</a></li>
  <li><a href="unix-domain-socket.html" class="page">Unix Domain Socket</a></li>
</ul>
</nav>
</div>
</header>

<aside class="show-for-large">
<header class="nav-header fixed-sidebar-header">
<div class="nav-header-title">
<h1><a href="index.html">Alpakka Documentation</a></h1>
</div>
<div class="nav-header-version">
Version 1.0-M1
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="site-nav fixed-sidebar-contents">
<div class="nav-toc">
<ul>
  <li><a href="overview.html" class="page">Overview</a></li>
  <li><a href="data-transformations/index.html" class="page">Data Transformations</a></li>
  <li><a href="amqp.html" class="page">AMQP</a></li>
  <li><a href="external/apache-camel.html" class="page">Apache Camel</a></li>
  <li><a href="cassandra.html" class="page">Apache Cassandra</a></li>
  <li><a href="geode.html" class="page">Apache Geode</a></li>
  <li><a href="kafka.html" class="page">Apache Kafka</a></li>
  <li><a href="kudu.html" class="page">Apache Kudu</a></li>
  <li><a href="solr.html" class="page">Apache Solr</a></li>
  <li><a href="avroparquet.html" class="page">Avro Parquet</a></li>
  <li><a href="dynamodb.html" class="page">AWS DynamoDB</a></li>
  <li><a href="kinesis.html" class="page">AWS Kinesis</a></li>
  <li><a href="awslambda.html" class="page">AWS Lambda</a></li>
  <li><a href="s3.html" class="page">AWS S3</a></li>
  <li><a href="sns.html" class="page">AWS SNS</a></li>
  <li><a href="sqs.html" class="page">AWS SQS</a></li>
  <li><a href="external/azure-event-hubs.html" class="page">Azure Event Hubs</a></li>
  <li><a href="external/azure-iot-hub.html" class="page">Azure IoT Hub</a></li>
  <li><a href="azure-storage-queue.html" class="page">Azure Storage Queue</a></li>
  <li><a href="external/couchbase.html" class="page">Couchbase</a></li>
  <li><a href="elasticsearch.html" class="page">Elasticsearch</a></li>
  <li><a href="external/eventuate.html" class="page">Eventuate</a></li>
  <li><a href="file.html" class="page">Files</a></li>
  <li><a href="external/fs2.html" class="page">FS2</a></li>
  <li><a href="ftp.html" class="page">FTP</a></li>
  <li><a href="google-cloud-pub-sub.html" class="page">Google Cloud Pub/Sub</a></li>
  <li><a href="google-cloud-pub-sub-grpc.html" class="page">Google Cloud Pub/Sub gRPC</a></li>
  <li><a href="google-fcm.html" class="page">Google Firebase Cloud Messaging</a></li>
  <li><a href="external/grpc.html" class="page">gRPC</a></li>
  <li><a href="hdfs.html#hadoop-distributed-file-system-hdfs" class="active page">Hadoop Distributed File System - HDFS</a>
  <ul>
    <li><a href="hdfs.html#reported-issues" class="header">Reported issues</a></li>
    <li><a href="hdfs.html#artifacts" class="header">Artifacts</a></li>
    <li><a href="hdfs.html#specifying-a-hadoop-version" class="header">Specifying a Hadoop Version</a></li>
    <li><a href="hdfs.html#set-up-client" class="header">Set up client</a></li>
    <li><a href="hdfs.html#writing" class="header">Writing</a></li>
    <li><a href="hdfs.html#configuration" class="header">Configuration</a></li>
    <li><a href="hdfs.html#reading" class="header">Reading</a></li>
    <li><a href="hdfs.html#running-the-example-code" class="header">Running the example code</a></li>
  </ul></li>
  <li><a href="hbase.html" class="page">HBase</a></li>
  <li><a href="external/http.html" class="page">HTTP</a></li>
  <li><a href="ironmq.html" class="page">IronMQ</a></li>
  <li><a href="jms.html" class="page">JMS</a></li>
  <li><a href="mongodb.html" class="page">MongoDB</a></li>
  <li><a href="mqtt.html" class="page">MQTT</a></li>
  <li><a href="orientdb.html" class="page">OrientDB</a></li>
  <li><a href="external/pulsar.html" class="page">Pulsar</a></li>
  <li><a href="sse.html" class="page">Server-sent Events (SSE)</a></li>
  <li><a href="slick.html" class="page">Slick (JDBC)</a></li>
  <li><a href="spring-web.html" class="page">Spring Web</a></li>
  <li><a href="external/tcp.html" class="page">TCP</a></li>
  <li><a href="udp.html" class="page">UDP</a></li>
  <li><a href="unix-domain-socket.html" class="page">Unix Domain Socket</a></li>
</ul>
</div>
</nav>
<footer class="nav-footer fixed-sidebar-footer">
<a href="https://akka.io"><img class="logo" src="images/akka-alpakka-reverse.svg"></a>

</footer>
</aside>

<main class="fixed-shift-for-large expanded row">
<section class="site-content small-12 column">

<article class="page-content row">
<div class="small-12 large-9 column" id="docs">
<h1><a href="#hadoop-distributed-file-system-hdfs" name="hadoop-distributed-file-system-hdfs" class="anchor"><span class="anchor-link"></span></a>Hadoop Distributed File System - HDFS</h1>
<p>The connector offers Flows and Sources that interact with HDFS file systems.</p>
<p>For more information about Hadoop, please visit the <a href="https://hadoop.apache.org/">Hadoop documentation</a>.</p>
<h3><a href="#reported-issues" name="reported-issues" class="anchor"><span class="anchor-link"></span></a>Reported issues</h3>
<p><a href="https://github.com/akka/alpakka/labels/p%3Ahdfs">Tagged issues at Github</a></p>
<h2><a href="#artifacts" name="artifacts" class="anchor"><span class="anchor-link"></span></a>Artifacts</h2><dl class="dependency"><dt>sbt</dt><dd><pre class="prettyprint"><code class="language-scala">libraryDependencies += "com.lightbend.akka" %% "akka-stream-alpakka-hdfs" % "1.0-M1"</code></pre></dd><dt>Maven</dt><dd><pre class="prettyprint"><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.lightbend.akka&lt;/groupId&gt;
  &lt;artifactId&gt;akka-stream-alpakka-hdfs_2.12&lt;/artifactId&gt;
  &lt;version&gt;1.0-M1&lt;/version&gt;
&lt;/dependency&gt;</code></pre></dd><dt>Gradle</dt><dd><pre class="prettyprint"><code class="language-gradle">dependencies {
  compile group: 'com.lightbend.akka', name: 'akka-stream-alpakka-hdfs_2.12', version: '1.0-M1'
}</code></pre></dd></dl>
<h2><a href="#specifying-a-hadoop-version" name="specifying-a-hadoop-version" class="anchor"><span class="anchor-link"></span></a>Specifying a Hadoop Version</h2>
<p>By default, HDFS connector uses Hadoop <strong>3.1.0</strong>. If you are using a different version of Hadoop, you should exclude the Hadoop libraries from the connector dependency and add the dependency for your preferred version.</p>
<h2><a href="#set-up-client" name="set-up-client" class="anchor"><span class="anchor-link"></span></a>Set up client</h2>
<p>Flows provided by this connector need a prepared <code>org.apache.hadoop.fs.FileSystem</code> to interact with HDFS.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileSystem

val conf = new Configuration()
conf.set(&quot;fs.default.name&quot;, &quot;hdfs://localhost:54310&quot;)

val fs: FileSystem = FileSystem.get(conf)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L34-L40" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Configuration conf = new Configuration();
conf.set(&quot;fs.default.name&quot;, &quot;hdfs://localhost:54310&quot;);

fs = FileSystem.get(conf);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L492-L495" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h2><a href="#writing" name="writing" class="anchor"><span class="anchor-link"></span></a>Writing</h2>
<p>The connector provides three Flows. Each flow requires <code>RotationStrategy</code> and <code>SyncStrategy</code> to run. <span class="group-scala"><a href="https://doc.akka.io/api/alpakka/1.0-M1/akka/stream/alpakka/hdfs/scaladsl/HdfsFlow$.html">HdfsFlow</a>.</span> <span class="group-java"><a href="https://doc.akka.io/api/alpakka/1.0-M1/akka/stream/alpakka/hdfs/javadsl/HdfsFlow$.html">HdfsFlow</a>.</span></p>
<p>The flows push <code>OutgoingMessage</code> to a downstream.</p>
<h3><a href="#data-writer" name="data-writer" class="anchor"><span class="anchor-link"></span></a>Data Writer</h3>
<p>Use <code>HdfsFlow.data</code> to stream with <a href="https://hadoop.apache.org/docs/r3.1.0/api/?org/apache/hadoop/fs/FSDataOutputStream.html">FSDataOutputStream</a> without any compression.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val flow = HdfsFlow.data(
  fs,
  SyncStrategy.count(500),
  RotationStrategy.size(1, FileUnit.GB),
  HdfsWritingSettings()
)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L110-L115" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Flow&lt;HdfsWriteMessage&lt;ByteString, NotUsed&gt;, RotationMessage, NotUsed&gt; flow =
    HdfsFlow.data(
        fs, SyncStrategy.count(500), RotationStrategy.size(1, FileUnit.GB()), settings);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L145-L147" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h3><a href="#compressed-data-writer" name="compressed-data-writer" class="anchor"><span class="anchor-link"></span></a>Compressed Data Writer</h3>
<p>First, create <a href="https://hadoop.apache.org/docs/r3.1.0/api/?org/apache/hadoop/io/compress/CompressionCodec.html">CompressionCodec</a>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val codec = new DefaultCodec()
codec.setConf(fs.getConf)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L267-L268" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">DefaultCodec codec = new DefaultCodec();
codec.setConf(fs.getConf());</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L295-L296" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<p>Then, use <code>HdfsFlow.compress</code> to stream with <a href="https://hadoop.apache.org/docs/r3.1.0/api/?org/apache/hadoop/io/compress/CompressionOutputStream.html">CompressionOutputStream</a> and <a href="https://hadoop.apache.org/docs/r3.1.0/api/?org/apache/hadoop/io/compress/CompressionCodec.html">CompressionCodec</a>. </p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val flow = HdfsFlow.compressed(
  fs,
  SyncStrategy.count(1),
  RotationStrategy.size(0.1, FileUnit.MB),
  codec,
  settings
)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L272-L278" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Flow&lt;HdfsWriteMessage&lt;ByteString, NotUsed&gt;, RotationMessage, NotUsed&gt; flow =
    HdfsFlow.compressed(
        fs, SyncStrategy.count(50), RotationStrategy.size(0.1, FileUnit.MB()), codec, settings);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L300-L302" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h3><a href="#sequence-writer" name="sequence-writer" class="anchor"><span class="anchor-link"></span></a>Sequence Writer</h3>
<p>Use <code>HdfsFlow.sequence</code> to stream a flat file consisting of binary key/value pairs.</p>
<h4><a href="#without-compression" name="without-compression" class="anchor"><span class="anchor-link"></span></a>Without Compression</h4>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val flow = HdfsFlow.sequence(
  fs,
  SyncStrategy.none,
  RotationStrategy.size(1, FileUnit.MB),
  settings,
  classOf[Text],
  classOf[Text]
)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L366-L373" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Flow&lt;HdfsWriteMessage&lt;Pair&lt;Text, Text&gt;, NotUsed&gt;, RotationMessage, NotUsed&gt; flow =
    HdfsFlow.sequence(
        fs,
        SyncStrategy.none(),
        RotationStrategy.size(1, FileUnit.MB()),
        SequenceFile.CompressionType.BLOCK,
        codec,
        settings,
        Text.class,
        Text.class);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L417-L426" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h4><a href="#with-compression" name="with-compression" class="anchor"><span class="anchor-link"></span></a>With Compression</h4>
<p>First, define a codec.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val codec = new DefaultCodec()
codec.setConf(fs.getConf)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L267-L268" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">DefaultCodec codec = new DefaultCodec();
codec.setConf(fs.getConf());</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L295-L296" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<p>Then, create a flow.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val flow = HdfsFlow.sequence(
  fs,
  SyncStrategy.none,
  RotationStrategy.size(1, FileUnit.MB),
  CompressionType.BLOCK,
  codec,
  settings,
  classOf[Text],
  classOf[Text]
)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L396-L405" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Flow&lt;HdfsWriteMessage&lt;Pair&lt;Text, Text&gt;, NotUsed&gt;, RotationMessage, NotUsed&gt; flow =
    HdfsFlow.sequence(
        fs,
        SyncStrategy.none(),
        RotationStrategy.size(1, FileUnit.MB()),
        settings,
        Text.class,
        Text.class);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L386-L393" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h3><a href="#passing-data-through-hdfsflow" name="passing-data-through-hdfsflow" class="anchor"><span class="anchor-link"></span></a>Passing data through HdfsFlow</h3>
<p>Use <code>HdfsFlow.dataWithPassThrough</code>, <code>HdfsFlow.compressedWithPassThrough</code> or <code>HdfsFlow.sequenceWithPassThrough</code>.</p>
<p>When streaming documents from Kafka, you might want to commit to Kafka. The flow will emit two messages. For every input, it will produce <code>WrittenMessage</code> and when it rotates, <code>RotationMessage</code>.</p>
<p>Let&rsquo;s say that we have these classes.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">case class Book(title: String)
case class KafkaOffset(offset: Int)
case class KafkaMessage(book: Book, offset: KafkaOffset)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L198-L200" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">public static class Book {
  final String title;

  Book(String title) {
    this.title = title;
  }
}

static class KafkaCommitter {
  List&lt;Integer&gt; committedOffsets = new ArrayList&lt;&gt;();

  void commit(KafkaOffset offset) {
    committedOffsets.add(offset.offset);
  }
}

static class KafkaOffset {
  final int offset;

  KafkaOffset(int offset) {
    this.offset = offset;
  }
}

static class KafkaMessage {
  final Book book;
  final KafkaOffset offset;

  KafkaMessage(Book book, KafkaOffset offset) {
    this.book = book;
    this.offset = offset;
  }
}</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L58-L90" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<p>Then, we can stream with <code>passThrough</code>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">// We&#39;re going to pretend we got messages from kafka.
// After we&#39;ve written them to HDFS, we want
// to commit the offset to Kafka
val messagesFromKafka = List(
  KafkaMessage(Book(&quot;Akka Concurrency&quot;), KafkaOffset(0)),
  KafkaMessage(Book(&quot;Akka in Action&quot;), KafkaOffset(1)),
  KafkaMessage(Book(&quot;Effective Akka&quot;), KafkaOffset(2)),
  KafkaMessage(Book(&quot;Learning Scala&quot;), KafkaOffset(3)),
  KafkaMessage(Book(&quot;Scala Puzzlers&quot;), KafkaOffset(4)),
  KafkaMessage(Book(&quot;Scala for Spark in Production&quot;), KafkaOffset(5))
)

var committedOffsets = List[KafkaOffset]()

def commitToKafka(offset: KafkaOffset): Unit =
  committedOffsets = committedOffsets :+ offset

val resF = Source(messagesFromKafka)
  .map { kafkaMessage: KafkaMessage =&gt;
    val book = kafkaMessage.book
    // Transform message so that we can write to hdfs
    HdfsWriteMessage(ByteString(book.title), kafkaMessage.offset)
  }
  .via(
    HdfsFlow.dataWithPassThrough[KafkaOffset](
      fs,
      SyncStrategy.count(50),
      RotationStrategy.count(4),
      HdfsWritingSettings(newLine = true)
    )
  )
  .map { message =&gt;
    message match {
      case WrittenMessage(passThrough, _) =&gt;
        commitToKafka(passThrough)
      case _ =&gt; ()
    }
    message
  }
  .collect {
    case rm: RotationMessage =&gt; rm
  }
  .runWith(Sink.seq)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L204-L246" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">// We&#39;re going to pretend we got messages from kafka.
// After we&#39;ve written them to HDFS, we want
// to commit the offset to Kafka
List&lt;KafkaMessage&gt; messagesFromKafka =
    Arrays.asList(
        new KafkaMessage(new Book(&quot;Akka Concurrency&quot;), new KafkaOffset(0)),
        new KafkaMessage(new Book(&quot;Akka in Action&quot;), new KafkaOffset(1)),
        new KafkaMessage(new Book(&quot;Effective Akka&quot;), new KafkaOffset(2)),
        new KafkaMessage(new Book(&quot;Learning Scala&quot;), new KafkaOffset(3)),
        new KafkaMessage(new Book(&quot;Scala Puzzlers&quot;), new KafkaOffset(4)),
        new KafkaMessage(new Book(&quot;Scala for Spark in Production&quot;), new KafkaOffset(5)));

final KafkaCommitter kafkaCommitter = new KafkaCommitter();

Flow&lt;HdfsWriteMessage&lt;ByteString, KafkaOffset&gt;, OutgoingMessage&lt;KafkaOffset&gt;, NotUsed&gt; flow =
    HdfsFlow.dataWithPassThrough(
        fs,
        SyncStrategy.count(50),
        RotationStrategy.count(4),
        HdfsWritingSettings.create().withNewLine(true));

CompletionStage&lt;List&lt;RotationMessage&gt;&gt; resF =
    Source.from(messagesFromKafka)
        .map(
            kafkaMessage -&gt; {
              Book book = kafkaMessage.book;
              // Transform message so that we can write to hdfs\
              return HdfsWriteMessage.create(
                  ByteString.fromString(book.title), kafkaMessage.offset);
            })
        .via(flow)
        .map(
            message -&gt; {
              if (message instanceof WrittenMessage) {
                kafkaCommitter.commit(((WrittenMessage&lt;KafkaOffset&gt;) message).passThrough());
                return message;
              } else {
                return message;
              }
            })
        .collectType(RotationMessage.class) // Collect only rotation messages
        .runWith(Sink.seq(), materializer);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L230-L271" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h2><a href="#configuration" name="configuration" class="anchor"><span class="anchor-link"></span></a>Configuration</h2>
<p>We can configure the sink by <code>HdfsWritingSettings</code>. </p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val settings =
  HdfsWritingSettings(
    overwrite = true,
    newLine = false,
    lineSeparator = System.getProperty(&quot;line.separator&quot;),
    pathGenerator = pathGenerator
  )</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L499-L505" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">HdfsWritingSettings.create()
    .withOverwrite(true)
    .withNewLine(false)
    .withLineSeparator(System.getProperty(&quot;line.separator&quot;))
    .withPathGenerator(pathGenerator);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L528-L532" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h3><a href="#file-path-generator" name="file-path-generator" class="anchor"><span class="anchor-link"></span></a>File path generator</h3>
<p><a href="https://doc.akka.io/api/alpakka/1.0-M1/akka/stream/alpakka/hdfs/FilePathGenerator$.html">FilePathGenerator</a> provides a functionality to generate rotation path in HDFS. </p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val pathGenerator =
  FilePathGenerator(
    (rotationCount: Long, timestamp: Long) =&gt; s&quot;/tmp/alpakka/$rotationCount-$timestamp&quot;
  )</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsWriterSpec.scala#L493-L496" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">BiFunction&lt;Long, Long, String&gt; func =
    (rotationCount, timestamp) -&gt; &quot;/tmp/alpakka/&quot; + rotationCount + &quot;-&quot; + timestamp;
FilePathGenerator pathGenerator = FilePathGenerator.create(func);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsWriterTest.java#L523-L525" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h3><a href="#rotation-strategy" name="rotation-strategy" class="anchor"><span class="anchor-link"></span></a>Rotation Strategy</h3>
<p><a href="https://doc.akka.io/api/alpakka/1.0-M1/akka/stream/alpakka/hdfs/RotationStrategy$.html">RotationStrategy</a> provides a functionality to decide when to rotate files.</p>
<h3><a href="#sync-strategy" name="sync-strategy" class="anchor"><span class="anchor-link"></span></a>Sync Strategy</h3>
<p><a href="https://doc.akka.io/api/alpakka/1.0-M1/akka/stream/alpakka/hdfs/SyncStrategy$.html">SyncStrategy</a> provides a functionality to decide when to synchronize the output.</p>
<h2><a href="#reading" name="reading" class="anchor"><span class="anchor-link"></span></a>Reading</h2>
<p>Use <code>HdfsSource</code> to read from HDFS. <span class="group-scala"><a href="https://doc.akka.io/api/alpakka/1.0-M1/akka/stream/alpakka/hdfs/scaladsl/HdfsSource$.html">HdfsSource</a>.</span> <span class="group-java"><a href="https://doc.akka.io/api/alpakka/1.0-M1/akka/stream/alpakka/hdfs/javadsl/HdfsSource$.html">HdfsSource</a>.</span></p>
<h3><a href="#data-reader" name="data-reader" class="anchor"><span class="anchor-link"></span></a>Data Reader</h3>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val source = HdfsSource.data(fs, path)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsReaderSpec.scala#L61" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Source&lt;ByteString, CompletionStage&lt;IOResult&gt;&gt; source = HdfsSource.data(fs, path);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsReaderTest.java#L64" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h3><a href="#compressed-data-reader" name="compressed-data-reader" class="anchor"><span class="anchor-link"></span></a>Compressed Data Reader</h3>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val source = HdfsSource.compressed(fs, path, codec)</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsReaderSpec.scala#L99" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Source&lt;ByteString, CompletionStage&lt;IOResult&gt;&gt; source = HdfsSource.compressed(fs, path, codec);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsReaderTest.java#L103" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h3><a href="#sequence-reader" name="sequence-reader" class="anchor"><span class="anchor-link"></span></a>Sequence Reader</h3>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><code class="language-scala">val source = HdfsSource.sequence(fs, path, classOf[Text], classOf[Text])</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/scala/akka/stream/alpakka/hdfs/HdfsReaderSpec.scala#L135" class="snippet-full-source github">Full source at GitHub</a></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><code class="language-java">Source&lt;Pair&lt;Text, Text&gt;, NotUsed&gt; source =
    HdfsSource.sequence(fs, path, Text.class, Text.class);</code></pre><a href="https://github.com/akka/alpakka/tree/v1.0-M1/hdfs/src/test/java/akka/stream/alpakka/hdfs/HdfsReaderTest.java#L144-L145" class="snippet-full-source github">Full source at GitHub</a></dd>
</dl>
<h2><a href="#running-the-example-code" name="running-the-example-code" class="anchor"><span class="anchor-link"></span></a>Running the example code</h2>
<p>The code in this guide is part of runnable tests of this project. You are welcome to edit the code and run it in sbt.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre><code>sbt
&gt; hdfs/testOnly *.HdfsWriterSpec
&gt; hdfs/testOnly *.HdfsReaderSpec
</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre><code>sbt
&gt; hdfs/testOnly *.HdfsWriterTest
&gt; hdfs/testOnly *.HdfsReaderTest
</code></pre></dd>
</dl>
</div>
</article>

<div class="row">
<div class="small-12 large-9 column">
<section class="nav-prev-next row">
<div class="nav-prev small-6 column">
<a href="external/grpc.html"><i class="icon-prev"></i> <span class="link-prev">gRPC</span></a>
</div>
<div class="nav-next small-6 column clearfix">
<a class="float-right" href="hbase.html">HBase <i class="icon-next"></i></a>
</div>
</section>
</div>
</div>

<div class="source-github row">
Found an error in this documentation? The source code for this page can be found <a href="https://github.com/akka/alpakka/tree/v1.0-M1/docs/src/main/paradox/hdfs.md">here</a>.
Please feel free to edit and contribute a pull request.
</div>


<footer class="page-footer row clearfix">
<img class="akka-icon float-left show-for-medium" src="images/akka-icon.svg">
<section class="copyright">
<div>Alpakka is Open Source and available under the Apache 2 License.</div>
<p class="legal">
&copy; 2011-2019 <a href="https://www.lightbend.com" target="_blank">Lightbend, Inc.</a> | 
<a href="https://www.lightbend.com/legal/licenses" target="_blank">Licenses</a> | 
<a href="https://www.lightbend.com/legal/terms" target="_blank">Terms</a> | 
<a href="https://www.lightbend.com/legal/privacy" target="_blank">Privacy Policy</a> | 
<a href="https://akka.io/cookie/" target="_blank">Cookie Listing</a> | 
<a class="optanon-toggle-display">Cookie Settings</a>
</p>
</section>
</footer>

</section>
</main>

<script type="text/javascript" src="js/scrollsneak.js"></script>
<script type="text/javascript">jQuery(document).foundation();</script>
<script type="text/javascript" src="js/groups.js"></script>
<script type="text/javascript" src="js/page.js"></script>
<script type="text/javascript" src="js/magellan.js"></script>

<style type="text/css">@import "lib/prettify/prettify.css";</style>
<script type="text/javascript" src="lib/prettify/prettify.js"></script>
<script type="text/javascript" src="lib/prettify/lang-scala.js"></script>
<script type="text/javascript">jQuery(function(){window.prettyPrint && prettyPrint()});</script>

<script type="text/javascript" src="assets/js/warnOldVersion.js"></script>
<script type="text/javascript">jQuery(function(jq){initOldVersionWarnings(jq, '1.0-M1', 'https://doc.akka.io/docs/alpakka/current/')});</script>


</body>
</html>
