<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
<title>Akka Split Brain Resolver &bull; Akka Enhancements</title>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="description" content="Akka Enhancements is a suite of useful components that complement Akka."/>
<link rel="canonical" href="https://doc.akka.io/docs/akka-enhancements/current/split-brain-resolver.html"/>
<script type="text/javascript" src="lib/jquery/jquery.min.js"></script>
<script type="text/javascript" src="lib/foundation/dist/js/foundation.min.js"></script>
<link rel="stylesheet" type="text/css" href="lib/normalize.css/normalize.css"/>
<link rel="stylesheet" type="text/css" href="lib/foundation/dist/css/foundation.min.css"/>
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/docsearch.js/2/docsearch.min.css" />
<link rel="stylesheet" type="text/css" href="css/icons.css"/>
<link rel="stylesheet" type="text/css" href="css/page-7.css"/>
<link rel="stylesheet" type="text/css" href="css/banner-1.css"/>
<link rel="shortcut icon" href="images/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png"/>
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png"/>
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png"/>
<link rel="manifest" href="images/manifest.json"/>
<meta name="msapplication-TileImage" content="images/mstile-150x150.png"/>
<meta name="msapplication-TileColor" content="#15a9ce"/>
<meta name="theme-color" content="#15a9ce"/>
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) start -->
<script src="https://optanon.blob.core.windows.net/consent/159bb13d-6748-4399-806e-ac28db879785.js" type="text/javascript" charset="UTF-8"></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) end -->
<!--Google Analytics-->
<script type="text/plain" class="optanon-category-2">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', '']);
_gaq.push(['_setDomainName', '']);
_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})()
</script>
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-2">
(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KBJGH35');
</script>
<!--Marketo-->
<script type="text/plain" class="optanon-category-3">
(function() {
var didInit = false;
function initMunchkin() {
if(didInit === false) {
didInit = true;
Munchkin.init('558-NCX-702', { 'asyncOnly': true, 'disableClickDelay': true });
}
}
var s = document.createElement('script');
s.type = 'text/javascript';
s.async = true;
s.src = '//munchkin.marketo.net/munchkin.js';
s.onreadystatechange = function() {
if (this.readyState == 'complete' || this.readyState == 'loaded') {
initMunchkin();
}
};
s.onload = initMunchkin;
document.getElementsByTagName('head')[0].appendChild(s);
})();
</script>
</head>

<body id="underlay" data-toggler="nav-open">
<div id="lightbend-banner" class="lightbend-banner akka full-width" data-category="OSS Lightbend Banner Impression" data-label="Akka Banner Impression">
<div class="wrapper">
<div class="brand">
<a class="oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Lightbend Logo - Akka Banner" href="https://www.lightbend.com?r=oss-banner-akka" target="_blank">
<img class="lightbend-logo" src="images/banner-logos/lightbend-reverse.svg" alt="Lightbend" title="Lightbend">
</a>
</div>
<div class="nav">
<a class="banner-btn oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Enhance your Akka systems with Akka Platform [Button] - Akka Banner" href="https://www.lightbend.com/akka-core-of-lightbend?r=oss-banner-akka" target="_blank">
<span>Enhance your Akka systems with</span>
<img class="akka-platform-reverse-logo" src="images/banner-logos/akka-platform-reverse.svg" alt="Akka Platform" title="Akka Platform">
</a>
<div class="drop-down">
<svg class="svg-chevon-circle-down" version="1.1" id="Chevron_circled_down" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 20 20" enable-background="new 0 0 20 20" xml:space="preserve">
<path fill="#ffffff" d="M12.505,8.698L10,11L7.494,8.698c-0.198-0.196-0.518-0.196-0.718,0c-0.197,0.196-0.197,0.515,0,0.71l2.864,2.807
c0.199,0.196,0.52,0.196,0.717,0l2.864-2.807c0.199-0.195,0.198-0.514,0-0.71C13.024,8.502,12.704,8.502,12.505,8.698z M10,0.4
c-5.302,0-9.6,4.298-9.6,9.6c0,5.303,4.298,9.6,9.6,9.6s9.6-4.297,9.6-9.6C19.6,4.698,15.302,0.4,10,0.4z M10,18.354
c-4.615,0-8.354-3.74-8.354-8.354c0-4.614,3.739-8.354,8.354-8.354c4.613,0,8.354,3.74,8.354,8.354
C18.354,14.614,14.613,18.354,10,18.354z" />
</svg>
<div class="drop-down-content">
<div class="lightbend-family">
<a href="https://cloudflow.io" class="cloudflow oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Cloudflow - Logo Tag Line - Akka Banner">
<img class="cloudflow-full-color-logo" src="images/banner-logos/cloudflow-full-color.svg" alt="Cloudflow by Lightbend" title="Cloudflow by Lightbend">
</a>
<a href="https://cloudstate.io" class="cloudstate oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Cloudstate - Logo Tag Line - Akka Banner">
<img class="cloudstate-full-color-logo" src="images/banner-logos/cloudstate-full-color.svg" alt="Cloudstate by Lightbend" title="Cloudstate by Lightbend">
</a>
<a href="https://www.lagomframework.com" class="lagom oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Lagom - Logo Tag Line - Akka Banner">
<img class="lagom-full-color-logo" src="images/banner-logos/lagom-full-color.svg" alt="Lagom Framework by Lightbend" title="Lagom Framework by Lightbend">
</a>
<a href="https://www.playframework.com" class="play oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Play - Logo Tag Line - Akka Banner">
<img class="play-full-color-logo" src="images/banner-logos/play-full-color.svg" alt="Play Framework by Lightbend" title="Play Framework by Lightbend">
</a>
<a href="https://www.scala-lang.org" class="scala oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Scala - Logo Tag Line - Akka Banner">
<img class="scala-full-color-logo" src="images/banner-logos/scala-full-color.svg" alt="Scala by Lightbend" title="Scala by Lightbend">
</a>
<div class="akka current">
<img class="akka-full-color-logo" src="images/banner-logos/akka-full-color.svg" alt="Akka by Lightbend" title="Akka by Lightbend">
<span>From the creators of <strong>Akka</strong>, get technology enhancements, monitoring, and expert support with Akka Platform from Lightbend.</span>
<a class="oss-track-link-label" data-category="OSS Lightbend Banner Clicks" data-label="Learn More [Button] - Akka Banner" href="https://www.lightbend.com/akka-core-of-lightbend?r=oss-banner-akka" target="_blank">Learn More</a>
</div>
</div>
<div class="title">The Lightbend Family</div>
</div>      
</div>
</div>
</div>
</div>
<header class="site-header hide-for-large">
<div class="sticky-header clearfix">
<a href="https://akka.io"><img class="logo" src="images/akka-logo-reverse.svg"/></a>

<button class="menu-icon float-right" type="button" data-toggle="underlay overlay"></button>
</div>
<div id="overlay" class="overlay-nav" data-toggler="nav-open">
<header class="nav-header">
<div class="nav-header-title">
<h1><a href="index.html">Akka Enhancements</a></h1>
</div>
<div class="nav-header-version">
Version 1.1.13
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Languages"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="nav-toc">
<ul>
  <li><a href="akka-resilience-enhancements.html" class="page">Akka Resilience Enhancements</a>
  <ul>
    <li><a href="split-brain-resolver.html#akka-split-brain-resolver" class="active page">Akka Split Brain Resolver</a>
    <ul>
      <li><a href="split-brain-resolver.html#the-problem" class="header">The Problem</a></li>
      <li><a href="split-brain-resolver.html#using-the-split-brain-resolver" class="header">Using the Split Brain Resolver</a></li>
      <li><a href="split-brain-resolver.html#strategies" class="header">Strategies</a></li>
      <li><a href="split-brain-resolver.html#indirectly-connected-nodes" class="header">Indirectly connected nodes</a></li>
      <li><a href="split-brain-resolver.html#down-all-when-unstable" class="header">Down all when unstable</a></li>
      <li><a href="split-brain-resolver.html#multiple-data-centers" class="header">Multiple data centers</a></li>
      <li><a href="split-brain-resolver.html#cluster-singleton-and-cluster-sharding" class="header">Cluster Singleton and Cluster Sharding</a></li>
    </ul></li>
    <li><a href="kubernetes-lease.html" class="page">Kubernetes Lease</a></li>
    <li><a href="starvation-detector.html" class="page">Akka Thread Starvation Detector</a></li>
    <li><a href="config-checker.html" class="page">Akka Config Checker</a></li>
    <li><a href="diagnostics-recorder.html" class="page">Akka Diagnostics Recorder</a></li>
    <li><a href="fast-failover.html" class="page">Fast Failover</a></li>
    <li><a href="akka-resilience-enhancements-release-notes.html" class="page">Akka Resilience Enhancements Release Notes</a></li>
  </ul></li>
  <li><a href="akka-persistence-enhancements.html" class="page">Akka Persistence Enhancements</a></li>
</ul>
</nav>
</div>
</header>
<div class="site-content-wrapper">
<aside class="sticky-sidebar show-for-large">
<header class="nav-header sticky-sidebar-header">
<div class="nav-header-title">
<h1><a href="index.html">Akka Enhancements</a></h1>
</div>
<div class="nav-header-version">
Version 1.1.13
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Languages"><option class="group" value="group-java">Java</option><option class="group" value="group-scala">Scala</option></select>
</div>
</header>
<nav class="site-nav sticky-sidebar-contents">
<div class="nav-toc">
<ul>
  <li><a href="akka-resilience-enhancements.html" class="page">Akka Resilience Enhancements</a>
  <ul>
    <li><a href="split-brain-resolver.html#akka-split-brain-resolver" class="active page">Akka Split Brain Resolver</a>
    <ul>
      <li><a href="split-brain-resolver.html#the-problem" class="header">The Problem</a></li>
      <li><a href="split-brain-resolver.html#using-the-split-brain-resolver" class="header">Using the Split Brain Resolver</a></li>
      <li><a href="split-brain-resolver.html#strategies" class="header">Strategies</a></li>
      <li><a href="split-brain-resolver.html#indirectly-connected-nodes" class="header">Indirectly connected nodes</a></li>
      <li><a href="split-brain-resolver.html#down-all-when-unstable" class="header">Down all when unstable</a></li>
      <li><a href="split-brain-resolver.html#multiple-data-centers" class="header">Multiple data centers</a></li>
      <li><a href="split-brain-resolver.html#cluster-singleton-and-cluster-sharding" class="header">Cluster Singleton and Cluster Sharding</a></li>
    </ul></li>
    <li><a href="kubernetes-lease.html" class="page">Kubernetes Lease</a></li>
    <li><a href="starvation-detector.html" class="page">Akka Thread Starvation Detector</a></li>
    <li><a href="config-checker.html" class="page">Akka Config Checker</a></li>
    <li><a href="diagnostics-recorder.html" class="page">Akka Diagnostics Recorder</a></li>
    <li><a href="fast-failover.html" class="page">Fast Failover</a></li>
    <li><a href="akka-resilience-enhancements-release-notes.html" class="page">Akka Resilience Enhancements Release Notes</a></li>
  </ul></li>
  <li><a href="akka-persistence-enhancements.html" class="page">Akka Persistence Enhancements</a></li>
</ul>
</div>
</nav>
<footer class="nav-footer sticky-sidebar-footer">
<a href="https://akka.io"><img class="logo" src="images/akka-logo-reverse.svg"/></a>

</footer>
</aside>
<main class="fixed-shift-for-large expanded row">
<section class="site-content small-12 column">
<article class="page-content row">
<div class="small-12 column" id="docs">
<a id="split-brain-resolver-scala"></a>
<h1><a href="#akka-split-brain-resolver" name="akka-split-brain-resolver" class="anchor"><span class="anchor-link"></span></a>Akka Split Brain Resolver</h1>
<p>When operating an Akka cluster you must consider how to handle <a href="http://en.wikipedia.org/wiki/Network_partition">network partitions</a> (a.k.a. split brain scenarios) and machine crashes (including JVM and hardware failures). This is crucial for correct behavior if you use Cluster Singleton or Cluster Sharding, especially together with Akka Persistence.</p><div class="callout note "><div class="callout-title">Note</div>
<p>This feature is included in a <a href="https://www.lightbend.com/lightbend-platform-subscription">subscription to Lightbend Platform</a>, which includes other technology enhancements, monitoring and telemetry, and one-to-one support from the expert engineers behind Akka.</p></div>
<h2><a href="#the-problem" name="the-problem" class="anchor"><span class="anchor-link"></span></a>The Problem</h2>
<p>A fundamental problem in distributed systems is that network partitions (split brain scenarios) and machine crashes are indistinguishable for the observer, i.e. a node can observe that there is a problem with another node, but it cannot tell if it has crashed and will never be available again or if there is a network issue that might or might not heal again after a while. Temporary and permanent failures are indistinguishable because decisions must be made in finite time, and there always exists a temporary failure that lasts longer than the time limit for the decision.</p>
<p>A third type of problem is if a process is unresponsive, e.g. because of overload, CPU starvation or long garbage collection pauses. This is also indistinguishable from network partitions and crashes. The only signal we have for decision is &ldquo;no reply in given time for heartbeats&rdquo; and this means that phenomena causing delays or lost heartbeats are indistinguishable from each other and must be handled in the same way.</p>
<p>When there is a crash, we would like to remove the affected node immediately from the cluster membership. When there is a network partition or unresponsive process we would like to wait for a while in the hope that it is a transient problem that will heal again, but at some point, we must give up and continue with the nodes on one side of the partition and shut down nodes on the other side. Also, certain features are not fully available during partitions so it might not matter that the partition is transient or not if it just takes too long. Those two goals are in conflict with each other and there is a trade-off between how quickly we can remove a crashed node and premature action on transient network partitions.</p>
<p>This is a difficult problem to solve given that the nodes on the different sides of the network partition cannot communicate with each other. We must ensure that both sides can make this decision by themselves and that they take the same decision about which part will keep running and which part will shut itself down.</p>
<p>Another type of problem that makes it difficult to see the &ldquo;right&rdquo; picture is when some nodes are not fully connected and cannot communicate directly to each other but information can be disseminated between them via other nodes.</p>
<p>The Akka cluster has a failure detector that will notice network partitions and machine crashes (but it cannot distinguish the two). It uses periodic heartbeat messages to check if other nodes are available and healthy. These observations by the failure detector are referred to as a node being <em>unreachable</em> and it may become <em>reachable</em> again if the failure detector observes that it can communicate with it again. </p>
<p>The failure detector in itself is not enough for making the right decision in all situations. The naive approach is to remove an unreachable node from the cluster membership after a timeout. This works great for crashes and short transient network partitions, but not for long network partitions. Both sides of the network partition will see the other side as unreachable and after a while remove it from its cluster membership. Since this happens on both sides the result is that two separate disconnected clusters have been created. This approach is provided by the opt-in (off by default) auto-down feature in the OSS version of Akka Cluster.</p>
<p>If you use the timeout based auto-down feature in combination with Cluster Singleton or Cluster Sharding that would mean that two singleton instances or two sharded entities with the same identifier would be running. One would be running: one in each cluster. For example when used together with Akka Persistence that could result in that two instances of a persistent actor with the same <code>persistenceId</code> are running and writing concurrently to the same stream of persistent events, which will have fatal consequences when replaying these events.</p>
<p>The default setting in Akka Cluster is to not remove unreachable nodes automatically and the <a href="https://doc.akka.io/docs/akka/2.5/cluster-usage.html#auto-downing-do-not-use-">recommendation</a> is that the decision of what to do should be taken by a human operator or an external monitoring system. This is a valid solution, but not very convenient if you do not have this staff or external system for other reasons.</p>
<p>If the unreachable nodes are not downed at all they will still be part of the cluster membership. Meaning that Cluster Singleton and Cluster Sharding will not failover to another node. While there are unreachable nodes new nodes that are joining the cluster will not be promoted to full worthy members (with status Up). Similarly, leaving members will not be removed until all unreachable nodes have been resolved. In other words, keeping unreachable members for an unbounded time is undesirable.</p>
<p>With that introduction of the problem domain, it is time to look at the provided strategies for handling network partition, unresponsive nodes and crashed nodes.</p>
<h2><a href="#using-the-split-brain-resolver" name="using-the-split-brain-resolver" class="anchor"><span class="anchor-link"></span></a>Using the Split Brain Resolver</h2>
<p>To use the Split Brain Resolver feature a dependency on the <em>akka-split-brain-resolver</em> artifact must be added.</p>
<dl>
  <dt>sbt</dt>
  <dd>
  <pre><code>// Add Lightbend Platform to your build as documented at https://developer.lightbend.com/docs/lightbend-platform/introduction/getting-started/subscription-and-credentials.html
&quot;com.lightbend.akka&quot; %% &quot;akka-split-brain-resolver&quot; % &quot;1.1.13&quot;
</code></pre></dd>
  <dt>Gradle</dt>
  <dd>
  <pre><code>// Add Lightbend Platform to your build as documented at https://developer.lightbend.com/docs/lightbend-platform/introduction/getting-started/subscription-and-credentials.html
dependencies {
  compile group: &#39;com.lightbend.akka&#39;, name: &#39;akka-split-brain-resolver_2.12&#39;, version: &#39;1.1.13&#39;
}
</code></pre></dd>
  <dt>Maven</dt>
  <dd>
  <pre><code>&lt;!-- Add Lightbend Platform to your build as documented at https://developer.lightbend.com/docs/lightbend-platform/introduction/getting-started/subscription-and-credentials.html --&gt;
&lt;dependency&gt;
  &lt;groupId&gt;com.lightbend.akka&lt;/groupId&gt;
  &lt;artifactId&gt;akka-split-brain-resolver_2.12&lt;/artifactId&gt;
  &lt;version&gt;1.1.13&lt;/version&gt;
&lt;/dependency&gt;
</code></pre></dd>
</dl><p>Before you can access this library, you&rsquo;ll need to <a href="https://developer.lightbend.com/docs/lightbend-platform/introduction/getting-started/subscription-and-credentials.html">configure the Lightbend repository and credentials in your build</a>.</p>
<p>Then you need to enable the Split Brain Resolver by configuring it with <em>akka.cluster.downing-provider-class</em> in the configuration of the <em>ActorSystem</em> (*application.conf*):</p>
<pre><code>akka.cluster.downing-provider-class = &quot;com.lightbend.akka.sbr.SplitBrainResolverProvider&quot;
</code></pre>
<p>You must also carefully select a downing strategy and configure that before the Split Brain Resolver is ready to be used. See below for details on choosing a strategy.</p>
<h2><a href="#strategies" name="strategies" class="anchor"><span class="anchor-link"></span></a>Strategies</h2>
<p>There is not a &ldquo;one size fits all&rdquo; solution to this problem. You have to pick a strategy that fits the characteristics of your system. Every strategy has a failure scenario where it makes a &ldquo;wrong&rdquo; decision. This section describes the different strategies and guidelines of when to use what.</p>
<p>When there is uncertainty it selects to down more nodes than necessary, or even downing of all nodes. Therefore Split Brain Resolver should always be combined with a mechanism to automatically start up nodes that have been shutdown, and join them to the existing cluster or form a new cluster again.</p>
<p>You enable a strategy with the configuration property <code>akka.cluster.split-brain-resolver.active-strategy</code>.</p><div class="callout note "><div class="callout-title">Note</div>
<p>You must also remove auto-down configuration, remove this property (or set to off): <code>akka.cluster.auto-down-unreachable-after</code></p></div>
<h3><a href="#stable-after" name="stable-after" class="anchor"><span class="anchor-link"></span></a>Stable after</h3>
<p>All strategies are inactive until the cluster membership and the information about unreachable nodes have been stable for a certain time period. Continuously adding more nodes while there is a network partition does not influence this timeout, since the status of those nodes will not be changed to Up while there are unreachable nodes. Joining nodes are not counted in the logic of the strategies. </p>
<pre class="prettyprint"><code class="language-conf"><br/># To enable the split brain resolver you first need to enable the provider in your application.conf:
# akka.cluster.downing-provider-class = &quot;com.lightbend.akka.sbr.SplitBrainResolverProvider&quot;

akka.cluster.split-brain-resolver {
  # Select one of the available strategies (see descriptions below):
  # static-quorum, keep-majority, keep-oldest, keep-referee, down-all
  # if left &quot;off&quot; when the downing provider is enabled cluster startup will fail.
  active-strategy = off

  # Time margin after which shards or singletons that belonged to a downed/removed
  # partition are created in surviving partition. The purpose of this margin is that
  # in case of a network partition the persistent actors in the non-surviving partitions
  # must be stopped before corresponding persistent actors are started somewhere else.
  # This is useful if you implement downing strategies that handle network partitions,
  # e.g. by keeping the larger side of the partition and shutting down the smaller side.
  # Decision is taken by the strategy when there has been no membership or
  # reachability changes for this duration, i.e. the cluster state is stable.
  stable-after = 20s

  # When reachability observations by the failure detector are changed the SBR decisions
  # are deferred until there are no changes within the &#39;stable-after&#39; duration.
  # If this continues for too long it might be an indication of an unstable system/network
  # and it could result in delayed or conflicting decisions on separate sides of a network
  # partition.
  # As a precaution for that scenario all nodes are downed if no decision is made within
  # `stable-after + down-all-when-unstable` from the first unreachability event.
  # The measurement is reset if all unreachable have been healed, downed or removed, or
  # if there are no changes within `stable-after * 2`.
  # The value can be on, off, or a duration.
  # By default it is &#39;on&#39; and then it is derived to be 3/4 of stable-after.
  down-all-when-unstable = on

}</code></pre>
<p>Set <code>akka.cluster.split-brain-resolver.stable-after</code> to a shorter duration to have quicker removal of crashed nodes, at the price of risking too early action on transient network partitions that otherwise would have healed. Do not set this to a shorter duration than the membership dissemination time in the cluster, which depends on the cluster size. Recommended minimum duration for different cluster sizes:</p>
<table>
  <thead>
    <tr>
      <th>cluster size </th>
      <th>stable-after</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5 </td>
      <td>7 s </td>
    </tr>
    <tr>
      <td>10 </td>
      <td>10 s</td>
    </tr>
    <tr>
      <td>20 </td>
      <td>13 s</td>
    </tr>
    <tr>
      <td>50 </td>
      <td>17 s</td>
    </tr>
    <tr>
      <td>100 </td>
      <td>20 s</td>
    </tr>
    <tr>
      <td>1000 </td>
      <td>30 s</td>
    </tr>
  </tbody>
</table>
<p>The different strategies may have additional settings that are described below.</p><div class="callout note "><div class="callout-title">Note</div>
<p>It is important that you use the same configuration on all nodes.</p></div>
<p>The side of the split that decides to shut itself down will use the cluster <em>down</em> command to initiate the removal of a cluster member. When that has been spread among the reachable nodes it will be removed from the cluster membership.</p>
<p>It&rsquo;s good to terminate the <code>ActorSystem</code> and exit the JVM when the node is removed from the cluster.</p>
<p>That is handled by <a href="https://doc.akka.io/docs/akka/2.5/actors.html#coordinated-shutdown">Coordinated Shutdown</a> but to exit the JVM it&rsquo;s recommended that you enable:</p>
<pre><code>akka.coordinated-shutdown.exit-jvm = on
</code></pre><div class="callout note "><div class="callout-title">Note</div>
<p>Some legacy containers may block calls to System.exit(..) and you may have to find an alternate way to shut the app down. For example, when running Akka on top of a Spring / Tomcat setup, you could replace the call to <code>System.exit(..)</code> with a call to Spring&rsquo;s ApplicationContext .close() method (or with a HTTP call to Tomcat Manager&rsquo;s API to un-deploy the app).</p></div>
<h3><a href="#static-quorum" name="static-quorum" class="anchor"><span class="anchor-link"></span></a>Static Quorum</h3>
<p>The strategy named <code>static-quorum</code> will down the unreachable nodes if the number of remaining nodes are greater than or equal to a configured <code>quorum-size</code>. Otherwise, it will down the reachable nodes, i.e. it will shut down that side of the partition. In other words, the <code>quorum-size</code> defines the minimum number of nodes that the cluster must have to be operational.</p>
<p>This strategy is a good choice when you have a fixed number of nodes in the cluster, or when you can define a fixed number of nodes with a certain role.</p>
<p>For example, in a 9 node cluster you will configure the <code>quorum-size</code> to 5. If there is a network split of 4 and 5 nodes the side with 5 nodes will survive and the other 4 nodes will be downed. After that, in the 5 node cluster, no more failures can be handled, because the remaining cluster size would be less than 5. In the case of another failure in that 5 node cluster all nodes will be downed.</p>
<p>Therefore it is important that you join new nodes when old nodes have been removed.</p>
<p>Another consequence of this is that if there are unreachable nodes when starting up the cluster, before reaching this limit, the cluster may shut itself down immediately. This is not an issue if you start all nodes at approximately the same time or use the <code>akka.cluster.min-nr-of-members</code> to define required number of members before the leader changes member status of &lsquo;Joining&rsquo; members to &lsquo;Up&rsquo; You can tune the timeout after which downing decisions are made using the <code>stable-after</code> setting.</p>
<p>You should not add more members to the cluster than <strong>quorum-size * 2 - 1</strong>. A warning is logged if this recommendation is violated. If the exceeded cluster size remains when a SBR decision is needed it will down all nodes because otherwise there is a risk that both sides may down each other and thereby form two separate clusters.</p>
<p>For rolling updates it&rsquo;s best to leave the cluster gracefully via <a href="https://doc.akka.io/docs/akka/2.5/actors.html#coordinated-shutdown">Coordinated Shutdown</a> (SIGTERM). For successful leaving SBR will not be used (no downing) but if there is an unreachability problem at the same time as the rolling update is in progress there could be an SBR decision. To avoid that the total number of members limit is not exceeded during the rolling update it&rsquo;s recommended to leave and fully remove one node before adding a new one, when using <code>static-quorum</code>.</p>
<p>If the cluster is split into 3 (or more) parts each part that is smaller than then configured <code>quorum-size</code> will down itself and possibly shutdown the whole cluster.</p>
<p>If more nodes than the configured <code>quorum-size</code> crash at the same time the other running nodes will down themselves because they think that they are not in the majority, and thereby the whole cluster is terminated.</p>
<p>The decision can be based on nodes with a configured <code>role</code> instead of all nodes in the cluster. This can be useful when some types of nodes are more valuable than others. You might, for example, have some nodes responsible for persistent data and some nodes with stateless worker services. Then it probably more important to keep as many persistent data nodes as possible even though it means shutting down more worker nodes.</p>
<p>There is another use of the <code>role</code> as well. By defining a <code>role</code> for a few (e.g. 7) stable nodes in the cluster and using that in the configuration of <code>static-quorum</code> you will be able to dynamically add and remove other nodes without this role and still have good decisions of what nodes to keep running and what nodes to shut down in the case of network partitions. The advantage of this approach compared to <code>keep-majority</code> (described below) is that you <em>do not</em> risk splitting the cluster into two separate clusters, i.e. <em>a split brain</em>*. You must still obey the rule of not starting too many nodes with this <code>role</code> as described above. It also suffers the risk of shutting down all nodes if there is a failure when there are not enough nodes with this <code>role</code> remaining in the cluster, as described above.</p>
<p>Configuration:</p>
<pre><code>akka.cluster.split-brain-resolver.active-strategy=static-quorum
</code></pre>
<pre class="prettyprint"><code class="language-conf">akka.cluster.split-brain-resolver.static-quorum {
  # minimum number of nodes that the cluster must have
  quorum-size = undefined

  # if the &#39;role&#39; is defined the decision is based only on members with that &#39;role&#39;
  role = &quot;&quot;
}</code></pre>
<h3><a href="#keep-majority" name="keep-majority" class="anchor"><span class="anchor-link"></span></a>Keep Majority</h3>
<p>The strategy named <code>keep-majority</code> will down the unreachable nodes if the current node is in the majority part based on the last known membership information. Otherwise down the reachable nodes, i.e. the own part. If the parts are of equal size the part containing the node with the lowest address is kept.</p>
<p>This strategy is a good choice when the number of nodes in the cluster change dynamically and you can therefore not use <code>static-quorum</code>.</p>
<p>This strategy also handles the edge case that may occur when there are membership changes at the same time as the network partition occurs. For example, the status of two members are changed to <code>Up</code> on one side but that information is not disseminated to the other side before the connection is broken. Then one side sees two more nodes and both sides might consider themselves having a majority. It will detect this situation and make the safe decision to down all nodes on the side that could be in minority if the joining nodes were changed to <code>Up</code> on the other side. Note that this has the drawback that if the joining nodes were not changed to <code>Up</code> and becoming a majority on the other side then each part will shut down itself, terminating the whole cluster.</p>
<p>Note that if there are more than two partitions and none is in majority each part will shut down itself, terminating the whole cluster.</p>
<p>If more than half of the nodes crash at the same time the other running nodes will down themselves because they think that they are not in majority, and thereby the whole cluster is terminated. </p>
<p>The decision can be based on nodes with a configured <code>role</code> instead of all nodes in the cluster. This can be useful when some types of nodes are more valuable than others. You might for example have some nodes responsible for persistent data and some nodes with stateless worker services. Then it probably more important to keep as many persistent data nodes as possible even though it means shutting down more worker nodes.</p>
<p>Configuration:</p>
<pre><code>akka.cluster.split-brain-resolver.active-strategy=keep-majority
</code></pre>
<pre class="prettyprint"><code class="language-conf">akka.cluster.split-brain-resolver.keep-majority {
  # if the &#39;role&#39; is defined the decision is based only on members with that &#39;role&#39;
  role = &quot;&quot;
}</code></pre>
<h3><a href="#keep-oldest" name="keep-oldest" class="anchor"><span class="anchor-link"></span></a>Keep Oldest</h3>
<p>The strategy named <code>keep-oldest</code> will down the part that does not contain the oldest member. The oldest member is interesting because the active Cluster Singleton instance is running on the oldest member.</p>
<p>There is one exception to this rule if <code>down-if-alone</code> is configured to <code>on</code>. Then, if the oldest node has partitioned from all other nodes the oldest will down itself and keep all other nodes running. The strategy will not down the single oldest node when it is the only remaining node in the cluster.</p>
<p>Note that if the oldest node crashes the others will remove it from the cluster when <code>down-if-alone</code> is <code>on</code>, otherwise they will down themselves if the oldest node crashes, i.e. shut down the whole cluster together with the oldest node.</p>
<p>This strategy is good to use if you use Cluster Singleton and do not want to shut down the node where the singleton instance runs. If the oldest node crashes a new singleton instance will be started on the next oldest node. The drawback is that the strategy may keep only a few nodes in a large cluster. For example, if one part with the oldest consists of 2 nodes and the other part consists of 98 nodes then it will keep 2 nodes and shut down 98 nodes.</p>
<p>This strategy also handles the edge case that may occur when there are membership changes at the same time as the network partition occurs. For example, the status of the oldest member is changed to <code>Exiting</code> on one side but that information is not disseminated to the other side before the connection is broken. It will detect this situation and make the safe decision to down all nodes on the side that sees the oldest as <code>Leaving</code>. Note that this has the drawback that if the oldest was <code>Leaving</code> and not changed to <code>Exiting</code> then each part will shut down itself, terminating the whole cluster.</p>
<p>The decision can be based on nodes with a configured <code>role</code> instead of all nodes in the cluster, i.e. using the oldest member (singleton) within the nodes with that role.</p>
<p>Configuration:</p>
<pre><code>akka.cluster.split-brain-resolver.active-strategy=keep-oldest
</code></pre>
<pre class="prettyprint"><code class="language-conf">akka.cluster.split-brain-resolver.keep-oldest {
  # Enable downing of the oldest node when it is partitioned from all other nodes
  down-if-alone = on

  # if the &#39;role&#39; is defined the decision is based only on members with that &#39;role&#39;,
  # i.e. using the oldest member (singleton) within the nodes with that role
  role = &quot;&quot;
}</code></pre>
<h3><a href="#keep-referee" name="keep-referee" class="anchor"><span class="anchor-link"></span></a>Keep Referee</h3>
<p>The strategy named <code>keep-referee</code> will down the part that does not contain the given referee node.</p>
<p>If the remaining number of nodes are less than the configured <em>down-all-if-less-than-nodes</em> all nodes will be downed. If the referee node itself is removed all nodes will be downed.</p>
<p>This strategy is good if you have one node that hosts some critical resource and the system cannot run without it. The drawback is that the referee node is a single point of failure, by design. <code>keep-referee</code> will never result in two separate clusters.</p>
<p>Configuration:</p>
<pre><code>akka.cluster.split-brain-resolver.active-strategy=keep-referee
</code></pre>
<pre class="prettyprint"><code class="language-conf">akka.cluster.split-brain-resolver.keep-referee {
  # referee address on the form of &quot;akka.tcp://system@hostname:port&quot;
  address = &quot;&quot;
  down-all-if-less-than-nodes = 1
}</code></pre>
<h3><a href="#down-all" name="down-all" class="anchor"><span class="anchor-link"></span></a>Down All</h3>
<p>The strategy named <code>down-all</code> will down all nodes.</p>
<p>This strategy can be a safe alternative if the network environment is highly unstable with unreachability observations that can&rsquo;t be fully trusted, and including frequent occurrences of <a href="split-brain-resolver.html#indirectly-connected-nodes">indirectly connected nodes</a>. Due to the instability there is an increased risk of different information on different sides of partitions and therefore the other strategies may result in conflicting decisions. In such environments it can be better to shutdown all nodes and start up a new fresh cluster.</p>
<p>Shutting down all nodes means that the system will be completely unavailable until nodes have been restarted and formed a new cluster. This strategy is not recommended for large clusters (&gt; 10 nodes) because any minor problem will shutdown all nodes, and that is more likely to happen in larger clusters since there are more nodes that may fail.</p>
<p>See also <a href="split-brain-resolver.html#down-all-when-unstable">Down all when unstable</a> and <a href="split-brain-resolver.html#indirectly-connected-nodes">indirectly connected nodes</a>.</p>
<h3><a href="#lease" name="lease" class="anchor"><span class="anchor-link"></span></a>Lease</h3><div class="callout warning "><div class="callout-title">Warning</div>
<p>The <code>lease-majority</code> is currently marked as <a href="https://doc.akka.io/docs/akka/2.5/common/may-change.html">May Change</a> in the sense of that the configuration and behavior might be changed based on feedback from initial usage.</p></div>
<p>The strategy named <code>lease-majority</code> is using a distributed lease (lock) to decide what nodes that are allowed to survive. Only one SBR instance can acquire the lease make the decision to remain up. The other side will not be able to aquire the lease and will therefore down itself.</p>
<p>Best effort is to keep the side that has most nodes, i.e. the majority side. This is achieved by adding a delay before trying to acquire the lease on the minority side.</p>
<p>There is currently one supported implementation of the lease which is backed by a <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definition (CRD)</a> in Kubernetes. It is described in the <a href="kubernetes-lease.html">Kubernetes Lease</a> documentation.</p>
<p>This strategy is very safe since coordination is added by an external arbiter. The trade-off compared to other strategies is that it requires additional infrastructure for implementing the lease and it reduces the availability of a decision to that of the system backing the lease store.</p>
<p>Similar to other strategies it is important that decisions are not deferred for too because the nodes that couldn&rsquo;t acquire the lease must decide to down themselves, see <a href="split-brain-resolver.html#down-all-when-unstable">Down all when unstable</a>.</p>
<p>In some cases the lease will be unavailable when needed for a decision from all SBR instances, e.g. because it is on another side of a network partition, and then all nodes will be downed.</p>
<p>Configuration:</p>
<pre><code>akka {
  cluster {
    downing-provider-class = &quot;com.lightbend.akka.sbr.SplitBrainResolverProvider&quot;
    split-brain-resolver {
      active-strategy = &quot;lease-majority&quot;
      lease-majority {
        lease-implementation = &quot;akka.lease.kubernetes&quot;
      }
    }
  }
}
</code></pre>
<pre class="prettyprint"><code class="language-conf">akka.cluster.split-brain-resolver.lease-majority {
  lease-implementation = &quot;&quot;

  # This delay is used on the minority side before trying to acquire the lease,
  # as an best effort to try to keep the majority side.
  acquire-lease-delay-for-minority = 2s

  # If the &#39;role&#39; is defined the majority/minority is based only on members with that &#39;role&#39;.
  role = &quot;&quot;
}</code></pre>
<p>See also configuration and additional dependency in <a href="kubernetes-lease.html">Kubernetes Lease</a></p>
<h2><a href="#indirectly-connected-nodes" name="indirectly-connected-nodes" class="anchor"><span class="anchor-link"></span></a>Indirectly connected nodes</h2>
<p>In a malfunctional network there can be situations where nodes are observed as unreachable via some network links but they are still indirectly connected via other nodes, i.e. it&rsquo;s not a clean network partition (or node crash).</p>
<p>When this situation is detected the Split Brain Resolvers will keep fully connected nodes and down all the indirectly connected nodes.</p>
<p>If there is a combination of indirectly connected nodes and a clean network partition it will combine the above decision with the ordinary decision, e.g. keep majority, after excluding suspicious failure detection observations.</p>
<h2><a href="#down-all-when-unstable" name="down-all-when-unstable" class="anchor"><span class="anchor-link"></span></a>Down all when unstable</h2>
<p>When reachability observations by the failure detector are changed the SBR decisions are deferred until there are no changes within the <code>stable-after</code> duration. If this continues for too long it might be an indication of an unstable system/network and it could result in delayed or conflicting decisions on separate sides of a network partition.</p>
<p>As a precaution for that scenario all nodes are downed if no decision is made within <code>stable-after + down-all-when-unstable</code> from the first unreachability event. The measurement is reset if all unreachable have been healed, downed or removed, or if there are no changes within <code>stable-after * 2</code>.</p>
<p>This is enabled by default for all strategies and by default the duration is derived to be 3/4 of <code>stable-after</code>.</p>
<p>The below property can be defined as a duration of for how long the changes are acceptable to continue after the <code>stable-after</code> or it can be set to <code>off</code> to disable this feature.</p>
<pre><code>akka.cluster.split-brain-resolver {
  down-all-when-unstable = 15s
  stable-after = 20s
}
</code></pre><div class="callout warning "><div class="callout-title">Warning</div>
<p>It is recommended to keep <code>down-all-when-unstable</code> enabled and not set it to a longer duration than <code>stable-after</code> (<code>down-removal-margin</code>) because that can result in delayed decisions on the side that should have been downed, e.g. in the case of a clean network partition followed by continued instability on the side that should be downed. That could result in that members are removed from one side but are still running on the other side.</p></div>
<h2><a href="#multiple-data-centers" name="multiple-data-centers" class="anchor"><span class="anchor-link"></span></a>Multiple data centers</h2>
<p>Akka Cluster has <a href="https://doc.akka.io/docs/akka/2.5/cluster-dc.html">support for multiple data centers</a>, where the cluster membership is managed by each data center separately and independently of network partitions across different data centers. The Split Brain Resolver is embracing that strategy and will not count nodes or down nodes in another data center.</p>
<p>When there is a network partition across data centers the typical solution is to wait the partition out until it heals, i.e. do nothing. Other decisions should be performed by an external monitoring tool or human operator.</p>
<h2><a href="#cluster-singleton-and-cluster-sharding" name="cluster-singleton-and-cluster-sharding" class="anchor"><span class="anchor-link"></span></a>Cluster Singleton and Cluster Sharding</h2>
<p>The purpose of Cluster Singleton and Cluster Sharding is to run at most one instance of a given actor at any point in time. When such an instance is shut down a new instance is supposed to be started elsewhere in the cluster. It is important that the new instance is not started before the old instance has been stopped. This is especially important when the singleton or the sharded instance is persistent, since there must only be one active writer of the journaled events of a persistent actor instance.</p>
<p>Since the strategies on different sides of a network partition cannot communicate with each other and they may take the decision at slightly different points in time there must be a time based margin that makes sure that the new instance is not started before the old has been stopped.</p>
<p>You would like to configure this to a short duration to have quick failover, but that will increase the risk of having multiple singleton/sharded instances running at the same time and it may take a different amount of time to act on the decision (dissemination of the down/removal). The duration is by default the same as the <code>stable-after</code> property (see <a href="split-brain-resolver.html#stable-after">Stable after</a> above). It is recommended to leave this value as is, but it can also be separately overriden with the <code>akka.cluster.down-removal-margin</code> property.</p>
<p>Another concern for setting this <code>stable-after</code>/<code>akka.cluster.down-removal-margin</code> is dealing with JVM pauses e.g. garbage collection. When a node is unresponsive it is not known if it is due to a pause, overload, a crash or a network partition. If it is pause that lasts longer than <code>stable-after</code> * 2 it gives time for SBR to down the node and for singletons and shards to be started on other nodes. When the node un-pauses there will be a short time before it sees its self as down where singletons and sharded actors are still running. It is therefore important to understand the max pause time your application is likely to incur and make sure it is smaller than <code>stable-margin</code>.</p>
<p>If you choose to set a separate value for <code>down-removal-margin</code>, the recommended minimum duration for different cluster sizes are:</p>
<table>
  <thead>
    <tr>
      <th>cluster size </th>
      <th>down-removal-margin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5 </td>
      <td>7 s </td>
    </tr>
    <tr>
      <td>10 </td>
      <td>10 s</td>
    </tr>
    <tr>
      <td>20 </td>
      <td>13 s</td>
    </tr>
    <tr>
      <td>50 </td>
      <td>17 s</td>
    </tr>
    <tr>
      <td>100 </td>
      <td>20 s</td>
    </tr>
    <tr>
      <td>1000 </td>
      <td>30 s</td>
    </tr>
  </tbody>
</table>
<h3><a href="#expected-failover-time" name="expected-failover-time" class="anchor"><span class="anchor-link"></span></a>Expected Failover Time</h3>
<p>As you have seen, there are several configured timeouts that add to the total failover latency. With default configuration those are:</p>
<ul>
  <li>failure detection 5 seconds</li>
  <li>stable-after 20 seconds</li>
  <li>down-removal-margin (by default the same as stable-after) 20 seconds</li>
</ul>
<p>In total, you can expect the failover time of a singleton or sharded instance to be around 45 seconds with default configuration. The default configuration is sized for a cluster of 100 nodes. If you have around 10 nodes you can reduce the <code>stable-after</code> to around 10 seconds, resulting in an expected failover time of around 25 seconds. </p>
</div>
</article>
<div class="row">
<div class="small-12 column">
<section class="nav-prev-next row">
<div class="nav-prev small-6 column">
<a href="akka-resilience-enhancements.html"><i class="icon-prev"></i> <span class="link-prev">Akka Resilience Enhancements</span></a>
</div>
<div class="nav-next small-6 column clearfix">
<a class="float-right" href="kubernetes-lease.html">Kubernetes Lease <i class="icon-next"></i></a>
</div>
</section>
</div>
</div>
<!-- no source links for private github repository -->

<footer class="page-footer row clearfix">
<img class="akka-icon float-left show-for-medium" src="images/akka-icon.svg" />
<section class="copyright">
<p class="legal">
&copy; 2011-2020 <a href="https://www.lightbend.com" target="_blank">Lightbend, Inc.</a> |
<a href="https://www.lightbend.com/legal/licenses" target="_blank">Licenses</a> |
<a href="https://www.lightbend.com/legal/terms" target="_blank">Terms</a> |
<a href="https://www.lightbend.com/legal/privacy" target="_blank">Privacy Policy</a> |
<a href="https://akka.io/cookie/" target="_blank">Cookie Listing</a> |
<a class="optanon-toggle-display">Cookie Settings</a>
</p>
</section>

</footer>
</section>
</main>
</div>

<script type="text/javascript" src="js/scrollsneak.js"></script>
<script type="text/javascript">jQuery(document).foundation();</script>
<script type="text/javascript" src="js/groups.js"></script>
<script type="text/javascript" src="js/page.js"></script>
<script type="text/javascript" src="js/magellan.js"></script>
<script type="text/javascript" src="js/metadata-toggle.js"></script>

<style type="text/css">@import "lib/prettify/prettify.css";</style>
<script type="text/javascript" src="lib/prettify/prettify.js"></script>
<script type="text/javascript" src="lib/prettify/lang-scala.js"></script>
<script type="text/javascript">//<![CDATA[
jQuery(function(){window.prettyPrint && prettyPrint()});
//]]></script>
<!-- hook for including project specific javascript into the generated docs -->

</body>
</html>
